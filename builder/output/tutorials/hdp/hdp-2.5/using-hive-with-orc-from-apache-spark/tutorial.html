<div class="tutorial-content">
  <p>In this tutorial, we will explore how you can access and analyze data on Hive from Spark. In particular, you will learn:</p>

<ul>
  <li>How to interact with Apache Spark through an interactive Spark shell</li>
  <li>How to read a text file from HDFS and create a RDD</li>
  <li>How to interactively analyze a data set through a rich set of Spark API operations</li>
  <li>How to create a Hive table in ORC File format</li>
  <li>How to query a Hive table using Spark SQL</li>
  <li>How to persist data in ORC file format</li>
</ul>

<p>Spark SQL uses the Spark engine to execute SQL queries either on data sets persisted in HDFS or on existing RDDs. It allows you to manipulate data with SQL statements within a Spark program.</p>

<h3 id="prerequisites">Prerequisites</h3>

<p>This tutorial is a part of series of hands-on tutorials to get you started with HDP using Hortonworks sandbox. Please ensure you complete the prerequisites before proceeding with this tutorial.</p>

<ul>
  <li>Download and Install <a href="https://hortonworks.com/products/sandbox/">Hortonworks Sandbox 2.5</a></li>
  <li><a href="https://hortonworks.com/hadoop-tutorial/learning-the-ropes-of-the-hortonworks-sandbox/">Learning the Ropes of the Hortonworks Sandbox</a></li>
</ul>

<h3 id="getting-the-dataset">Getting the dataset</h3>

<p>To begin, login to Hortonworks Sandbox through SSH:</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot_2015-04-13_07_58_43.png?dl=1" alt="" /></p>

<p>The default password is <code class="highlighter-rouge">hadoop</code>.</p>

<p>Now let’s download the dataset with the command below:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>wget http://hortonassets.s3.amazonaws.com/tutorial/data/yahoo_stocks.csv
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot%202015-05-28%2008.49.00.png?dl=1" alt="" /></p>

<p>and copy the downloaded file to HDFS:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>hadoop fs -put ./yahoo_stocks.csv /tmp/
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot%202015-05-28%2008.49.55.png?dl=1" alt="" /></p>

<h3 id="starting-the-spark-shell">Starting the Spark shell</h3>

<p>Use the command below to launch the Scala REPL for Apache Spark:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>spark-shell
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot%202015-05-28%2008.53.08.png?dl=1" alt="" /></p>

<p>Notice it is already starting with Hive integration as we have preconfigured it on the Hortonworks Sandbox.</p>

<p>Before we get started with the actual analytics let’s import some of the libraries we are going to use below.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">org.apache.spark.sql.hive.orc._</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.sql._</span>
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot%202015-05-21%2011.43.56.png?dl=1" alt="" /></p>

<h3 id="creating-hivecontext">Creating HiveContext</h3>

<p>HiveContext is an instance of the Spark SQL execution engine that integrates with data stored in Hive. The more basic SQLContext provides a subset of the Spark SQL support that does not depend on Hive. It reads the configuration for Hive from hive-site.xml on the classpath.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">val</span> <span class="n">hiveContext</span> <span class="o">=</span> <span class="k">new</span> <span class="n">org</span><span class="o">.</span><span class="na">apache</span><span class="o">.</span><span class="na">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">.</span><span class="na">hive</span><span class="o">.</span><span class="na">HiveContext</span><span class="o">(</span><span class="n">sc</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot%202015-05-21%2011.47.33.png?dl=1" alt="" /></p>

<h3 id="creating-orc-tables">Creating ORC tables</h3>

<p>ORC is a self-describing type-aware columnar file format designed for Hadoop workloads. It is optimized for large streaming reads and with integrated support for finding required rows fast. Storing data in a columnar format lets the reader read, decompress, and process only the values required for the current query. Because ORC files are type aware, the writer chooses the most appropriate encoding for the type and builds an internal index as the file is persisted.</p>

<p>Predicate pushdown uses those indexes to determine which stripes in a file need to be read for a particular query and the row indexes can narrow the search to a particular set of 10,000 rows. ORC supports the complete set of types in Hive, including the complex types: structs, lists, maps, and unions.</p>

<p>Specifying <code class="highlighter-rouge">as orc</code> at the end of the SQL statement below ensures that the Hive table is stored in the ORC format.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">hiveContext</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">"create table yahoo_orc_table (date STRING, open_price FLOAT, high_price FLOAT, low_price FLOAT, close_price FLOAT, volume INT, adj_price FLOAT) stored as orc"</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot%202015-05-28%2009.33.34.png?dl=1" alt="" /></p>

<h3 id="loading-the-file-and-creating-a-rdd">Loading the file and creating a RDD</h3>

<p>A <strong>Resilient Distributed Dataset</strong> (RDD), is an immutable collection of objects that is partitioned and distributed across multiple physical nodes of a YARN cluster and that can be operated in parallel.</p>

<p>Once an RDD is instantiated, you can apply a <a href="https://spark.apache.org/docs/1.2.0/programming-guide.html#rdd-operations">series of operations</a>. All operations fall into one of two types: <a href="https://spark.apache.org/docs/1.2.0/programming-guide.html#transformations">transformations</a> or <a href="https://spark.apache.org/docs/1.2.0/programming-guide.html#actions">actions</a>. <strong>Transformation</strong> operations, as the name suggests, create new datasets from an existing RDD and build out the processing DAG that can then be applied on the partitioned dataset across the YARN cluster. An <strong>Action</strong> operation, on the other hand, executes DAG and returns a value.</p>

<p>Normally, we would have directly loaded the data in the ORC table we created above and then created an RDD from the same, but in this to cover a little more surface of Spark we will create an RDD directly from the CSV file on HDFS and then apply Schema on the RDD and write it back to the ORC table.</p>

<p>With the command below we instantiate an RDD:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">val</span> <span class="n">yahoo_stocks</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">textFile</span><span class="o">(</span><span class="s">"hdfs://sandbox.hortonworks.com:8020/tmp/yahoo_stocks.csv"</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot%202015-05-21%2012.08.16.png?dl=1" alt="" /></p>

<p>To preview data in <code class="highlighter-rouge">yahoo_stocks</code> type:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">yahoo_stocks</span><span class="o">.</span><span class="na">take</span><span class="o">(</span><span class="mi">10</span><span class="o">)</span>
</code></pre>
</div>

<p>Note that <code class="highlighter-rouge">take(10)</code> returns only ten records that are not in any particular order.</p>

<h3 id="separating-the-header-from-the-data">Separating the header from the data</h3>

<p>Let’s assign the first row of the RDD above to a new variable</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">val</span> <span class="n">header</span> <span class="o">=</span> <span class="n">yahoo_stocks</span><span class="o">.</span><span class="na">first</span>
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot%202015-05-28%2010.14.21.png?dl=1" alt="" /></p>

<p>Let’s dump this new RDD in the console to see what we have here:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">header</span>
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot%202015-05-28%2010.22.10.png?dl=1" alt="" /></p>

<p>Now we need to separate the data into a new RDD where we do not have the header above and :</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">val</span> <span class="n">data</span> <span class="o">=</span> <span class="n">yahoo_stocks</span><span class="o">.</span><span class="na">mapPartitionsWithIndex</span> <span class="o">{</span> <span class="o">(</span><span class="n">idx</span><span class="o">,</span> <span class="n">iter</span><span class="o">)</span> <span class="o">=&gt;</span> <span class="k">if</span> <span class="o">(</span><span class="n">idx</span> <span class="o">==</span> <span class="mi">0</span><span class="o">)</span> <span class="n">iter</span><span class="o">.</span><span class="na">drop</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span> <span class="k">else</span> <span class="n">iter</span> <span class="o">}</span>
</code></pre>
</div>

<p>the first row to be seen is indeed only the data in the RDD</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">data</span><span class="o">.</span><span class="na">first</span>
</code></pre>
</div>

<h3 id="creating-a-schema">Creating a schema</h3>

<p>There’s two ways of doing this.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="k">case</span> <span class="kd">class</span> <span class="nf">YahooStockPrice</span><span class="o">(</span><span class="nl">date:</span> <span class="n">String</span><span class="o">,</span> <span class="nl">open:</span> <span class="n">Float</span><span class="o">,</span> <span class="nl">high:</span> <span class="n">Float</span><span class="o">,</span> <span class="nl">low:</span> <span class="n">Float</span><span class="o">,</span> <span class="nl">close:</span> <span class="n">Float</span><span class="o">,</span> <span class="nl">volume:</span> <span class="n">Integer</span><span class="o">,</span> <span class="nl">adjClose:</span> <span class="n">Float</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot%202015-05-28%2011.54.06.png?dl=1" alt="" /></p>

<h3 id="attaching-the-schema-to-the-parsed-data">Attaching the schema to the parsed data</h3>

<p>Create an RDD of Yahoo Stock Price objects and register it as a table.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">val</span> <span class="n">stockprice</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="n">_</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="s">","</span><span class="o">)).</span><span class="na">map</span><span class="o">(</span><span class="n">row</span> <span class="o">=&gt;</span> <span class="n">YahooStockPrice</span><span class="o">(</span><span class="n">row</span><span class="o">(</span><span class="mi">0</span><span class="o">),</span> <span class="n">row</span><span class="o">(</span><span class="mi">1</span><span class="o">).</span><span class="na">trim</span><span class="o">.</span><span class="na">toFloat</span><span class="o">,</span> <span class="n">row</span><span class="o">(</span><span class="mi">2</span><span class="o">).</span><span class="na">trim</span><span class="o">.</span><span class="na">toFloat</span><span class="o">,</span> <span class="n">row</span><span class="o">(</span><span class="mi">3</span><span class="o">).</span><span class="na">trim</span><span class="o">.</span><span class="na">toFloat</span><span class="o">,</span> <span class="n">row</span><span class="o">(</span><span class="mi">4</span><span class="o">).</span><span class="na">trim</span><span class="o">.</span><span class="na">toFloat</span><span class="o">,</span> <span class="n">row</span><span class="o">(</span><span class="mi">5</span><span class="o">).</span><span class="na">trim</span><span class="o">.</span><span class="na">toInt</span><span class="o">,</span> <span class="n">row</span><span class="o">(</span><span class="mi">6</span><span class="o">).</span><span class="na">trim</span><span class="o">.</span><span class="na">toFloat</span><span class="o">)).</span><span class="na">toDF</span><span class="o">()</span>
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot%202015-05-28%2011.59.33.png?dl=1" alt="" /></p>

<p>Let’s verify that the data has been correctly parsed by the statement above by dumping the first row of the RDD containing the parsed data:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">stockprice</span><span class="o">.</span><span class="na">first</span>
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot%202015-05-28%2014.02.58.png?dl=1" alt="" /></p>

<p>if we want to dump more all the rows, we can use</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">stockprice</span><span class="o">.</span><span class="na">show</span>
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot%202015-05-28%2014.08.33.png?dl=1" alt="" /></p>

<p>To verify the schema, let’s dump the schema:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">stockprice</span><span class="o">.</span><span class="na">printSchema</span>
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot%202015-05-28%2014.12.38.png?dl=1" alt="" /></p>

<h3 id="registering-a-temporary-table">Registering a temporary table</h3>

<p>Now let’s give this RDD a name, so that we can use it in Spark SQL statements:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">stockprice</span><span class="o">.</span><span class="na">registerTempTable</span><span class="o">(</span><span class="s">"yahoo_stocks_temp"</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot%202015-05-28%2014.19.30.png?dl=1" alt="" /></p>

<h3 id="querying-against-the-table">Querying against the table</h3>

<p>Now that our schema’s RDD with data has a name, we can use Spark SQL commands to query it. Remember the table below is not a Hive table, it is just a RDD we are querying with SQL.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">val</span> <span class="n">results</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">"SELECT * FROM yahoo_stocks_temp"</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot%202015-05-28%2016.24.14.png?dl=1" alt="" /></p>

<p>The resultset returned from the Spark SQL query is now loaded in the <code class="highlighter-rouge">results</code> RDD. Let’s pretty print it out on the command line.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">results</span><span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="n">t</span> <span class="o">=&gt;</span> <span class="s">"Stock Entry: "</span> <span class="o">+</span> <span class="n">t</span><span class="o">.</span><span class="na">toString</span><span class="o">).</span><span class="na">collect</span><span class="o">().</span><span class="na">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot%202015-05-21%2013.08.32.png?dl=1" alt="" /></p>

<h3 id="saving-as-an-orc-file">Saving as an ORC file</h3>

<p>Now let’s persist back the RDD into the Hive ORC table we created before.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">results</span><span class="o">.</span><span class="na">write</span><span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">"orc"</span><span class="o">).</span><span class="na">save</span><span class="o">(</span><span class="s">"yahoo_stocks_orc"</span><span class="o">)</span>
</code></pre>
</div>

<p>To store results in a hive directory rather than user directory, use this path instead:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>/apps/hive/warehouse/yahoo_stocks_orc
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot%202015-05-28%2016.52.44.png?dl=1" alt="" /></p>

<h3 id="reading-the-orc-file">Reading the ORC file</h3>

<p>Let’s now try to read back the ORC file, we just created back into an RDD. But before we do so, we need a <code class="highlighter-rouge">hiveContext</code>:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">val</span> <span class="n">hiveContext</span> <span class="o">=</span> <span class="k">new</span> <span class="n">org</span><span class="o">.</span><span class="na">apache</span><span class="o">.</span><span class="na">spark</span><span class="o">.</span><span class="na">sql</span><span class="o">.</span><span class="na">hive</span><span class="o">.</span><span class="na">HiveContext</span><span class="o">(</span><span class="n">sc</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot%202015-05-28%2017.23.06.png?dl=1" alt="" /></p>

<p>now we can try to read the ORC file with:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">val</span> <span class="n">yahoo_stocks_orc</span> <span class="o">=</span> <span class="n">hiveContext</span><span class="o">.</span><span class="na">read</span><span class="o">.</span><span class="na">format</span><span class="o">(</span><span class="s">"orc"</span><span class="o">).</span><span class="na">load</span><span class="o">(</span><span class="s">"yahoo_stocks_orc"</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot%202015-05-28%2017.24.05.png?dl=1" alt="" /></p>

<p>Let’s register it as a temporary in-memory table mapped to the ORC table:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">yahoo_stocks_orc</span><span class="o">.</span><span class="na">registerTempTable</span><span class="o">(</span><span class="s">"orcTest"</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot%202015-05-28%2017.24.53.png?dl=1" alt="" /></p>

<p>Now we can verify whether we can query it back:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">hiveContext</span><span class="o">.</span><span class="na">sql</span><span class="o">(</span><span class="s">"SELECT * from orcTest"</span><span class="o">).</span><span class="na">collect</span><span class="o">.</span><span class="na">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdp/hdp-2.5/using-hive-with-orc-from-apache-spark/assets/Screenshot%202015-05-28%2017.26.08.png?dl=1" alt="" /></p>

<p>Voila! We just did a round trip of using Spark shell, reading data from HDFS, creating an Hive table in ORC format, querying the Hive Table, and persisting data using Spark SQL.</p>

<p>Hope this tutorial illustrated some of the ways you can integrate Hive and Spark.</p>

</div>

<hr>

<div id="tutorial-footer">
  <h2>Tutorial Q&amp;A and Reporting Issues</h2>

  <p>If you need help or have questions with this tutorial, please check HCC for answers to existing questions about this tutorial by using the Find Answers button below.  You can post a new HCC question by using the Ask Questions button below.</p>

  <p>
    <a class="btn" href="https://community.hortonworks.com/topics/tutorial-400.html" role="button">Find Answers</a>
    <a class="btn pull-right" href="https://community.hortonworks.com/questions/ask.html?space=81&topics=tutorial-400&amp;topics=hdp-2.5.0" role="button">Ask Questions</a>
  </p>

  <p>Tutorial Name: <strong>Using Hive with ORC from Apache Spark</strong></p>
  <p>HCC Tags:<strong> tutorial-400</strong> and <strong>hdp-2.5.0</strong></p>
  <p>If the tutorial has multiple components please indicate which one your question relates to.</p>

  <p>All Hortonworks, partner and community tutorials are posted in the Hortonworks GitHub repository and can be contributed to by following the <a href="https://github.com/hortonworks/big-data-tutorials/wiki">Tutorial Contribution Guide</a>.  For issues/bugs/feedback, please <a href="https://github.com/hortonworks/big-data-tutorials/issues/new">submit an issue</a> and we will do our best to resolve it!</p>
</div>

