<div class="tutorial-content">
  <h1 id="tutorial-0-set-up-simulator-apache-services-and-ide-environment">Tutorial 0: Set Up Simulator, Apache Services and IDE Environment</h1>

<h2 id="introduction">Introduction</h2>

<p>In this tutorial, we are going to set up all the services required to run the Connected Car Application(Trucking Demo). We will install NiFi onto our Hortonworks Sandbox while activating Kafka and Storm for later use in the tutorial series. We will also walkthrough how to set up an IDE on our local machine for Storm development testing and deploy our Storm project to the Sandbox for further testing.</p>

<h2 id="prerequisites">Prerequisites</h2>

<ul>
  <li>Downloaded and Installed latest <a href="https://hortonworks.com/products/hortonworks-sandbox/#install">Hortonworks Sandbox</a></li>
  <li>If you are new to the sandbox shell, refer to <a href="https://hortonworks.com/hadoop-tutorial/learning-the-ropes-of-the-hortonworks-sandbox/">Learning the Ropes of the Hortonworks Sandbox</a></li>
  <li>We recommend you have some experience with Java Concepts, Syntax and OOP, refer to this <a href="https://www.udacity.com/course/intro-to-java-programming--cs046">Intro to Java Programming Course</a> if you are interested in building a strong foundation.</li>
  <li>Memory must be at least 8GB RAM, preferably 4 processor cores, else errors may occur in fourth tutorial</li>
  <li>For windows users, to run linux terminal commands in these tutorials, download <a href="https://openhatch.org/missions/windows-setup/install-git-bash">Git Bash</a>.</li>
  <li>Read <strong>Table 1</strong> to figure out some basic details about your sandbox</li>
</ul>

<p><strong>Table 1: Virtual Machine Information</strong> <a id="table1-virtual-machine-information"></a></p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Value (VirtualBox)</th>
      <th>Value(VMware)</th>
      <th>Value(MS Azure)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Host Name</td>
      <td>127.0.0.1</td>
      <td>172.16.110.129</td>
      <td>23.99.9.233</td>
    </tr>
    <tr>
      <td>Port</td>
      <td>2222</td>
      <td>2222</td>
      <td>22</td>
    </tr>
    <tr>
      <td>Terminal Username</td>
      <td>root</td>
      <td>root</td>
      <td>{username-of-azure}</td>
    </tr>
    <tr>
      <td>Terminal Password</td>
      <td>hadoop</td>
      <td>hadoop</td>
      <td>{password-of-azure}</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>Note: <strong>Host Name</strong> values are unique for VMware &amp; Azure Sandbox compared to the table. For VMware and VirtualBox, <strong>Host Name</strong> is located on welcome screen. For Azure, <strong>Host Name</strong> is located under <strong>Public IP Address</strong> on Sandbox Dashboard. For Azure users, the terminal <strong>username</strong> and <strong>password</strong> is one you created while deploying the sandbox on azure. For VMware and VirtualBox users, terminal password changes after first login.</p>
</blockquote>

<ul>
  <li>Added <code class="highlighter-rouge">sandbox.hortonworks.com</code> to your <code class="highlighter-rouge">/private/etc/hosts</code> file (mac and linux users)</li>
  <li>Added <code class="highlighter-rouge">sandbox.hortonworks.com</code> to your <code class="highlighter-rouge">/c/Windows/System32/Drivers/etc/hosts</code> file (windows 7 users)</li>
</ul>

<p>The following terminal commands in the tutorial instructions are performed in VirtualBox Sandbox and Mac machine. For windows users, to run the following terminal commands, download <a href="https://openhatch.org/missions/windows-setup/install-git-bash">Git Bash</a>.</p>

<p>If on mac or linux, to add <code class="highlighter-rouge">sandbox.hortonworks.com</code> to your list of hosts, open the terminal, enter the following command, replace {Host-Name} with the appropriate host for your sandbox:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="nb">echo</span> <span class="s1">'{Host-Name} sandbox.hortonworks.com'</span> | sudo tee -a /private/etc/hosts
</code></pre>
</div>

<p>If on windows 7, to add <code class="highlighter-rouge">sandbox.hortonworks.com</code> to your list of hosts, open git bash, enter the following command, replace {Host-Name} with the appropriate host for your sandbox:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="nb">echo</span> <span class="s1">'{Host-Name} sandbox.hortonworks.com'</span> | tee -a /c/Windows/System32/Drivers/etc/hosts
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/lab0-nifi/changing-hosts-file.png" alt="changing-hosts-file.png" /></p>

<h2 id="outline">Outline</h2>
<ul>
  <li><a href="#setup-nifi-environment">Section 1: Setup NiFi Environment</a></li>
  <li><a href="#setup-kafka-service">Section 2: Setup Kafka Service</a></li>
  <li><a href="#setup-storm-service">Section 3: Setup Storm &amp; HBase Service</a></li>
  <li><a href="#run-simulator-terminal">Section 4: Run the Simulator by Terminal</a></li>
  <li><a href="#setup-intellij-locally">Section 5: Setup Intellij IDE Locally and Run Topologies on Sandbox</a></li>
  <li><a href="#summary">Summary</a></li>
  <li><a href="#further-reading-tutorial0">Further Reading</a></li>
  <li><a href="#install-kafka-tutorial0">Appendix A: Install Kafka</a></li>
  <li><a href="#enable-remote-desktop-setup-topology-lab3">Appendix B: Enable remote desktop on sandbox and set up Storm topology as Eclipse project</a></li>
  <li><a href="#update-iot-truck-streaming-project-lab3">Appendix C: Update iot-truck-streaming Project</a></li>
</ul>

<h2 id="section-1-setup-nifi-environment-">Section 1: Setup NiFi Environment <a id="setup-nifi-environment"></a></h2>

<ul>
  <li>Install and Activate NiFi By Ambari Install Wizard</li>
</ul>

<h3 id="step-1-install-nifi-">Step 1: Install NiFi <a id="step1-install-nifi-tutorial0"></a></h3>

<p>NiFi will be installed into the Ambari Stack of the Hortonworks Sandbox VirtualBox image because it
will be used to activate the truck events simulator and transport data to Kafka.</p>

<p>1. If you do not have NiFi installed on your sandbox, refer to <a href="https://hortonworks.com/hadoop-tutorial/learning-ropes-apache-nifi#nifi-ambari-wizard">Section 2: Setup NiFi on Sandbox by Ambari Wizard</a> from Tutorial 0: Download, Install, and Start NiFi of
Learning the Ropes of Apache NiFi for step-by-step instructions.</p>

<h3 id="step-2-start-nifi-">Step 2: Start NiFi <a id="step2-start-nifi-tutorial0"></a></h3>

<p>1. To activate the NiFi service, refer to <a href="https://hortonworks.com/hadoop-tutorial/learning-ropes-apache-nifi#start-nifi-sandbox">Section 2: Start NiFi via Ambari Service</a> from Tutorial 0: Set Up NiFi Environment of
Learning the Ropes of Apache NiFi for step-by-step instructions.</p>

<h2 id="section-2-setup-kafka-service-">Section 2: Setup Kafka Service <a id="setup-kafka-service"></a></h2>

<ul>
  <li>Activate Kafka and Configure with Zookeeper</li>
</ul>

<p>We need to setup Kafka because it will be used as secure cluster or the location
where NiFi transports the data. Storm will pull that data from the cluster and
push it into it’s topology(dataflow).</p>

<h3 id="step-1-start-kafka-">Step 1: Start Kafka <a id="start-kafka"></a></h3>

<h4 id="11-access-ambari">1.1 Access Ambari</h4>

<p>If you haven’t reset your Ambari admin password, refer to Section <strong><a href="https://hortonworks.com/hadoop-tutorial/learning-the-ropes-of-the-hortonworks-sandbox/#setup-ambari-admin-password">2.2 SETUP AMBARI ADMIN PASSWORD MANUALLY</a></strong> from Learning the Ropes of the Hortonworks Sandbox. Login to Ambari to activate Kafka. Enter the URL in your browser <code class="highlighter-rouge">http://sandbox.hortonworks.com:8080</code></p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/lab1-kafka/login_page_ambari.png" alt="login_page_ambari" /></p>

<blockquote>
  <p>Note: username for admin is admin. Password is the password you defined.</p>
</blockquote>

<h4 id="12-use-ambari-to-activate-kafka">1.2 Use Ambari to Activate Kafka</h4>

<p>1. Click on Kafka located in the left sidebar list of installed services. (If Kafka is not installed, refer to <a href="https://hortonworks.com/hadoop-tutorial/realtime-event-processing-nifi-kafka-storm/#install-kafka-tutorial0">Appendix A: Install Kafka</a> instructions.):</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/lab1-kafka/kafka_service_on_off.png" alt="kafka_service_on_off" /></p>

<p>2. Click on <strong>Service Actions -&gt; Start</strong> located at the top left of the Kafka Services Page:</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/lab1-kafka/start_kafka_service_iot.png" alt="" /></p>

<p>3. Check the <strong>Turn off Maintenance Mode for Kafka</strong> box and click on <strong>Confirm Start</strong>:</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/lab1-kafka/confirmation_kafka_service_start.png" alt="Screen Shot 2015-06-04 at 3.06.10 PM.png" /></p>

<p>Wait for Kafka to start.</p>

<h3 id="step-2-configure-kafka-with-zookeeper-">Step 2: Configure Kafka with Zookeeper <a id="config-kafka-zookeeper"><a></a></a></h3>

<p>ZooKeeper is the coordination interface between the Kafka broker and consumers:</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/lab1-kafka/zookeeper-kafka-producer-broker-consumer.jpg" alt="Single Broker based Kakfa cluster" /></p>

<p>The important Zookeeper properties can be checked in Ambari.</p>

<h4 id="21--configure-zookeeper">2.1  Configure ZooKeeper</h4>

<p>Click on <strong>ZooKeeper</strong> in the list of services, then open the Configs tab. Verify ZooKeeper runs on port 2181:</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/lab1-kafka/zookeeper_port_config.png" alt="zookeeper_port_config" /></p>

<p>If this port 2181 is busy or consumed by other processes, you can change the default port number of ZooKeeper to any other valid port number. If ZooKeeper is not running, you can start the Zookeeper service from Ambari:</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/lab1-kafka/zookeeper_start_service_iot.png" alt="zookeeper_start_service_iot" /></p>

<h4 id="22-configure-kafka">2.2 Configure Kafka</h4>

<p>From the Kafka page, click on the <strong>Configs</strong> tab. Verify the <code class="highlighter-rouge">zookeeper.connect</code> property points to your ZooKeeper server name and port:</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/lab1-kafka/verify_zookeeper_connect_pts_zoo_server_kafka.png" alt="" /></p>

<h2 id="section-3-setup-storm--hbase-services-">Section 3: Setup Storm &amp; HBase Services <a id="setup-storm-service"></a></h2>

<ul>
  <li>Activate Storm &amp; HBase</li>
</ul>

<p>We need to setup the Storm &amp; HBase services because storm will deploy a topology
that is configured to send data to HBase and HBase is used to create the tables
that storm populates.</p>

<h3 id="step-1-start-storm--hbase-">Step 1: Start Storm &amp; HBase <a id="step-1-start-hbase-storm-lab3"></a></h3>

<p>1.  <strong>View the HBase Services page</strong></p>

<p>From the previous tutorials: HDFS, Hive, YARN and Kafka should already be running but HBase may be down. From the Dashboard page of Ambari, click on HBase from the list of installed services.</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/lab2-hbase-hive-storm/hbase_service_on_off_iot.png" alt="hbase_service_on_off_iot" /></p>

<h4 id="11-setup-hbase">1.1 Setup HBase</h4>

<p>2. Start HBase</p>

<p>From the HBase page, click on <strong>Service Actions -&gt; Start</strong></p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/lab2-hbase-hive-storm/start_hbase_service_iot.png" alt="start_hbase_service_iot" /></p>

<p>Check the box and click on <strong>Confirm Start</strong>:</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/lab2-hbase-hive-storm/confirm_hbase_start_iot.png" alt="confirm_hbase_start_iot" /></p>

<p>Wait for HBase to start (It may take a few minutes to turn green)</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/lab2-hbase-hive-storm/hbase_started_iot.png" alt="hbase_started_iot" /></p>

<h4 id="12-setup-storm">1.2 Setup Storm</h4>

<p>3. Start Storm the same way we started HBase in the previous steps. We will need it later for streaming real-time event data.</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/lab2-hbase-hive-storm/storm_service_on_off_iot.png" alt="storm_service_on_off_iot" /></p>

<p>4. After starting storm, a green check symbol will be present:</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/lab2-hbase-hive-storm/storm_service_started_iot.png" alt="storm_service_started_iot" /></p>

<p>Now that we have storm activated, we need to download a storm demo project for
later when we use Storm’s Visualization feature.</p>

<p>5. Let’s SSH into the sandbox by shell:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>ssh root@sandbox.hortonworks.com -p 2222
</code></pre>
</div>

<p>6. We will download the demo project because it contains the necessary
code and configuration properties required for storm to deploy the topology.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>cd ~
git clone -b hdp25experiment https://github.com/james94/iot-truck-streaming
</code></pre>
</div>

<p>7. Navigate to the iot-truck-streaming folder, which is the location where the
appropriate storm configuration files reside for configuring our topology.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>cd ~/iot-truck-streaming
</code></pre>
</div>

<p>8. Since we are at the base of our project, let’s export our storm demo
configurations. We will create a new folder <strong>storm_demo</strong> as the new location
where storm will look for instructions on configuring the storm topology:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>sudo mkdir /etc/storm_demo
sudo cp config.properties /etc/storm_demo/
sudo cp -R storm-demo-webapp/routes/ /etc/storm_demo/
</code></pre>
</div>

<blockquote>
  <p>Note: Storm will refer to the properties in the config.properties file to configure the topology.</p>
</blockquote>

<p>You can use the Ambari dashboard to check status of other components too. If <strong>HDFS, Hive, YARN</strong> are down, you can start them in the same way: by selecting the service and then using the Service Actions to start it. The remaining components do not have to be up. (Oozie can be stopped to save memory, as it is not needed for this tutorial)</p>

<h2 id="section-4-run-the-simulator-by-terminal-">Section 4: Run the Simulator by Terminal <a id="run-simulator-terminal"></a></h2>

<ul>
  <li>Understand the Stream Simulator Functionality</li>
  <li>Run the Simulator By Shell</li>
</ul>

<p>The simulator must be setup in order for NiFi to pull data from it and push
that data into the dataflow.</p>

<h3 id="stream-simulator-">Stream Simulator <a id="stream-simulator-lab0"></a></h3>

<p>The stream simulator is a lightweight framework that generates truck event data. The simulator uses <a href="http://www.nyc.gov/html/dot/downloads/misc/all_truck_routes_nyc.kml">New York City Truck Routes (kml)</a> which defines driver road paths with Latitude and Longitude information.</p>

<p>The simulator uses <strong><a href="http://akka.io/">Akka</a></strong> to simplify concurrency, messaging and inheritance. It has two <strong><a href="https://en.wikipedia.org/wiki/Plain_Old_Java_Object">Plain Old Java Objects (POJOS)</a></strong>, one for Trucks and another for Drivers that generate the events. Consequently, the <strong>AbstractEventEmitter</strong> class becomes extended while the <strong>onReceive</strong> method generates events, creates new <strong><a href="http://doc.akka.io/docs/akka/snapshot/java/untyped-actors.html">Actors</a></strong>, sends messages to Actors and delivers those events to an <strong>EventCollector</strong> class. This class’s purpose is to collect events generated from the domain objects and print them to standard output. Let’s run the simulator through the terminal and see the events that are generated.</p>

<h3 id="step-1-run-the-simulator-by-shell-">Step 1: Run the Simulator By shell <a id="step1-run-simulator-shell-lab0"></a></h3>

<p>Before we run the simulator, let’s install and download the simulator. For VMware and Azure users, refer to <a href="#table1-virtual-machine-information">Table 1</a> for more details on using ssh for that environment. For windows users, download <a href="https://openhatch.org/missions/windows-setup/install-git-bash">Git Bash</a> to run the terminal commands.</p>

<h4 id="11-setup-the-simulator">1.1 Setup The Simulator</h4>

<p>1. If you have not already ssh into sandbox shell, type the command to access the sandbox by shell:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>ssh root@sandbox.hortonworks.com -p 2222
</code></pre>
</div>

<p>2. Install Apache Maven. We will use it to compile the simulator code,
so we can activate the simulator by shell or NiFi later. Execute the command:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>cd ~/iot-truck-streaming
./setup/bin/install_maven.sh
</code></pre>
</div>

<blockquote>
  <p>Note: You will be prompted to allow maven to install, type ‘y’ for yes</p>
</blockquote>

<p>After your maven package installs, you should obtain the message: Complete!</p>

<p>3. For maven to run, it needs to detect the pom.xml file. Rename pom25.xml to pom.xml, copy/paste the commands:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>mv -f storm-streaming/pom25.xml storm-streaming/pom.xml

/usr/maven/bin/mvn clean package -DskipTests
</code></pre>
</div>

<blockquote>
  <p>Note: You should receive that all sub projects compiled successfully.</p>
</blockquote>

<h4 id="12-run-the-simulator">1.2 Run The Simulator</h4>

<p>1. To test the simulator, run <code class="highlighter-rouge">generate.sh</code> script.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>cd stream-simulator
chmod 750 *.sh
./generate.sh
</code></pre>
</div>

<blockquote>
  <p>Note: press <strong>ctrl+c</strong> stop the simulator</p>
</blockquote>

<p>You should see message data generated. The data in the image includes logs as can be seen in the top portion and truck events bottom portion. We will use NiFi to separate this data.</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/lab0-nifi/generateSH_data_logs_truckevents_iot.png" alt="generate_sh_data" /></p>

<blockquote>
  <p>Note: generate.sh runs java source code located at <code class="highlighter-rouge">iot-truck-streaming/stream-simulator/src/main/java/com/hortonworks/streaming/impl/collectors/StdOutEventCollector.java</code>. If you would like to see modify/run the code.</p>
</blockquote>

<h2 id="section-5-setup-intellij-ide-locally-and-run-topologies-on-sandbox--">Section 5: Setup Intellij IDE Locally and Run Topologies on Sandbox  <a id="setup-intellij-locally"></a></h2>

<ul>
  <li>Setup Intellij For Hadoop Related Development</li>
  <li>Run Topologies On Sandbox that were built on Local Machine</li>
</ul>

<p>We will use Intellij as our editor to write and change storm code on our local computer and then send our storm project to the sandbox.</p>

<h3 id="step-1-install-intellij-locally">Step 1: Install Intellij Locally</h3>

<p>If you have not installed <strong>Intellij</strong>, refer to JetBrains <a href="https://www.jetbrains.com/help/idea/2016.2/installing-and-launching.html">Installing and Launching</a> instructions.</p>

<h3 id="step-2-download-trucking-demo-for-development-with-ide">Step 2: Download Trucking Demo for Development with IDE</h3>

<p>Earlier we cloned the iot-truck-streaming project from github onto our sandbox. Now we will clone it onto our local machine.</p>

<p>1. Perform the git clone command on the local machine. Feel free to clone it in any directory, just remember the location. In the tutorial, let’s clone it in our Documents folder.</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="nb">cd</span> ~/Documents
git clone -b hdp25experiment https://github.com/james94/iot-truck-streaming.git
</code></pre>
</div>

<blockquote>
  <p>Note: You may receive an error if you don’t have git installed on your local machine.</p>
</blockquote>

<h3 id="step-3-open-trucking-demo-in-intellij">Step 3: Open Trucking Demo in Intellij</h3>

<p>1. Open Intellij. On the welcome screen, click on the Open button. We will open our <strong>iot-truck-streaming</strong> project.</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/tutorial0-setup-environ/welcome_button_intellij.png" alt="welcome_button_intellij" /></p>

<p>2. Select <strong>iot-truck-streaming</strong> project.</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/tutorial0-setup-environ/open_storm_project.png" alt="open_storm_project" /></p>

<p>3. Intellij will display the project in its IDE.</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/tutorial0-setup-environ/storm_project_not_recognized_initial.png" alt="storm_project_not_recognized_initial" /></p>

<h3 id="step-4-configure-intellij-to-recognize-maven-project">Step 4: Configure Intellij To Recognize Maven Project</h3>

<p>You’ll notice, the image icons next to the code files have small red circles on them. Intellij does not recognize the Trucking Demo is a Maven Project. The solution we will use is to run <code class="highlighter-rouge">mvn clean package</code> from the command line and cause Intellij to warn us that Maven Projects need to be imported to be recognized by the IDE. When the <strong>Enable Auto-Import</strong> box appears in Intellij, we will activate for Maven projects.</p>

<p>1. Let’s begin the process to run maven against our Trucking Demo Project.</p>

<h4 id="41-specify-pomxml-for-maven">4.1 Specify pom.xml for Maven</h4>

<ul>
  <li>For maven to work, we have to tell it which pom file to use from the storm-streaming directory. Let’s rename pom25 to pom using shell.</li>
</ul>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>mv iot-truck-streaming/storm-streaming/pom25.xml iot-truck-streaming/storm-streaming/pom.xml
</code></pre>
</div>

<h4 id="42-compile-and-package-demo-files-into-jar">4.2 Compile and Package Demo Files into Jar</h4>

<ul>
  <li>Now that we specified the pom for maven, it knows how to build the project. Let’s execute the command:</li>
</ul>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="nb">cd </span>iot-truck-streaming
mvn clean package -DskipTests
</code></pre>
</div>

<p>Output should show success for each sub project within the overall project.</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/tutorial0-setup-environ/maven_success_for_IDE_storm_project.png" alt="maven_success_for_IDE_storm_project" /></p>

<h3 id="apache-maven-command">Apache Maven command:</h3>

<p>mvn clean deletes everything in the target folder. For example, if this was the second time you ran the mvn command from iot-truck-streaming folder, the storm-streaming folder as well as other folders that contain pom.xml files will have their target folders impacted. Clean part of the command removes the old target folder, while the package part of the command compiles the code and packages it into jar files according to the pom file.</p>

<blockquote>
  <p>Note: packaging may take around 9 minutes. Add -DskipTests to the end of mvn command to speed up process.</p>
</blockquote>

<p><strong>Why is Maven important for development testing?</strong></p>

<p>It enables developers to test their large software projects that contain many
java files at fast speeds.</p>

<h4 id="43-instruct-intellij-to-enable-auto-import-for-maven-projects">4.3 Instruct Intellij to Enable Auto-Import for Maven Projects</h4>

<p>1. Switch to the Intellij IDE to see if it recognizes the Maven Project.</p>

<p>2. As you run maven, you will see in Intellij that a box in the top right corner appears and states <strong>Maven projects need to be imported</strong>. Click on <strong>Enable Auto-Import</strong>.</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/tutorial0-setup-environ/enable_auto_import_maven.png" alt="enable_auto_import_maven" /></p>

<h4 id="44-intellij-recognizes-trucking-demo-is-maven-project">4.4 Intellij Recognizes Trucking Demo is Maven Project</h4>

<ul>
  <li>Notice that the icons next to the code files changed to blue circles.</li>
</ul>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/tutorial0-setup-environ/code_files_recognized_unlock_symbol.png" alt="code_files_recognized_unlock_symbol" /></p>

<blockquote>
  <p>Note: If you want to make commits to github from Intellij, feel free to try out Intellij’s git feature, else ignore the “Add Files to Git” Window if it appears.</p>
</blockquote>

<h4 id="45-intellij-setup-to-develop-hadoop-projects-locally">4.5 Intellij Setup To Develop Hadoop Projects Locally</h4>

<p>1. Now we can develop our storm code from the storm-streaming folder directly on our local machine. Feel free to explore the code, modify or add bolts, spouts, or topology.</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/tutorial0-setup-environ/develop_hadoop_projects_locally.png" alt="develop_hadoop_projects_locally" /></p>

<p>2. Once you’ve added or modified the code, we can run mvn command used earlier to package our storm project into a jar.</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="nb">cd</span> ~/Documents/iot-truck-streaming
mvn clean package -DskipTests
</code></pre>
</div>

<blockquote>
  <p>Note: If you want to add an enhancement to the demo, all you need to do is re-execute the above steps and the new modifications will be added to the jar.</p>
</blockquote>

<p>3. As you will see, mvn generates a target folder in every sub project folder; for instance, let’s view the project we are working on, storm-streaming, it’s target folder contains:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>ls -ltr storm-streaming/target
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/tutorial0-setup-environ/list_files_target_folder.png" alt="list_files_target_folder" /></p>

<blockquote>
  <p>Notice the target folder contains <strong>storm-streaming-1.0-SNAPSHOT.jar</strong> file. This jar is a collection of java classes for our storm project. When you add an enhancement to the demo and maven is executed, the jar file will be removed and replaced with a new version.</p>
</blockquote>

<p>4. Let’s send our jar to the sandbox for later use in tutorial 3.</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>scp -P 2222 ~/Documents/iot-truck-streaming/storm-streaming/target/storm-streaming-1.0-SNAPSHOT.jar root@sandbox.hortonworks.com:/root/iot-truck-streaming/storm-streaming/target
</code></pre>
</div>

<blockquote>
  <p>Note: Each time we update the demo, we have to transport the latest jar file to the sandbox.</p>
</blockquote>

<h2 id="summary">Summary</h2>

<p>Since we can build jars on our local machine and have an instance of the <strong>iot-truck-streaming</strong> project on our sandbox with the appropriate configurations, we can make changes to our code locally, and then send the jar to any directory on our sandbox. In our case, we will send it to <code class="highlighter-rouge">storm-streaming/target</code> folder. This approach makes it easy to develop storm topologies locally, and test it on an HDP environment. Therefore, we can test if our code performs as expected by viewing the topology visually through the Storm View’s Visualization. For example, did we connect our spouts and bolts properly. We can also use HBase to view if Storm sending the proper data to the tables. We will <strong>perform these tests in tutorial 3</strong></p>

<h2 id="further-reading-">Further Reading <a id="further-reading-tutorial0"></a></h2>

<p>Refer to Readings below if you want to learn best practices for installing and configuring the NiFi, Kafka, Storm, Simulator, IDE Environments</p>

<ul>
  <li><a href="http://docs.hortonworks.com/HDPDocuments/HDF1/HDF-1.2.0.1/bk_AdminGuide/content/ch_administration_guide.html">NiFi System Administrator’s Guide</a></li>
  <li><a href="https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.2/bk_kafka-user-guide/content/ch_installing_kafka.html">Installing and Configuring Kafka</a></li>
  <li><a href="https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.3.4/bk_installing_manually_book/content/ch_installing_storm_chapter.html">Installing and Configuring Storm</a></li>
  <li><a href="https://github.com/hortonworks-gallery/iot-truck-streaming/tree/master/stream-simulator">Stream Simulator Documentation</a></li>
  <li>Our Trucking Demo project is a Maven Project, refer to <a href="https://www.jetbrains.com/help/idea/2016.2/discover-intellij-idea.html#BuildTools">Working with Build Tools(Maven/Gradle) Intellij</a> for more in depth resources on the topic</li>
  <li><a href="https://www.jetbrains.com/help/idea/2016.2/meet-intellij-idea.html">Meet Intellij IDEA</a></li>
</ul>

<h2 id="appendix-a-install-kafka-">Appendix A: Install Kafka <a id="install-kafka-tutorial0"></a></h2>

<p>Follow these steps if your version of the Sandbox does not have Kafka installed:</p>

<p>1.  From the Ambari Dashboard, select Actions -&gt; Add Service:</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/lab1-kafka/add_service_kafka.png" alt="" /></p>

<p>2.  Select Kafka from the list of Services and click Next:</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/lab1-kafka/addServiceWizard_kafka_service_iot.png" alt="" /></p>

<p>3.  Keep clicking Next with the selected defaults until you reach the following screen:</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/lab1-kafka/log_dirs_kafka_broker_iot.png" alt="" /></p>

<p>4.  Set the value of logs.dir to  /tmp/kafka-logs</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/lab1-kafka/change_kafka_broker_log_dirs_iot.png" alt="" /></p>

<p>5.  Click the Deploy button:</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/lab1-kafka/deploy_kafka_service_iot.png" alt="" /></p>

<p>6.  Wait for Kafka to install:</p>

<p><img src="https://raw.githubusercontent.com/orendain/big-data-tutorials/master/tutorials/hdf/hdf-2.1/realtime-event-processing-in-hadoop-with-nifi-kafka-and-storm/assets/lab1-kafka/wait_kafka_service_install_iot.png" alt="" /></p>

<p>7.  After Kafka is installed, you may be asked to restart some dependent Services. Please select the appropriate Services and click Restart.</p>

<h2 id="appendix-b-enable-remote-desktop-on-sandbox-and-set-up-storm-topology-as-eclipse-project-">Appendix B: Enable remote desktop on sandbox and set up Storm topology as Eclipse project <a id="enable-remote-desktop-setup-topology-lab3"></a></h2>

<ol>
  <li>Setup Ambari VNC service on the sandbox to enable remote desktop via VNC and install eclipse using steps here <a href="https://github.com/hortonworks-gallery/ambari-vnc-service%23setup-vnc-service">https://github.com/hortonworks-gallery/ambari-vnc-service#setup-vnc-service</a></li>
  <li>Import code as Eclipse project using steps here:</li>
</ol>

<p><a href="https://github.com/hortonworks-gallery/ambari-vnc-service%23getting-started-with-storm-and-maven-in-eclipse-environment">https://github.com/hortonworks-gallery/ambari-vnc-service#getting-started-with-storm-and-maven-in-eclipse-environment</a></p>

<h2 id="appendix-c-update-iot-truck-streaming-project-">Appendix C: Update iot-truck-streaming Project <a id="update-iot-truck-streaming-project-lab3"></a></h2>

<ul>
  <li>Copy /etc/hbase/conf/hbase-site.xml to src/main/resources/ directory</li>
</ul>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="o">[</span>root@sandbox ~]# <span class="nb">cd</span> /iot-truck-streaming
<span class="o">[</span>root@sandbox ~]# cp /etc/hbase/conf/hbase-site.xml src/main/resources/
</code></pre>
</div>

<ul>
  <li>Check pom.xml to ensure it includes the below dependencies (check after <strong>line 104</strong>)</li>
</ul>

<div class="language-html highlighter-rouge"><pre class="highlight"><code>    <span class="nt">&lt;dependency&gt;</span>
      <span class="nt">&lt;groupId&gt;</span>xerces<span class="nt">&lt;/groupId&gt;</span>
      <span class="nt">&lt;artifactId&gt;</span>xercesImpl<span class="nt">&lt;/artifactId&gt;</span>
      <span class="nt">&lt;version&gt;</span>2.9.1<span class="nt">&lt;/version&gt;</span>
    <span class="nt">&lt;/dependency&gt;</span>

    <span class="nt">&lt;dependency&gt;</span>
      <span class="nt">&lt;groupId&gt;</span>xalan<span class="nt">&lt;/groupId&gt;</span>
      <span class="nt">&lt;artifactId&gt;</span>xalan<span class="nt">&lt;/artifactId&gt;</span>
      <span class="nt">&lt;version&gt;</span>2.7.1<span class="nt">&lt;/version&gt;</span>
    <span class="nt">&lt;/dependency&gt;</span>

    <span class="nt">&lt;dependency&gt;</span>
      <span class="nt">&lt;groupId&gt;</span>org.htrace<span class="nt">&lt;/groupId&gt;</span>
      <span class="nt">&lt;artifactId&gt;</span>htrace-core<span class="nt">&lt;/artifactId&gt;</span>
      <span class="nt">&lt;version&gt;</span>3.0.4<span class="nt">&lt;/version&gt;</span>
    <span class="nt">&lt;/dependency&gt;</span>

    <span class="nt">&lt;dependency&gt;</span>
      <span class="nt">&lt;groupId&gt;</span>org.apache.hadoop<span class="nt">&lt;/groupId&gt;</span>
      <span class="nt">&lt;artifactId&gt;</span>hadoop-hdfs<span class="nt">&lt;/artifactId&gt;</span>
      <span class="nt">&lt;version&gt;</span>2.6.0<span class="nt">&lt;/version&gt;</span>
    <span class="nt">&lt;/dependency&gt;</span>
</code></pre>
</div>

<ul>
  <li>recompile the Maven project. This may run for 10+ min</li>
</ul>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="o">[</span>root@sandbox ~]# mvn clean package
</code></pre>
</div>

<p>The maven build should succeed.</p>

</div>

<hr>

<div id="tutorial-footer">
  <h2>Tutorial Q&amp;A and Reporting Issues</h2>

  <p>If you need help or have questions with this tutorial, please check HCC for answers to existing questions about this tutorial by using the Find Answers button below.  You can post a new HCC question by using the Ask Questions button below.</p>

  <p>
    <a class="btn" href="https://community.hortonworks.com/topics/tutorial-220.html" role="button">Find Answers</a>
    <a class="btn pull-right" href="https://community.hortonworks.com/questions/ask.html?space=81&topics=tutorial-220&amp;topics=hdp-2.5.0" role="button">Ask Questions</a>
  </p>

  <p>Tutorial Name: <strong>Realtime Event Processing in Hadoop with NiFi, Kafka and Storm</strong></p>
  <p>HCC Tags:<strong> tutorial-220</strong> and <strong>hdp-2.5.0</strong></p>
  <p>If the tutorial has multiple components please indicate which one your question relates to.</p>

  <p>All Hortonworks, partner and community tutorials are posted in the Hortonworks GitHub repository and can be contributed to by following the <a href="https://github.com/hortonworks/big-data-tutorials/wiki">Tutorial Contribution Guide</a>.  For issues/bugs/feedback, please <a href="https://github.com/hortonworks/big-data-tutorials/issues/new">submit an issue</a> and we will do our best to resolve it!</p>
</div>

