

<div class="tutorial-content">
  <h2 id="introduction">Introduction</h2>

<p>Apache Falcon is a framework to simplify data pipeline processing and management on Hadoop clusters.</p>

<p>It makes it much simpler to onboard new workflows/pipelines, with support for late data handling and retry policies. It allows you to easily define relationships between various data and processing elements and integrate with metastore/catalog such as Apache Hive/HCatalog. Finally it also lets you capture lineage information for feeds and processes. In this tutorial we are going to walkthrough the process of:</p>

<ul>
  <li>Defining the feeds and processes</li>
  <li>Defining and executing a data pipeline to ingest, process and persist data continuously</li>
</ul>

<h2 id="pre-requisites">Pre-Requisites</h2>

<ul>
  <li><a href="http://hortonworks.com/products/hortonworks-sandbox/#install">Download Hortonworks Sandbox</a></li>
  <li>Complete the <a href="http://hortonworks.com/hadoop-tutorial/learning-the-ropes-of-the-hortonworks-sandbox/">Learning the Ropes of the Hortonworks Sandbox</a> tutorial, you will need it for logging into ambari as an administrator user.</li>
  <li>Complete the <a href="http://hortonworks.com/hadoop-tutorial/create-falcon-cluster/">Creating Falcon Cluster tutorial</a> to start the falcon service, prepare HDFS directories for Falcon cluster and to create Falcon cluster entities.</li>
</ul>

<p>Once you have downloaded the Hortonworks sandbox and run the VM, navigate to the Ambari interface on port <code class="highlighter-rouge">8080</code> of the host IP address of your Sandbox VM. Login with the username of <code class="highlighter-rouge">admin</code> and password that you set for the Ambari admin user as part of the <code class="highlighter-rouge">Learning the Ropes of the Hortonworks Sandbox</code> tutorial:</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/Screenshot%202015-08-19%2016.28.48.png?dl=1" alt="" /></p>

<h2 id="outline">Outline</h2>
<ul>
  <li><a href="#scenario">Scenario</a></li>
  <li><a href="#starting-falcon">Starting Falcon</a></li>
  <li><a href="#download-and-stage-the-dataset">Download and stage the dataset</a></li>
  <li><a href="#define-the-rawEmailFeed-entity-wizard">Define the rawEmailFeed entity using Wizard</a></li>
  <li><a href="#define-the-rawEmailFeed-entity-XML">Define the rawEmailFeed entity using XML</a></li>
  <li><a href="#define-the-rawEmailIngestProcess-entity-wizard">Define the rawEmailIngestProcess entity using Wizard</a></li>
  <li><a href="#define-the-rawEmailIngestProcess-entity-XML">Define the rawEmailIngestProcess entity using XML</a></li>
  <li><a href="#define-the-cleansedEmailFeed-wizard">Define the cleansedEmailFeed using Wizard</a></li>
  <li><a href="#define-the-cleansedEmailFeed-XML">Define the cleansedEmailFeed using XML</a></li>
  <li><a href="#define-the-cleansedEmailProcess-wizard">Define the cleanseEmailProcess using Wizard</a></li>
  <li><a href="#define-the-cleansedEmailProcess-XML">Define the cleanseEmailProcess using XML</a></li>
  <li><a href="#run-the-feeds">Run the feeds</a></li>
  <li><a href="#run-the-processes">Run the processes</a></li>
  <li><a href="#input-and-output-of-the-pipeline">Input and Output of the pipeline</a></li>
  <li><a href="#summary">Summary</a></li>
</ul>

<p>For this tutorial, we will utilize the virtualbox environment. Therefore, VMware and Azure Sandbox users your instructions may be slightly different.</p>

<h2 id="scenario-a-idscenarioa">Scenario <a id="scenario"></a></h2>

<p>In this tutorial, we will walk through a scenario where email data lands hourly on a cluster. In our example:</p>

<ul>
  <li>This cluster is the primary cluster located in the Oregon data center.</li>
  <li>Data arrives from all the West Coast production servers. The input data feeds are often late for up to 4 hrs.</li>
</ul>

<p>The goal is to clean the raw data to remove sensitive information like credit card numbers and make it available to our marketing data science team for customer churn analysis.</p>

<p>To simulate this scenario, we have a Pig script grabbing the freely available Enron Corpus emails from the internet and feeding it into the pipeline.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/arch.png" alt="" /></p>

<h2 id="starting-falcon-a-idstarting-falcona">Starting Falcon <a id="starting-falcon"></a></h2>

<p>By default, Falcon is not started on the sandbox, but you should have started the service while completing the <code class="highlighter-rouge">Creating a Falcon Cluster</code> tutorial. Do the following to verify that the Falcon service is started, or to start it if it was disabled.</p>

<p>In the Ambari UI,  click on the Falcon icon in the left hand pane.</p>

<p>Then click on the Service Actions button on the top right.</p>

<p>Then, if the service is disabled, click on <code class="highlighter-rouge">Start</code>.</p>

<p>Once Falcon starts, Ambari should clearly indicate as below that the service has started:</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/Screenshot%202015-08-19%2016.34.32.png?dl=1" alt="" /></p>

<h2 id="download-and-stage-the-dataset-a-iddownload-and-stage-the-dataseta">Download and stage the dataset <a id="download-and-stage-the-dataset"></a></h2>

<p>Now let's stage the dataset you will use for this tutorial. Although we perform many of these file operations below using the command line, you can also do the same with the <code class="highlighter-rouge">HDFS Files  View</code> in Ambari.</p>

<blockquote>
  <p>Tip: You can copy and paste the commands from this tutorial.</p>
</blockquote>

<p>First, enter the shell with your preferred shell client. For this tutorial, we will SSH into Hortonworks Sandbox with the command:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>ssh root@127.0.0.1 -p 2222;
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/sshTerminal.png" alt="sshTerminal" /></p>

<p>The default password isÂ <code class="highlighter-rouge">hadoop</code>.</p>

<p>Then login as user <code class="highlighter-rouge">hdfs</code>:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>su - hdfs
</code></pre>
</div>

<p>Then download the file falcon.zip with the following command:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>wget http://hortonassets.s3.amazonaws.com/tutorial/falcon/falcon.zip
</code></pre>
</div>

<p>and then unzip with the command:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>unzip falcon.zip
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/unzippingFalconFiles.png" alt="unzippingFalconFiles" /></p>

<p>Now let's give ourselves permission to upload files:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>hadoop fs -chmod -R 777 /user/ambari-qa
</code></pre>
</div>

<p>Then let's create a folder <code class="highlighter-rouge">falcon</code> under <code class="highlighter-rouge">ambari-qa</code> with the command:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>hadoop fs -mkdir /user/ambari-qa/falcon
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/creatingDirectory.png" alt="creatingDirectory" /></p>

<p>Now let's upload the decompressed folder with the command</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>hadoop fs -copyFromLocal demo /user/ambari-qa/falcon/
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/copyingDemoFile.png" alt="copyingDemoFile" /></p>

<!---
## Create the cluster entities <a id="create-the-cluster-entities"></a>

Before creating the cluster entities, we need to create the directories on HDFS representing the two clusters that we are going to define, namely `primaryCluster` and `backupCluster`. This must be done as a Falcon superuser.

From hdfs@sandbox, first execute `su -root`, then `su -falcon`.

Use `hadoop fs -mkdir` commands to create the directories `/apps/falcon/primaryCluster` and `/apps/falcon/backupCluster` directories on HDFS.

~~~bash
hadoop fs -mkdir /apps/falcon/primaryCluster
hadoop fs -mkdir /apps/falcon/backupCluster
~~~

![](/assets/falcon-processing-pipelines/Screenshot%202015-08-07%2010.29.58.png?dl=1)  


Further create directories called `staging` inside each of the directories we created above:

~~~bash
hadoop fs -mkdir /apps/falcon/primaryCluster/staging
hadoop fs -mkdir /apps/falcon/backupCluster/staging
~~~
![](/assets/falcon-processing-pipelines/Screenshot%202015-08-07%2010.31.37.png?dl=1)  


Next we will need to create the `working` directories for `primaryCluster` and `backupCluster`

~~~bash
hadoop fs -mkdir /apps/falcon/primaryCluster/working
hadoop fs -mkdir /apps/falcon/backupCluster/working
~~~

![](/assets/falcon-processing-pipelines/Screenshot%202015-08-07%2010.36.12.png?dl=1)  


Finally you need to set the proper permissions on the staging/working directories:

~~~bash
hadoop fs -chmod 777 /apps/falcon/primaryCluster/staging
hadoop fs -chmod 755 /apps/falcon/primaryCluster/working
hadoop fs -chmod 777 /apps/falcon/backupCluster/staging
hadoop fs -chmod 755 /apps/falcon/backupCluster/working
~~~

Let's open the Falcon Web UI. You can easily launch the Falcon Web UI from Ambari:

![](/assets/falcon-processing-pipelines/Screenshot%202015-08-19%2016.31.12.png?dl=1)  


You can also navigate to the Falcon Web UI directly on your browser. The Falcon UI is by default at port 15000\. The default username is `ambari-qa`.

![](/assets/falcon-processing-pipelines/Screenshot%202015-08-07%2010.45.40.png?dl=1)  


This UI allows us to create and manage the various [entities](https://falcon.apache.org/restapi/EntityValidate.html) like Cluster, Feed, Process and Mirror. Each of these entities are represented by an XML file that you either directly upload or generate by completing the various fields.

You can also search for existing entities and then edit, change state, etc.

![](/assets/falcon-processing-pipelines/Screenshot%202015-08-07%2010.46.23.png?dl=1)  


Let's first create a couple of cluster entities. To create a cluster entity click on the `Cluster` button on the top.

A cluster entity defines the default access points for various resources on the cluster as well as default working directories to be used by Falcon jobs.

To define a cluster entity, we must specify a unique name by which we can identify the cluster.  In this tutorial, we use:

~~~
primaryCluster
~~~

Next enter a data center name or location of the cluster and a description for the cluster.  The data center name can be used by Falcon to improve performance of jobs that run locally or across data centers.

All entities defined in Falcon can be grouped and located using tags.  To clearly identify and locate entities, we assign the tag:

~~~
EntityType
~~~

With the value

~~~
Cluster
~~~

We then need to specify the owner and permissions for the cluster.  

So we enter:

~~~
Owner:  ambari-qa
Group: users
Permissions: 755
~~~

Next, we enter the URI for the various resources Falcon requires to manage data on the clusters.  These include the NameNode dfs.http.address, the NameNode IPC address used for Filesystem metadata operations,  the Yarn client IPC address used for executing jobs on Yarn, the Oozie address used for running Falcon Feeds and Processes, and the Falcon messaging address.  The values we will use are the defaults for the Hortonworks Sandbox,  if you run this tutorial on your own test cluster, modify the addresses to match those defined in Ambari:

~~~
Readonly hftp://sandbox.hortonworks.com:50070
Write hdfs://sandbox.hortonworks.com:8020"
Execute sandbox.hortonworks.com:8050         
Workflow http://sandbox.hortonworks.com:11000/oozie/
Messaging tcp://sandbox.hortonworks.com:61616?daemon=true
~~~

The version numbers are not used and will be removed in the next version of the Falcon UI.

You can also override cluster properties for a specific cluster.  This can be useful for test or backup clusters which may have different physical configurations.  In this tutorial, we'll just use the properties defined in Ambari.

After the resources are defined, you must define default staging, temporary and working directories for use by Falcon jobs based on the HDFS directories created earlier in the tutorial.  These can be overridden by specific jobs, but will be used in the event no directories are defined at the job level.  In the current version of the UI, these directories must exist, be owned by Falcon, and have the proper permissions.

~~~
Staging  /apps/falcon/primaryCluster/staging
Temp /tmp         
Working /apps/falcon/primaryCluster/working
~~~

![](/assets/falcon-processing-pipelines/Screenshot%202015-08-07%2010.49.25.png?dl=1)

Once you have verified that the entities are the correct values, press `Next`.

![](/assets/falcon-processing-pipelines/Screenshot%202015-08-07%2010.50.01.png?dl=1)  


Click `Save` to persist the entity.

![](/assets/falcon-processing-pipelines/Screenshot%202015-08-07%2010.50.18.png?dl=1)  

You should receive a notification that the operation was successful.


Falcon jobs require a source and target cluster.  For some jobs, this may be the same cluster, for others, such as Mirroring and Disaster Recovery, the source and target clusters will be different.  Let's go ahead and create a second cluster by creating a cluster with the name:

~~~
backupCluster
~~~

Reenter the same information you used above except for the directory information.  For the directories, use the backupCluster directories created earlier in the tutorial.

~~~
Staging  /apps/falcon/backupCluster/staging
Temp /tmp         
Working /apps/falcon/backupCluster/working
~~~



![](/assets/falcon-processing-pipelines/Screenshot%202015-08-07%2010.51.14.png?dl=1)  


Click `Save` to persist the `backupCluster` entity.

![](/assets/falcon-processing-pipelines/Screenshot%202015-08-07%2010.51.33.png?dl=1)  
-->

<h2 id="define-the-rawemailfeed-entity-using-wizard-a-iddefine-the-rawemailfeed-entity-wizarda">Define the rawEmailFeed entity using Wizard <a id="define-the-rawEmailFeed-entity-wizard"></a></h2>

<p>To create a feed entity click on the <code class="highlighter-rouge">Feed</code> button on the top of the main page on the Falcon Web UI.</p>

<p><strong>NOTE : If you want to create it from XML, skip this section, and move on to the next one.</strong></p>

<p>Then enter the definition for the feed by giving the feed a unique name and a description.  For this tutorial we will use</p>

<div class="highlighter-rouge"><pre class="highlight"><code>rawEmailFeed
</code></pre>
</div>

<p>and</p>

<div class="highlighter-rouge"><pre class="highlight"><code>Raw customer email feed.
</code></pre>
</div>

<p>Let's also enter a tag key and value, so we can easily locate this Feed later:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>externalSystem=USWestEmailServers
</code></pre>
</div>

<p>Feeds can be further categorised by identifying them with one or more groups.  In this demo, we will group all the Feeds together by defining the group:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>churnAnalysisDataPipeline
</code></pre>
</div>

<p>We then set the ownership information for the Feed:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>Owner:  ambari-qa
Group:  users
Permissions: 755
</code></pre>
</div>

<p>For the Schema Location and Provider, enter "/none", then click Next.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/feed1.png" alt="feed1" /></p>

<p>On the Properties page, specify to run the job hourly by specifying the frequency as 1 hour, check Late Arrival Checkbox and specify the value as 1 hour. Change the timezone to UTC and click <code class="highlighter-rouge">Next</code>.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/feed2.png" alt="feed2" /></p>

<p>Enter the path of our data set on the Locations page:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>/user/ambari-qa/falcon/demo/primary/input/enron/${YEAR}-${MONTH}-${DAY}-${HOUR}
</code></pre>
</div>

<p>We will set the stats and meta paths to / for now. Click <code class="highlighter-rouge">Next</code>.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/feed3.png" alt="feed3" /></p>

<p>On the Clusters page select the cluster you created, then enter today's date and the current time for the validity start time, and enter an hour or two later for the end time.  The validity time specifies the period during which the feed will run.  For many feeds, validity time will be set to the time the feed is scheduled to go into production and the end time will be set into the far future.  Because we are running this tutorial on the Sandbox, we want to limit the time the process will run to conserve resources.</p>

<p><strong>DO NOT forget to provide the validity time in the timezone you have selected.</strong></p>

<p>Specify the retention period of 90 hours, then click <code class="highlighter-rouge">Next</code>.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/feed4.png" alt="feed4" /></p>

<p>Save the feed.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/feed5.png" alt="feed5" /></p>

<!---

Next we specify how often the job should run.
=======
Add the Schema Location and Provider, the click Next.
>>>>>>> hortonworks/hdp

On the Properties page, specify to run the job hourly by specifying the frequency as 1 hour.
Click Next and enter the path of our data set on the Locations page:

~~~
/user/ambari-qa/falcon/demo/primary/input/enron/${YEAR}-${MONTH}-${DAY}-${HOUR}
~~~

We will set the stats and meta paths to `/` for now.


![](/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.09.14.png?dl=1)  


Once you have verified that these are the correct values press `Next`.


![](/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.14.21.png?dl=1)  


On the Clusters page, select the cluster you created, then enter today's date and the current time for the validity start time, and enter an hour or two later for the end time.  The validity time specifies the period during which the feed will run.  For many feeds, validity time will be set to the time the feed is scheduled to go into production and the end time will be set into the future. Because we are running this tutorial on the Sandbox, we want to limit the time the process will run to conserve resources. Set the retention value, then click `Next`

![](/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.15.35.png?dl=1)  


Save the feed.

![](/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.16.01.png?dl=1)  
-->

<h2 id="define-the-rawemailfeed-entity-using-xml-a-iddefine-the-rawemailfeed-entity-xmla">Define the rawEmailFeed entity using XML <a id="define-the-rawEmailFeed-entity-XML"></a></h2>

<p>Then click on the <code class="highlighter-rouge">Edit</code> button over XML Preview area on the right hand side of the screen and replace the XML content with the XML document below:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="cp">&lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?&gt;</span>
<span class="nt">&lt;feed</span> <span class="na">name=</span><span class="s">"rawEmailFeed"</span> <span class="na">description=</span><span class="s">"Raw customer email feed"</span> <span class="na">xmlns=</span><span class="s">"uri:falcon:feed:0.1"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;tags&gt;</span>externalSystem=USWestEmailServers<span class="nt">&lt;/tags&gt;</span>
    <span class="nt">&lt;groups&gt;</span>churnAnalysisDataPipeline<span class="nt">&lt;/groups&gt;</span>
    <span class="nt">&lt;frequency&gt;</span>hours(1)<span class="nt">&lt;/frequency&gt;</span>
    <span class="nt">&lt;timezone&gt;</span>UTC<span class="nt">&lt;/timezone&gt;</span>
    <span class="nt">&lt;late-arrival</span> <span class="na">cut-off=</span><span class="s">"hours(1)"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;clusters&gt;</span>
        <span class="nt">&lt;cluster</span> <span class="na">name=</span><span class="s">"primaryCluster"</span> <span class="na">type=</span><span class="s">"source"</span><span class="nt">&gt;</span>
            <span class="nt">&lt;validity</span> <span class="na">start=</span><span class="s">"2016-06-05T05:00Z"</span> <span class="na">end=</span><span class="s">"2016-06-05T06:00Z"</span><span class="nt">/&gt;</span>
            <span class="nt">&lt;retention</span> <span class="na">limit=</span><span class="s">"hours(90)"</span> <span class="na">action=</span><span class="s">"delete"</span><span class="nt">/&gt;</span>
        <span class="nt">&lt;/cluster&gt;</span>
    <span class="nt">&lt;/clusters&gt;</span>
    <span class="nt">&lt;locations&gt;</span>
        <span class="nt">&lt;location</span> <span class="na">type=</span><span class="s">"data"</span> <span class="na">path=</span><span class="s">"/user/ambari-qa/falcon/demo/primary/input/enron/${YEAR}-${MONTH}-${DAY}-${HOUR}"</span><span class="nt">/&gt;</span>
        <span class="nt">&lt;location</span> <span class="na">type=</span><span class="s">"stats"</span> <span class="na">path=</span><span class="s">"/"</span><span class="nt">/&gt;</span>
        <span class="nt">&lt;location</span> <span class="na">type=</span><span class="s">"meta"</span> <span class="na">path=</span><span class="s">"/"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;/locations&gt;</span>
    <span class="nt">&lt;ACL</span> <span class="na">owner=</span><span class="s">"ambari-qa"</span> <span class="na">group=</span><span class="s">"users"</span> <span class="na">permission=</span><span class="s">"0755"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;schema</span> <span class="na">location=</span><span class="s">"/none"</span> <span class="na">provider=</span><span class="s">"/none"</span><span class="nt">/&gt;</span>
<span class="nt">&lt;/feed&gt;</span>
</code></pre>
</div>

<p><strong>NOTE : DO NOT copy the validity start and end time. Change it as per your time.</strong></p>

<p>Click <code class="highlighter-rouge">Finish</code> on the top of the XML Preview area.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/feedXML1.png" alt="feedXML1" /></p>

<p>Falcon UI should have automatically parsed out the values from the XML and populated in the right fields. Once you have verified that these are the correct values, save the feed.</p>

<h2 id="define-the-rawemailingestprocess-entity-using-wizard-a-iddefine-the-rawemailingestprocess-entity-wizarda">Define the rawEmailIngestProcess entity using Wizard <a id="define-the-rawEmailIngestProcess-entity-wizard"></a></h2>

<p>Now lets define the <code class="highlighter-rouge">rawEmailIngestProcess</code>.</p>

<p>To create a process entity click on the <code class="highlighter-rouge">Process</code> button on the top of the main page on the Falcon Web UI.</p>

<p><strong>NOTE : If you want to create it from XML, skip this section, and move on to the next one.</strong></p>

<p>Use the information below to create the process:</p>

<p>This job will run on the primaryCluster. Use the information below to create the process:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>process name rawEmailIngestProcess
Tags email
With the value: testemail
</code></pre>
</div>

<p>And assign the workflow the name:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>emailIngestWorkflow
</code></pre>
</div>

<p>Select Oozie as the execution engine and provide the following path:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>/user/ambari-qa/falcon/demo/apps/ingest/fs
</code></pre>
</div>
<p>Accept the default values for the Access Control List and click Next.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/process1.png" alt="process1" /></p>

<p>On the Properties page, select the time zone, and set the frequency to 1 hour. Set the number of parallel processes to 1. This prevents a new instance from starting prior to the previous one completing. Specify the order as first-in, first-out (FIFO).
Select the exp-backoff retry policy, then set the attempts to 3 and the delay to 3 minutes. Click <code class="highlighter-rouge">Next</code>.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/process2.png" alt="process2" /></p>

<p>On the Clusters page, select primaryCluster again. As you did previously, modify the validity to a time slice which is in the very near future; set the validity to start now and end in an hour or two. Click <code class="highlighter-rouge">Next</code>.</p>

<p><strong>Note : Time should be specified as per the timezone selected.</strong></p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/process3.png" alt="process3" /></p>

<p>On the Inputs &amp; Outputs page, ignore the Inputs section. For the output, enter a name and select the <code class="highlighter-rouge">rawEmailFeed</code> we created in the previous step and specify now(0,0) for the instance.</p>

<p><code class="highlighter-rouge">now(hours,minutes)</code>: now refer to the instance start time. Hours and minutes given are in reference with the start time of instance.  For example now(-2,40) corresponds to feed instance at -2 hr and +40 minutes i.e. feed instance 80 mins before the instance start time. now(0,0) corresponds to feed instance at current time.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/process4.png" alt="process4" /></p>

<p>Lets save the process.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/process5.png" alt="process5" /></p>

<!---
This job will run on the primaryCluster.
Again, set the validity to start now and end in an hour or two.

For the properties, set the number of parallel processes to 1, this prevents a new instance from starting prior to the previous one completing.

Specify the order as `first-in, First-out (FIFO)` and the Frequency to `1 hour`.

For inputs and output, enter the rawEmailFeed we created in the previous step and specify now(0,0) for the instance.  And assign the workflow the name:
=======
>>>>>>> hortonworks/hdp

~~~
emailIngestWorkflow
~~~

Select Oozie as the execution engine and provide the following path:

~~~
/user/ambari-qa/falcon/demo/apps/ingest/fs
~~~


![](/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.17.01.png?dl=1)  


Accept the default values for the Access Control List and click Next.

![](/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.17.19.png?dl=1)  


On the Properties page, select the time zone, and set the frequency to 1 hour. Set the number of parallel processes to 1. This prevents a new instance from starting prior to the previous one completing. Specify the order as first-in, first-out (FIFO).
Select the exp-backoff retry policy, then set the attempts to 3 and the delay to 3 minutes. Click next.

On the Clusters page, select primaryCluster again. As you did previously, modify the validity to a time slice which is in the very near future; set the validity to start now and end in an hour or two. Click Next.

![](/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.18.02.png?dl=1)  


On the Inputs & Outputs page, ignore the Inputs section For the output, enter a name and select the rawEmailFeed we created in the previous step and specify now(0,0) for the instance.

![](/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.18.15.png?dl=1)  


Let's `Save` the process.

![](/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.18.37.png?dl=1)  
-->

<h2 id="define-the-rawemailingestprocess-entity-using-xml-a-iddefine-the-rawemailingestprocess-entity-xmla">Define the rawEmailIngestProcess entity using XML <a id="define-the-rawEmailIngestProcess-entity-XML"></a></h2>

<p>Click on the <code class="highlighter-rouge">Edit</code> button over XML Preview area on the right hand side of the screen and replace the XML content with the XML document below:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="cp">&lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?&gt;</span>
<span class="nt">&lt;process</span> <span class="na">name=</span><span class="s">"rawEmailIngestProcess"</span> <span class="na">xmlns=</span><span class="s">"uri:falcon:process:0.1"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;tags&gt;</span>email=testemail<span class="nt">&lt;/tags&gt;</span>
    <span class="nt">&lt;clusters&gt;</span>
        <span class="nt">&lt;cluster</span> <span class="na">name=</span><span class="s">"primaryCluster"</span><span class="nt">&gt;</span>
            <span class="nt">&lt;validity</span> <span class="na">start=</span><span class="s">"2016-06-05T05:00Z"</span> <span class="na">end=</span><span class="s">"2016-06-05T06:00Z"</span><span class="nt">/&gt;</span>
        <span class="nt">&lt;/cluster&gt;</span>
    <span class="nt">&lt;/clusters&gt;</span>
    <span class="nt">&lt;parallel&gt;</span>1<span class="nt">&lt;/parallel&gt;</span>
    <span class="nt">&lt;order&gt;</span>FIFO<span class="nt">&lt;/order&gt;</span>
    <span class="nt">&lt;frequency&gt;</span>hours(1)<span class="nt">&lt;/frequency&gt;</span>
    <span class="nt">&lt;timezone&gt;</span>UTC<span class="nt">&lt;/timezone&gt;</span>
    <span class="nt">&lt;outputs&gt;</span>
        <span class="nt">&lt;output</span> <span class="na">name=</span><span class="s">"output"</span> <span class="na">feed=</span><span class="s">"rawEmailFeed"</span> <span class="na">instance=</span><span class="s">"now(0,0)"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;/outputs&gt;</span>
    <span class="nt">&lt;workflow</span> <span class="na">name=</span><span class="s">"emailIngestWorkflow"</span> <span class="na">version=</span><span class="s">"4.0.1"</span> <span class="na">engine=</span><span class="s">"oozie"</span> <span class="na">path=</span><span class="s">"/user/ambari-qa/falcon/demo/apps/ingest/fs"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;retry</span> <span class="na">policy=</span><span class="s">"exp-backoff"</span> <span class="na">delay=</span><span class="s">"minutes(3)"</span> <span class="na">attempts=</span><span class="s">"3"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;ACL</span> <span class="na">owner=</span><span class="s">"ambari-qa"</span> <span class="na">group=</span><span class="s">"users"</span> <span class="na">permission=</span><span class="s">"0755"</span><span class="nt">/&gt;</span>
<span class="nt">&lt;/process&gt;</span>
</code></pre>
</div>

<p><strong>NOTE : DO NOT copy the validity start and end time. Change it as per your time..</strong></p>

<p>Click <code class="highlighter-rouge">Finish</code> on the top of the XML Preview area.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/processXML1.png" alt="processXML1" /></p>

<p>Verify all the values and click <code class="highlighter-rouge">Next</code>.</p>

<p>Save the process.</p>

<h2 id="define-the-cleansedemailfeed-using-wizard-a-iddefine-the-cleansedemailfeed-wizarda">Define the cleansedEmailFeed using Wizard <a id="define-the-cleansedEmailFeed-wizard"></a></h2>

<p>As the name suggests, the raw feeds which you have created earlier get cleansed in the Process and the feed which comes out from that Process is the cleansed feed. So, to create a cleansed feed entity click on the <code class="highlighter-rouge">Feed</code> button on the top of the main page on the Falcon Web UI.</p>

<p><strong>NOTE : If you want to create it from XML, skip this section, and move on to the next one.</strong></p>

<p>Use the following information to create the feed:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>name cleansedEmailFeed
description Cleansed customer emails
tag cleanse with value cleaned
Group churnAnalysisDataPipeline
</code></pre>
</div>

<p>We then set the ownership information for the Feed:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>Owner:  ambari-qa
Group:  users
Permissions: 755
</code></pre>
</div>

<p>For the Schema Location and Provider, enter "/none", then click <code class="highlighter-rouge">Next</code>.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/feed21.png" alt="feed21" /></p>

<p>On the Properties page, specify to run the job hourly by specifying the frequency as 1 hour, check Late Arrival Checkbox and specify the value as 4 hours. Change the timezone to UTC, accept the default values and click <code class="highlighter-rouge">Next</code>.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/feed22.png" alt="feed22" /></p>

<p>Set the default storage location to</p>

<div class="highlighter-rouge"><pre class="highlight"><code>/user/ambari-qa/falcon/demo/processed/enron/${YEAR}-${MONTH}-${DAY}-${HOUR}
</code></pre>
</div>

<p>Keep stats path and meta path as /.
Click <code class="highlighter-rouge">Next</code>.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/feed23.png" alt="feed23" /></p>

<p>On the Clusters page,select source cluster as a primary cluster ensure you modify the validity to a time slice which is in the very near future. Provide time in current UTC. Specify the path for the data as:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>/user/ambari-qa/falcon/demo/primary/processed/enron/${YEAR}-${MONTH}-${DAY}-${HOUR}
</code></pre>
</div>

<p>And enter / for the stats and meta data locations.
Keep the retention period as 90 hours.</p>

<p>Set the target cluster as backupCluster and again set the validity start for the current time and end time to an hour or two from now
And specify the data path for the target to</p>

<div class="highlighter-rouge"><pre class="highlight"><code>/falcon/demo/bcp/processed/enron/${YEAR}-${MONTH}-${DAY}-${HOUR}
</code></pre>
</div>

<p>Set the statistics and meta data locations to /
Enter the retention period as 90 hours for target cluster as well.
Click <code class="highlighter-rouge">Next</code>.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/feed24.png" alt="feed24" /></p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/feed25.png" alt="feed25" /></p>

<p>Verify all the entries you have filled out and then click <code class="highlighter-rouge">Save</code>.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/feed26.png" alt="feed26" /></p>

<h2 id="define-the-cleansedemailfeed-using-xml-a-iddefine-the-cleansedemailfeed-xmla">Define the cleansedEmailFeed using XML <a id="define-the-cleansedEmailFeed-XML"></a></h2>

<p>Click on the <code class="highlighter-rouge">Edit</code> button over XML Preview area on the right hand side of the screen and replace the XML content with the XML document below:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="cp">&lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?&gt;</span>
<span class="nt">&lt;feed</span> <span class="na">name=</span><span class="s">"cleansedEmailFeed"</span> <span class="na">description=</span><span class="s">"Cleansed customer emails"</span> <span class="na">xmlns=</span><span class="s">"uri:falcon:feed:0.1"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;tags&gt;</span>cleanse=cleaned<span class="nt">&lt;/tags&gt;</span>
    <span class="nt">&lt;groups&gt;</span>churnAnalysisDataPipeline<span class="nt">&lt;/groups&gt;</span>
    <span class="nt">&lt;frequency&gt;</span>hours(1)<span class="nt">&lt;/frequency&gt;</span>
    <span class="nt">&lt;timezone&gt;</span>UTC<span class="nt">&lt;/timezone&gt;</span>
    <span class="nt">&lt;late-arrival</span> <span class="na">cut-off=</span><span class="s">"hours(4)"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;clusters&gt;</span>
        <span class="nt">&lt;cluster</span> <span class="na">name=</span><span class="s">"primaryCluster"</span> <span class="na">type=</span><span class="s">"source"</span><span class="nt">&gt;</span>
            <span class="nt">&lt;validity</span> <span class="na">start=</span><span class="s">"2016-06-05T05:00Z"</span> <span class="na">end=</span><span class="s">"2016-06-05T06:00Z"</span><span class="nt">/&gt;</span>
            <span class="nt">&lt;retention</span> <span class="na">limit=</span><span class="s">"hours(90)"</span> <span class="na">action=</span><span class="s">"delete"</span><span class="nt">/&gt;</span>
            <span class="nt">&lt;locations&gt;</span>
                <span class="nt">&lt;location</span> <span class="na">type=</span><span class="s">"data"</span> <span class="na">path=</span><span class="s">"/user/ambari-qa/falcon/demo/primary/processed/enron/${YEAR}-${MONTH}-${DAY}-${HOUR}"</span><span class="nt">/&gt;</span>
                <span class="nt">&lt;location</span> <span class="na">type=</span><span class="s">"stats"</span> <span class="na">path=</span><span class="s">"/"</span><span class="nt">/&gt;</span>
                <span class="nt">&lt;location</span> <span class="na">type=</span><span class="s">"meta"</span> <span class="na">path=</span><span class="s">"/"</span><span class="nt">/&gt;</span>
            <span class="nt">&lt;/locations&gt;</span>
        <span class="nt">&lt;/cluster&gt;</span>
        <span class="nt">&lt;cluster</span> <span class="na">name=</span><span class="s">"backupCluster"</span> <span class="na">type=</span><span class="s">"target"</span><span class="nt">&gt;</span>
            <span class="nt">&lt;validity</span> <span class="na">start=</span><span class="s">"2016-06-05T05:00Z"</span> <span class="na">end=</span><span class="s">"2016-06-05T06:00Z"</span><span class="nt">/&gt;</span>
            <span class="nt">&lt;retention</span> <span class="na">limit=</span><span class="s">"hours(90)"</span> <span class="na">action=</span><span class="s">"delete"</span><span class="nt">/&gt;</span>
            <span class="nt">&lt;locations&gt;</span>
                <span class="nt">&lt;location</span> <span class="na">type=</span><span class="s">"data"</span> <span class="na">path=</span><span class="s">"/falcon/demo/bcp/processed/enron/${YEAR}-${MONTH}-${DAY}-${HOUR}"</span><span class="nt">/&gt;</span>
                <span class="nt">&lt;location</span> <span class="na">type=</span><span class="s">"stats"</span> <span class="na">path=</span><span class="s">"/"</span><span class="nt">/&gt;</span>
                <span class="nt">&lt;location</span> <span class="na">type=</span><span class="s">"meta"</span> <span class="na">path=</span><span class="s">"/"</span><span class="nt">/&gt;</span>
            <span class="nt">&lt;/locations&gt;</span>
        <span class="nt">&lt;/cluster&gt;</span>
    <span class="nt">&lt;/clusters&gt;</span>
    <span class="nt">&lt;locations&gt;</span>
        <span class="nt">&lt;location</span> <span class="na">type=</span><span class="s">"data"</span> <span class="na">path=</span><span class="s">"/user/ambari-qa/falcon/demo/processed/enron/${YEAR}-${MONTH}-${DAY}-${HOUR}"</span><span class="nt">/&gt;</span>
        <span class="nt">&lt;location</span> <span class="na">type=</span><span class="s">"stats"</span> <span class="na">path=</span><span class="s">"/"</span><span class="nt">/&gt;</span>
        <span class="nt">&lt;location</span> <span class="na">type=</span><span class="s">"meta"</span> <span class="na">path=</span><span class="s">"/"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;/locations&gt;</span>
    <span class="nt">&lt;ACL</span> <span class="na">owner=</span><span class="s">"ambari-qa"</span> <span class="na">group=</span><span class="s">"users"</span> <span class="na">permission=</span><span class="s">"0755"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;schema</span> <span class="na">location=</span><span class="s">"/none"</span> <span class="na">provider=</span><span class="s">"/none"</span><span class="nt">/&gt;</span>
<span class="nt">&lt;/feed&gt;</span>
</code></pre>
</div>

<p><strong>NOTE : DO NOT copy the validity start and end time. Change it as per your time.</strong></p>

<p>Click <code class="highlighter-rouge">Finish</code> on the top of the XML Preview area. Save the feed.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/feedXML2.png" alt="feedXML2" /></p>

<!---
~~~
name cleansedEmailFeed"
description Cleansed customer emails"     
tag cleanse with value cleaned
Group churnAnalysisDataPipeline
Frequency 1 hour
~~~

We then set the ownership information for the Feed:

~~~
Owner:  ambari-qa
Group:  users
Permissions: 755
~~~

Set the default storage location to

~~~
/user/ambari-qa/falcon/demo/processed/enron/${YEAR}-${MONTH}-${DAY}-${HOUR}"
~~~

Select the primary cluster for the source and again set the validity start for the current time and end time to an hour or two from now.

Specify the path for the data as:

~~~
/user/ambari-qa/falcon/demo/primary/processed/enron/${YEAR}-${MONTH}-${DAY}-${HOUR}
~~~

And enter `/` for the stats and meta data locations

Set the target cluster as backupCluster and again set the validity start for the current time and end time to an hour or two from now

And specify the data path for the target to

~~~
/falcon/demo/bcp/processed/enron/${YEAR}-${MONTH}-${DAY}-${HOUR}
~~~

Set the statistics and meta data locations to `/`


![](/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.35.10.png?dl=1)  


Accept the default values and click Next

![](/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.35.49.png?dl=1)  


Accept the default values and click Next

![](/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.35.58.png?dl=1)  


On the Clusters page ensure you modify the validity to a time slice which is in the very near future and then click Next

![](/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.36.35.png?dl=1)  


![](/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.37.05.png?dl=1)  


Accept the default values and click Save

![](/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.37.21.png?dl=1)  
-->

<h2 id="define-the-cleanseemailprocess-using-wizard-a-iddefine-the-cleansedemailprocess-wizarda">Define the cleanseEmailProcess using Wizard <a id="define-the-cleansedEmailProcess-wizard"></a></h2>

<p>Now lets define the <code class="highlighter-rouge">cleanseEmailProcess</code>.
Again, to create a process entity click on the <code class="highlighter-rouge">Process</code> button on the top of the main page on the Falcon Web UI.</p>

<p><strong>NOTE : If you want to create it from XML, skip this section, and move on to the next one.</strong></p>

<p>Create this process with the following information</p>

<div class="highlighter-rouge"><pre class="highlight"><code>process name cleanseEmailProcess
</code></pre>
</div>

<p>Tag cleanse with the value yes</p>

<p>Then assign the workflow the name:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>emailCleanseWorkflow
</code></pre>
</div>

<p>Select Pig as the execution engine and provide the following path:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>/user/ambari-qa/falcon/demo/apps/pig/id.pig
</code></pre>
</div>

<p>Accept the default values for Access Control List and click <code class="highlighter-rouge">Next</code>.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/process21.png" alt="process21" /></p>

<p>For the properties, set the number of parallel processes to 1, this prevents a new instance from starting prior to the previous one completing.
Specify the order as first-in, First-out (FIFO)
And the Frequency to 1 hour.
Select the exp-backoff retry policy, then set the attempts to 3 and the delay to 3 minutes. Click <code class="highlighter-rouge">Next</code>.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/process22.png" alt="process22" /></p>

<p>On the Clusters page ensure you modify the validity to a time slice which is in the very near future and then click <code class="highlighter-rouge">Next</code>.</p>

<p><strong>NOTE : Time should be specified as per the timezone selected.</strong></p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/process23.png" alt="process23" /></p>

<p>Select the Input and Output Feeds as shown below and click Next.
Select rawEmailFeed as an input feed and cleansedEmailFeed as an output feed.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/process24.png" alt="process24" /></p>

<p>Verify all the values you have entered and then click <code class="highlighter-rouge">Save</code>.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/process25.png" alt="process25" /></p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/process26.png" alt="process26" /></p>

<!---
We then set the ownership information:

~~~
Owner:  ambari-qa
Group:  users
Permissions: 755
~~~


This job will run on the primaryCluster.

Again, set the validity to start now and end in an hour or two.



For the properties, set the number of parallel processes to 1, this prevents a new instance from starting prior to the previous one completing.

Specify the order as first-in, First-out (FIFO)

And the Frequency to 1 hour.

For inputs and output, enter the rawEmailFeed we created in the previous step and specify it as input and now(0,0) for the instance.  

Add an output using `cleansedEmailFeed` and specify now(0,0) for the instance.  

Then assign the workflow the name:

~~~
emailCleanseWorkflow
~~~

Select Pig as the execution engine and provide the following path:

~~~
/user/ambari-qa/falcon/demo/apps/pig/id.pig
~~~



![](/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.39.34.png?dl=1)  


Accept the default values and click Next

![](/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.39.53.png?dl=1)  


On the Clusters page ensure you modify the validity to a time slice which is in the very near future and then click Next

![](/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.40.24.png?dl=1)  


Select the Input and Output Feeds as shown below and Save

![](/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.40.40.png?dl=1)  
-->

<h2 id="define-the-cleanseemailprocess-using-xml-a-iddefine-the-cleansedemailprocess-xmla">Define the cleanseEmailProcess using XML <a id="define-the-cleansedEmailProcess-XML"></a></h2>

<p>Click on the <code class="highlighter-rouge">Edit</code> button over XML Preview area on the right hand side of the screen and replace the XML content with the XML document below:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="cp">&lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?&gt;</span>
<span class="nt">&lt;process</span> <span class="na">name=</span><span class="s">"cleanseEmailProcess"</span> <span class="na">xmlns=</span><span class="s">"uri:falcon:process:0.1"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;tags&gt;</span>cleanse=yes<span class="nt">&lt;/tags&gt;</span>
    <span class="nt">&lt;clusters&gt;</span>
        <span class="nt">&lt;cluster</span> <span class="na">name=</span><span class="s">"primaryCluster"</span><span class="nt">&gt;</span>
            <span class="nt">&lt;validity</span> <span class="na">start=</span><span class="s">"2016-06-05T05:00Z"</span> <span class="na">end=</span><span class="s">"2016-06-05T06:00Z"</span><span class="nt">/&gt;</span>
        <span class="nt">&lt;/cluster&gt;</span>
    <span class="nt">&lt;/clusters&gt;</span>
    <span class="nt">&lt;parallel&gt;</span>1<span class="nt">&lt;/parallel&gt;</span>
    <span class="nt">&lt;order&gt;</span>FIFO<span class="nt">&lt;/order&gt;</span>
    <span class="nt">&lt;frequency&gt;</span>hours(1)<span class="nt">&lt;/frequency&gt;</span>
    <span class="nt">&lt;timezone&gt;</span>UTC<span class="nt">&lt;/timezone&gt;</span>
    <span class="nt">&lt;inputs&gt;</span>
        <span class="nt">&lt;input</span> <span class="na">name=</span><span class="s">"input"</span> <span class="na">feed=</span><span class="s">"rawEmailFeed"</span> <span class="na">start=</span><span class="s">"now(0,0)"</span> <span class="na">end=</span><span class="s">"now(0,0)"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;/inputs&gt;</span>
    <span class="nt">&lt;outputs&gt;</span>
        <span class="nt">&lt;output</span> <span class="na">name=</span><span class="s">"output"</span> <span class="na">feed=</span><span class="s">"cleansedEmailFeed"</span> <span class="na">instance=</span><span class="s">"now(0,0)"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;/outputs&gt;</span>
    <span class="nt">&lt;workflow</span> <span class="na">name=</span><span class="s">"emailCleanseWorkflow"</span> <span class="na">version=</span><span class="s">"pig-0.13.0"</span> <span class="na">engine=</span><span class="s">"pig"</span> <span class="na">path=</span><span class="s">"/user/ambari-qa/falcon/demo/apps/pig/id.pig"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;retry</span> <span class="na">policy=</span><span class="s">"exp-backoff"</span> <span class="na">delay=</span><span class="s">"minutes(3)"</span> <span class="na">attempts=</span><span class="s">"3"</span><span class="nt">/&gt;</span>
    <span class="nt">&lt;ACL</span> <span class="na">owner=</span><span class="s">"ambari-qa"</span> <span class="na">group=</span><span class="s">"users"</span> <span class="na">permission=</span><span class="s">"0755"</span><span class="nt">/&gt;</span>
<span class="nt">&lt;/process&gt;</span>
</code></pre>
</div>

<p><strong>NOTE : DO NOT copy the validity start and end time. Change it as per your time.</strong></p>

<p>Click <code class="highlighter-rouge">Finish</code> on the top of the XML Preview area, verify all the values and save the process.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/processXML2.png" alt="processXML2" /></p>

<h2 id="run-the-feeds-a-idrun-the-feedsa">Run the feeds <a id="run-the-feeds"></a></h2>

<p>From the Falcon Web UI home page search for the Feeds we created</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.41.34.png?dl=1" alt="" /></p>

<p>Select the rawEmailFeed by clicking on the checkbox</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.41.56.png?dl=1" alt="" /></p>

<p>Then click on the Schedule button on the top of the search results</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.42.04.png?dl=1" alt="" /></p>

<p>Next run the <code class="highlighter-rouge">cleansedEmailFeed</code> in the same way</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.42.30.png?dl=1" alt="" /></p>

<h2 id="run-the-processes-a-idrun-the-processesa">Run the processes <a id="run-the-processes"></a></h2>

<p>From the Falcon Web UI home page search for the Process we created</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.42.55.png?dl=1" alt="" /></p>

<p>Select the <code class="highlighter-rouge">cleanseEmailProcess</code> by clicking on the checkbox</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.43.07.png?dl=1" alt="" /></p>

<p>Then click on the Schedule button on the top of the search results</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.43.31.png?dl=1" alt="" /></p>

<p>Next run the <code class="highlighter-rouge">rawEmailIngestProcess</code> in the same way</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.43.41.png?dl=1" alt="" /></p>

<p>If you visit the Oozie process page, you can seen the processes running</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.44.23.png?dl=1" alt="" /></p>

<h2 id="input-and-output-of-the-pipeline-a-idinput-and-output-of-the-pipelinea">Input and Output of the pipeline <a id="input-and-output-of-the-pipeline"></a></h2>

<p>Now that the feeds and processes are running, we can check the dataset being ingressed and the dataset egressed on HDFS.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2015.45.48.png?dl=1" alt="" /></p>

<p>Here is the data being ingressed</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2016.31.37.png?dl=1" alt="" /></p>

<p>and here is the data being egressed from the pipeline</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp/assets/falcon-processing-pipelines/Screenshot%202015-08-11%2017.13.05.png?dl=1" alt="" /></p>

<h2 id="summary-a-idsummarya">Summary <a id="summary"></a></h2>

<p>In this tutorial we walked through a scenario to clean the raw data to remove sensitive information like credit card numbers and make it available to our marketing data science team for customer churn analysis by defining a data pipeline with Apache Falcon. You may suspend the running feeds or processes if no longer required.</p>

</div>

<div id="tutorial-footer">
  <hr>
  <h2>Tutorial Q&amp;A and Reporting Issues</h2>
  <p>If you need help or have questions with this tutorial, please first check HCC for existing Answers to questions on this tutorial using the Find Answers button.  If you don't find your answer you can post a new HCC question for this tutorial using the Ask Questions button.</p>
  <p><a class="btn" href="https://community.hortonworks.com/topics/tutorial-330.html" role="button">Find Answers</a> <a class="btn pull-right" href="https://community.hortonworks.com/questions/ask.html?space=81&topics=tutorial-330&topics=hdp-2.4.0" role="button">Ask Questions</a></p>
  <p>Tutorial Name: <strong>Define and Process Data Pipelines in Hadoop Using Apache Falcon</strong></p>
  <p>HCC Tutorial Tag:<strong> tutorial-330</strong> and <strong>hdp-2.4.0</strong></p>
  <p>If the tutorial has multiple labs please indicate which lab your question corresponds to. Please provide any feedback related to that lab.</p>
  <p>All Hortonworks, partner and community tutorials are posted in the Hortonworks github and can be contributed via the <a href="https://github.com/hortonworks/tutorials/wiki">Hortonworks Tutorial Contribution Guide</a>.  If you are certain there is an issue or bug with the tutorial, please <a href="https://github.com/hortonworks/tutorials/wiki#issues-with-tutorials">create an issue</a> on the repository and we will do our best to resolve it!</p>
</div>
