

<div class="tutorial-content">
  <h3 id="introduction">Introduction</h3>

<p>In this tutorial, we are going to walk through the process of using Apache Zeppelin and Apache Spark to interactively analyze data on a Apache Hadoop Cluster. In particular, you will learn:</p>

<ol>
  <li>How to interact with Apache Spark from Apache Zeppelin</li>
  <li>How to read a text file from HDFS and create a RDD</li>
  <li>How to interactively analyze a data set through a rich set of Spark API operations</li>
</ol>

<h3 id="prerequisites">Prerequisites</h3>

<p>This tutorial is a part of series of hands-on tutorials to get you started with HDP using Hortonworks sandbox. Please ensure you complete the prerequisites before proceeding with this tutorial.</p>

<ul>
  <li>Downloaded and Installed latest <a href="http://hortonworks.com/products/hortonworks-sandbox/#install">Hortonworks Sandbox</a></li>
  <li><a href="http://hortonworks.com/hadoop-tutorial/learning-the-ropes-of-the-hortonworks-sandbox/">Learning the Ropes of the Hortonworks Sandbox</a></li>
</ul>

<h3 id="getting-started">Getting started</h3>

<p><strong>Spark</strong> and <strong>Zeppelin Notebook</strong> services should already be running when you start your Virtual Machine, as highlighted below in Ambari manager.</p>

<p><img src="/assets/interacting-with-data-using-zeppelin-and-spark/68747470733a2f2f7777772e676f6f676c6564726976652e636f6d2f686f73742f30427a686c4f79776e4f70713864304a50596931345456524c536c553f7261773d74727565.png" alt="" /></p>

<p>To launch <strong>Zeppelin</strong> from your browser, go to <code class="highlighter-rouge">http://&lt;host IP&gt;:9995</code>.</p>

<p>For example, the default local <strong>VirtualBox</strong> address is <a href="http://127.0.0.1:9995">http://127.0.0.1:9995</a> and the default local <strong>VmWare</strong> address is <a href="http://172.16.148.128:9995">http://172.16.148.128:9995</a>.</p>

<p><strong>Note:</strong> In local mode your <code class="highlighter-rouge">host IP</code> should be <code class="highlighter-rouge">127.0.0.1</code> for VirtualBox and <code class="highlighter-rouge">172.16.148.128</code> for VmWare, however if you are running your Sandbox in the cloud, review <a href="http://hortonworks.com/hadoop-tutorial/learning-the-ropes-of-the-hortonworks-sandbox/">Learning the Ropes</a> to learn how to determine your <code class="highlighter-rouge">host IP</code> address.</p>

<p><img src="/assets/interacting-with-data-using-zeppelin-and-spark/68747470733a2f2f7777772e676f6f676c6564726976652e636f6d2f686f73742f30427a686c4f79776e4f707138566b3556614731434f5756694e56553f7261773d74727565.png" alt="" /></p>

<p>Click on Notebook and create a new note.</p>

<p><img src="/assets/interacting-with-data-using-zeppelin-and-spark/68747470733a2f2f7777772e676f6f676c6564726976652e636f6d2f686f73742f30427a686c4f79776e4f707138595578794e6b633061336b3061306b3f7261773d74727565.png" alt="" /><br />
<img src="/assets/interacting-with-data-using-zeppelin-and-spark/68747470733a2f2f7777772e676f6f676c6564726976652e636f6d2f686f73742f30427a686c4f79776e4f707138527a4935516b52464f465a535657383f7261773d74727565.png" alt="" /></p>

<p>You can use the Shell Interpreter by adding <code class="highlighter-rouge">%sh</code> at the beginning.</p>

<p>Next, let’s save <code class="highlighter-rouge">littlelog.csv</code> in our local sandbox <code class="highlighter-rouge">/tmp</code> directory and upload it to the HFDS <code class="highlighter-rouge">/tmp</code> directory</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>%sh

<span class="c"># Save csv file in /tmp directory</span>
wget --no-check-certificate <span class="s1">'https://docs.google.com/uc?export=download&amp;id=0BzhlOywnOpq8OWFzQjJObUtlck0'</span> -O /tmp/littlelog.csv

<span class="c"># Cleanup HDFS directory</span>
hadoop fs -rm /tmp/littlelog.csv

<span class="c"># Copy to HDFS directory /tmp</span>
hadoop fs -put /tmp/littlelog.csv /tmp/
</code></pre>
</div>

<p><img src="/assets/interacting-with-data-using-zeppelin-and-spark/68747470733a2f2f7777772e676f6f676c6564726976652e636f6d2f686f73742f30427a686c4f79776e4f707138596c5a6f613039705a306c56626a673f7261773d74727565.png" alt="" /></p>

<p>Open the HDFS Files view and navigate to <code class="highlighter-rouge">/tmp</code> to verify that <code class="highlighter-rouge">littlelog.csv</code> has been uploaded correctly.</p>

<p><img src="/assets/interacting-with-data-using-zeppelin-and-spark/68747470733a2f2f7777772e676f6f676c6564726976652e636f6d2f686f73742f30427a686c4f79776e4f70713855306c6164455268527a644d6155453f7261773d74727565.png" alt="" /></p>

<p>In Spark, datasets are represented as a list of entries, where the list is broken up into many different partitions that are each stored on a different machine. Each partition holds a unique subset of the entries in the list. Spark calls datasets that it stores <strong>Resilient Distributed Datasets</strong> (RDDs).</p>

<p>So let’s create a RDD from our littlelog.csv:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">val</span> <span class="n">file</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">textFile</span><span class="o">(</span><span class="s">"hdfs://sandbox.hortonworks.com:8020/tmp/littlelog.csv"</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="/assets/interacting-with-data-using-zeppelin-and-spark/68747470733a2f2f7777772e676f6f676c6564726976652e636f6d2f686f73742f30427a686c4f79776e4f707138513142794e5652336254524851556b3f7261773d74727565.png" alt="" /></p>

<p>Now we have a freshly created RDD. We have to use an action operation like collect() to gather up the data into the driver’s memory and then to print out the contents of the file:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">file</span><span class="o">.</span><span class="na">collect</span><span class="o">().</span><span class="na">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="/assets/interacting-with-data-using-zeppelin-and-spark/68747470733a2f2f7777772e676f6f676c6564726976652e636f6d2f686f73742f30427a686c4f79776e4f7071385a32354c4c546c4f59305131616c453f7261773d74727565.png" alt="" /></p>

<p>Remember doing a <code class="highlighter-rouge">collect()</code> action operation on a very large distributed RDD can cause your driver program to run out of memory and crash. So, <strong>do not use collect()</strong> except for when you are prototyping your Spark program on a small dataset.</p>

<p>Another way to print the content of the RDD is</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">file</span><span class="o">.</span><span class="na">toArray</span><span class="o">.</span><span class="na">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="/assets/interacting-with-data-using-zeppelin-and-spark/68747470733a2f2f7777772e676f6f676c6564726976652e636f6d2f686f73742f30427a686c4f79776e4f7071385a3170754d48466b4d546c4c55306b3f7261773d74727565.png" alt="" /></p>

<p>In fact you can easily discover other methods that apply to this RDD by auto-completion.</p>

<p>Type the name of the RDD followed by a <code class="highlighter-rouge">.</code>, in our case it’s <code class="highlighter-rouge">file</code>. and then press the <code class="highlighter-rouge">crtl</code> + <code class="highlighter-rouge">-</code> + <code class="highlighter-rouge">.</code></p>

<p><img src="/assets/interacting-with-data-using-zeppelin-and-spark/68747470733a2f2f7777772e676f6f676c6564726976652e636f6d2f686f73742f30427a686c4f79776e4f7071384e6c644a63484a5a563159785654413f7261773d74727565.png" alt="" /></p>

<p>Now let’s extract some information from this data.</p>

<p>Let’s create a map where the <strong>state</strong> is the key and the <strong>number of visitors</strong> is the value.</p>

<p>Since state is the 6th element in each row of our text in littlelog.csv <em>(index 5)</em>, we need to use a map operator to pass in the lines of text to a function that will parse out the 6th element and store it in a new RDD containing two elements as the key, then count the number of times it appears in the set and provide that number as the value in the second element of this new RDD.</p>

<p>By using the Spark API operator <code class="highlighter-rouge">map</code>, we have created or transformed our original RDD into a newer one.</p>

<p>So let’s do it step by step. First let’s filter out the blank lines.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">val</span> <span class="n">fltr</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="na">filter</span><span class="o">(</span><span class="n">_</span><span class="o">.</span><span class="na">length</span> <span class="o">&gt;</span>  <span class="mi">0</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="/assets/interacting-with-data-using-zeppelin-and-spark/68747470733a2f2f7777772e676f6f676c6564726976652e636f6d2f686f73742f30427a686c4f79776e4f7071385a32354c4c546c4f59305131616c453f7261773d74727565.png" alt="" /></p>

<p>WAIT!* What is that doing there? *</p>

<p><code class="highlighter-rouge">_</code> is a shortcut or wildcard in Scala that essentially means ‘whatever happens to be passed to me’.</p>

<p>You can also write:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">val</span> <span class="n">fltr</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="na">filter</span><span class="o">(</span> <span class="n">x</span> <span class="o">=&gt;</span> <span class="n">x</span><span class="o">.</span><span class="na">length</span> <span class="o">&gt;</span>  <span class="mi">0</span> <span class="o">)</span>
</code></pre>
</div>

<p>So, in the above code the <code class="highlighter-rouge">_</code> or the <code class="highlighter-rouge">x</code> stands <strong>for each row of our file</strong> RDD: if the row length &gt; 0 is, hence not empty, then assign it to fltr which is a new RDD.</p>

<p>So, we are invoking the method length on an unknown <strong>whatever</strong> and trusting that Scala will figure out that the thing in each row of the file RDD is actually a <strong>String that supports the length</strong> operator.</p>

<p>In other words within the parenthesis of our filter method we are defining the argument: ‘whatever’, and the logic to be applied to it.</p>

<p>This pattern of constructing a function within the argument to a method is one of the <strong>fundamental characteristics</strong> of Scala and once you get used to it, it will make sense and speed up your programming a lot.</p>

<p>Then let’s split the line into individual columns separated by <code class="highlighter-rouge">,</code> and then let’s grab the 6th columns, which means the column with index 5.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">val</span> <span class="n">keys</span> <span class="o">=</span> <span class="n">fltr</span><span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="n">_</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="s">","</span><span class="o">)).</span><span class="na">map</span><span class="o">(</span><span class="n">a</span> <span class="o">=&gt;</span> <span class="n">a</span><span class="o">(</span><span class="mi">5</span><span class="o">))</span>
</code></pre>
</div>

<h5 id="lets-illustrate-the-query-above-with-an-example">Let’s illustrate the query above with an example:</h5>

<p>This is a row of the littlelog.csv file:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>20120315 01:17:06,99.122.210.248,[http://www.acme.com/SH55126545/VD55170364,<span class="o">{</span>7AAB8415-E803-3C5D-7100-E362D7F67CA7<span class="o">}</span>,homestead,fl,usa]<span class="o">(</span>http://www.acme.com/SH55126545/VD55170364,<span class="o">{</span>7AAB8415-E803-3C5D-7100-E362D7F67CA7<span class="o">}</span>,homestead,fl,usa<span class="o">)</span>
</code></pre>
</div>

<p>if we execute the query, first, <strong>each row</strong> of the fltr RDD is having the <code class="highlighter-rouge">split(“,”)</code> method called on it.<br />
It will seperate all columns with a <code class="highlighter-rouge">,</code> between them.</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="o">[</span>
  20120315 01:17:06,
  99.122.210.248,
  <span class="o">[</span>http://www.acme.com/SH55126545/VD55170364,
  <span class="o">{</span>7AAB8415-E803-3C5D-7100-E362D7F67CA7<span class="o">}</span>,
  homestead,
  fl,
  usa]<span class="o">(</span>http://www.acme.com/SH55126545/VD55170364,
  <span class="o">{</span>7AAB8415-E803-3C5D-7100-E362D7F67CA7<span class="o">}</span>,
  homestead,
  fl,
  usa<span class="o">)</span>
<span class="o">]</span>
</code></pre>
</div>

<p>This split function results in an anonymous RDD consisting of arrays like the one above.</p>

<p>The anonymous RDD is passed to the map function.</p>

<p>In this case, each array in the anonymous RDD is assigned to the variable ‘a’.</p>

<p>Then we extract the 6th element from it, which ends up being added to the named RDD called <strong>‘keys’</strong> we declared at the start of the line of code.</p>

<p><img src="/assets/interacting-with-data-using-zeppelin-and-spark/68747470733a2f2f7777772e676f6f676c6564726976652e636f6d2f686f73742f30427a686c4f79776e4f707138566c42694e3342594d5642505454513f7261773d74727565.png" alt="" /></p>

<p>Then let’s print out the values of the key.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">keys</span><span class="o">.</span><span class="na">collect</span><span class="o">().</span><span class="na">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="/assets/interacting-with-data-using-zeppelin-and-spark/68747470733a2f2f7777772e676f6f676c6564726976652e636f6d2f686f73742f30427a686c4f79776e4f7071384f565a505a5331306344566a4f474d3f7261773d74727565.png" alt="" /></p>

<p>Notice that some of the states are not unique and repeat. We need to count how many times each key (state) appears in the log.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">val</span> <span class="n">stateCnt</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="na">map</span><span class="o">(</span><span class="n">key</span> <span class="o">=&gt;</span>  <span class="o">(</span><span class="n">key</span><span class="o">,</span><span class="mi">1</span><span class="o">))</span>  <span class="c1">//print stateCnt stateCnt.toArray.foreach(println)</span>
</code></pre>
</div>

<p><img src="/assets/interacting-with-data-using-zeppelin-and-spark/68747470733a2f2f7777772e676f6f676c6564726976652e636f6d2f686f73742f30427a686c4f79776e4f707138626c4253536a6469654655794c566b3f7261773d74727565.png" alt="" /></p>

<p>You can see, how our new RDD stateCnt looks like.<code class="highlighter-rouge">RDD[(String,  Int)]</code>.<br />
The String is our key and the Integer is 1.</p>

<p>Next, we will iterate through each row of the stateCnt RDD and pass their contents to a utility method available to our RDD that counts the distinct number of rows containing each key</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">val</span> <span class="n">lastMap</span> <span class="o">=</span> <span class="n">stateCnt</span><span class="o">.</span><span class="na">countByKey</span>
</code></pre>
</div>

<p><img src="/assets/interacting-with-data-using-zeppelin-and-spark/68747470733a2f2f7777772e676f6f676c6564726976652e636f6d2f686f73742f30427a686c4f79776e4f70713856556c594d306431656e684d4c574d3f7261773d74727565.png" alt="" /></p>

<p>Now, let’s print out the result.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">lastMap</span><span class="o">.</span><span class="na">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</code></pre>
</div>

<p>Result: a listing of state abbreviations and the count of how many times visitors from that state hit our website.</p>

<p><img src="/assets/interacting-with-data-using-zeppelin-and-spark/68747470733a2f2f7777772e676f6f676c6564726976652e636f6d2f686f73742f30427a686c4f79776e4f70713862484e305a445656626b3545516a513f7261773d74727565.png" alt="" /></p>

<p>Note that at this point you still have access to all the RDDs you have created during this session. You can reprocess any one of them, for instance, again printing out the values contained in the keys RDD:</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">keys</span><span class="o">.</span><span class="na">collect</span><span class="o">().</span><span class="na">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="/assets/interacting-with-data-using-zeppelin-and-spark/68747470733a2f2f7777772e676f6f676c6564726976652e636f6d2f686f73742f30427a686c4f79776e4f707138626d6456566e4e754d5559334e484d3f7261773d74727565.png" alt="" /></p>

<p>I hope this has proved informative and that you have enjoyed this simple example of how you can interact with Data on HDP using Scala and Apache Spark.</p>

</div>

<div id="tutorial-footer">
  <hr>
  <h2>Tutorial Q&amp;A and Reporting Issues</h2>
  <p>If you need help or have questions with this tutorial, please first check HCC for existing Answers to questions on this tutorial using the Find Answers button.  If you don't find your answer you can post a new HCC question for this tutorial using the Ask Questions button.</p>
  <p><a class="btn" href="https://community.hortonworks.com/topics/tutorial-370.html" role="button">Find Answers</a> <a class="btn pull-right" href="https://community.hortonworks.com/questions/ask.html?space=81&topics=tutorial-370&topics=hdp-2.4.0" role="button">Ask Questions</a></p>
  <p>Tutorial Name: <strong>Interacting with Data on HDP using Apache Zeppelin and Apache Spark</strong></p>
  <p>HCC Tutorial Tag:<strong> tutorial-370</strong> and <strong>hdp-2.4.0</strong></p>
  <p>If the tutorial has multiple labs please indicate which lab your question corresponds to. Please provide any feedback related to that lab.</p>
  <p>All Hortonworks, partner and community tutorials are posted in the Hortonworks github and can be contributed via the <a href="https://github.com/hortonworks/tutorials/wiki">Hortonworks Tutorial Contribution Guide</a>.  If you are certain there is an issue or bug with the tutorial, please <a href="https://github.com/hortonworks/tutorials/wiki#issues-with-tutorials">create an issue</a> on the repository and we will do our best to resolve it!</p>
</div>
