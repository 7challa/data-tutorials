

<div class="tutorial-content">
  <h1 id="lab-2-hive---data-etl">Lab 2: Hive - Data ETL</h1>

<h2 id="data-manipulation-with-hive">Data manipulation with Hive</h2>

<h2 id="introduction">Introduction</h2>

<p>In this tutorial, you will be introduced to Apache(<sup>TM</sup>) Hive. In the earlier section, we covered how to load data into HDFS. So now you have <strong>geolocation</strong> and <strong>trucks</strong> files stored in HDFS as csv files. In order to use this data in Hive, we will guide you on how to create a table and how to move data into a Hive warehouse, from where it can be queried. We will analyze this data using SQL queries in Hive User Views and store it as ORC. We will also walk through Apache Tez and how a DAG is created when you specify Tez as execution engine for Hive. Let’s start..!!</p>

<h2 id="pre-requisites">Pre-Requisites</h2>

<p>The tutorial is a part of a series of hands on tutorials to get you started on HDP using the Hortonworks sandbox. Please ensure you complete the prerequisites before proceeding with this tutorial.</p>

<ul>
  <li><a href="http://hortonworks.com/hadoop-tutorial/learning-the-ropes-of-the-hortonworks-sandbox/">Learning the Ropes of the Hortonworks Sandbox</a></li>
  <li>Downloaded and Installed latest <a href="http://hortonworks.com/products/hortonworks-sandbox/#install">Hortonworks Sandbox</a></li>
  <li>Lab 1: Load sensor data into HDFS</li>
  <li>Allow yourself around <strong>one hour</strong> to complete this tutorial.</li>
</ul>

<h2 id="outline">Outline</h2>

<ul>
  <li><a href="#hive-basics">Hive basics</a></li>
  <li><a href="#use-ambari-hive-user-views">Step 2.1: Use Ambari Hive User Views</a></li>
  <li><a href="#define-a-hive-table">Step 2.2: Define a Hive Table</a></li>
  <li><a href="#load-data-hive-table">Step 2.3: Load Data into Hive Table</a></li>
  <li><a href="#define-orc-table-hive">Step 2.4: Define an ORC table in Hive</a></li>
  <li><a href="#explore-hive-settings">Step 2.5: Explore Hive Settings</a></li>
  <li><a href="#analyze-truck-data">Step 2.6: Analyze Truck Data</a></li>
  <li><a href="#summary-lab2">Summary</a></li>
  <li><a href="#suggested-readings-lab2">Suggested readings</a></li>
</ul>

<h2 id="hive-a-idhive-basicsa">Hive <a id="hive-basics"></a></h2>

<p>Hive is an SQL like query language that enables analysts familiar with SQL to run queries on large volumes of data.  Hive has three main functions: data summarization, query and analysis. Hive provides tools that enable easy data extraction, transformation and loading (ETL).</p>

<h2 id="step-21-become-familiar-with-ambari-hive-user-view-a-iduse-ambari-hive-user-viewsa">Step 2.1: Become Familiar with Ambari Hive User View <a id="use-ambari-hive-user-views"></a></h2>

<p>Apache Hive presents a relational view of data in HDFS and ensures that users need not worry about where or in what format their data is stored.  Hive can display data from RCFile format, text files, ORC, JSON, parquet,  sequence files and many of other formats in a tabular view.   Through the use of SQL you can view your data as a table and create queries like you would in an RDBMS.</p>

<p>To make it easy to interact with Hive we use a tool in the Hortonworks Sandbox called the Ambari Hive User View.   Ambari Hive User View provides an interactive interface to Hive.   We can create, edit, save and run queries, and have Hive evaluate them for us using a series of MapReduce jobs or Tez jobs.</p>

<p>Let’s now open the Ambari Hive User View and get introduced to the environment. Go to the 9 square Ambari User View icon and select <strong>Hive</strong>:</p>

<p><img src="/assets/hello-hdp/hive_view_hdp_2_4_current.png" alt="Screen Shot 2015-07-21 at 10.10.18 AM" /></p>

<p>The Ambari Hive User View looks like the following:</p>

<p><img src="/assets/hello-hdp/ambari_hive_user_view_interface_hello_hdp_concepts.png" alt="Lab2_2" /></p>

<p>Now let’s take a closer look at the SQL editing capabilities in the User View:</p>

<ol>
  <li>There are <em>five tabs</em> to interact with SQL:
    <ol>
      <li><strong>Query</strong>: This is the interface shown above and the primary interface to write, edit and execute new SQL statements</li>
      <li><strong>Saved Queries</strong>: You can save your favorite queries and quickly have access to them to rerun or edit.</li>
      <li><strong>History</strong>: This allows you to look at past queries or currently running queries to view, edit and rerun.  It also allows you to see all SQL queries you have authority to view.  For example, if you are an operator and an analyst needs help with a query, then the Hadoop operator can use the History feature to see the query that was sent from the reporting tool.</li>
      <li><strong>UDFs</strong>:  Allows you to define UDF interfaces and associated classes so you can access them from the SQL editor.</li>
      <li><strong>Upload Table</strong>: Allows you to upload your hive query tables to your preferred database and appears instantly in the Query Editor for execution.</li>
    </ol>
  </li>
  <li><strong>Database Explorer:</strong>  The Database Explorer helps you navigate your database objects.  You can either search for a database object in the Search tables dialog box, or you can navigate through Database -&gt; Table -&gt; Columns in the navigation pane.</li>
  <li>The principal pane to write and edit SQL statements. This editor includes content assist via <strong>CTRL + Space</strong> to help you build queries. Content assist helps you with SQL syntax and table objects.</li>
  <li>Once you have created your SQL statement you have 3 options:
    <ol>
      <li><strong>Execute</strong>: This runs the SQL statement.</li>
      <li><strong>Explain</strong>: This provides you a visual plan, from the Hive optimizer, of how the SQL statement will be executed.</li>
      <li><strong>Save as</strong>:  Allows you to persist your queries into your list of saved queries.</li>
      <li><strong>Kill Session</strong>: Terminates the SQL statement.</li>
    </ol>
  </li>
  <li>When the query is executed you can see the Logs or the actual query results.
    <ol>
      <li><strong>Logs:</strong> When the query is executed you can see the logs associated with the query execution.  If your query fails this is a good place to get additional information for troubleshooting.</li>
      <li><strong>Results</strong>: You can view results in sets of 50 by default.</li>
    </ol>
  </li>
  <li>There are five sliding views on the right hand side with the following capabilities, which are in context of the tab you are in:
    <ol>
      <li><strong>Query</strong>: This is the default operation,which allows you to write and edit SQL.</li>
      <li><strong>Settings</strong>:  This allows you to set properties globally or associated with an individual query.</li>
      <li><strong>Data Visualization</strong>: Allows you to visualize your numeric data through different charts.</li>
      <li><strong>Visual Explain</strong>: This will generate an explain for the query.  This will also show the progress of the query.</li>
      <li><strong>TEZ</strong>: If you use TEZ as the query execution engine then you can view the DAG associated with the query.  This integrates the TEZ User View so you can check for correctness and helps with performance tuning by visualizing the TEZ jobs associated with a SQL query.</li>
      <li><strong>Notifications</strong>: This is how to get feedback on query execution.</li>
    </ol>
  </li>
</ol>

<p>Take a few minutes to explore the various Hive User View features.</p>

<h2 id="step-22-define-a-hive-table-a-iddefine-a-hive-tablea">Step 2.2: Define a Hive Table <a id="define-a-hive-table"></a></h2>

<p>Now that you are familiar with the Hive User View, let’s create the initial staging tables for the geolocation and trucks data. In this section we will learn how to use the Ambari Hive User View to create four tables: geolocaiton_stage, trucking_stage, geolocation, trucking.  First we are going to create 2 tables to stage the data in their original csv text format and then will create two more tables where we will optimize the storage with ORC. Here is a <strong>visual representation of the Data Flow</strong>:</p>

<p><img src="/assets/hello-hdp/Lab2_31.png" alt="Lab2_3" /></p>

<h3 id="create-table-geolocationstage-for-staging-initial-load">2.2.1 Create Table geolocation_stage For Staging Initial Load</h3>

<p>Copy-and-paste the the following table DDL into the empty <strong>Worksheet</strong> of the <strong>Query Editor</strong> to define a new table named geolocation_stage:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>CREATE TABLE geolocation_stage (truckid string, driverid string, event string, latitude DOUBLE, longitude DOUBLE, city string, state string, velocity BIGINT, event_ind BIGINT, idling_ind BIGINT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
TBLPROPERTIES ("skip.header.line.count"="1");
</code></pre>
</div>

<h3 id="execute-query">2.2.2 Execute Query</h3>

<p>Click the green <strong>Execute</strong> button to run the command. If successful, you should see the <strong>Succeeded</strong> status in the <strong>Query Process Results</strong> section:</p>

<p><img src="/assets/hello-hdp/create_geolocation_stage_table_hello_hdp_lab2.png" alt="Lab2_4" /></p>

<h3 id="create-new-worksheet">2.2.3 Create New Worksheet</h3>

<p>Click the blue <strong>New Worksheet</strong> button:</p>

<p><img src="/assets/hello-hdp/Lab2_51.png" alt="Lab2_5" /></p>

<h3 id="rename-query-worksheet">2.2.4 Rename Query Worksheet</h3>

<p>Notice the tab of your new Worksheet is labeled <strong>“Worksheet (1)”</strong>. Double-click on this tab to rename the label to <strong>“trucks_stage”</strong>:</p>

<p><img src="/assets/hello-hdp/Lab2_6.png" alt="Lab2_6" /></p>

<h3 id="create-table-trucksstage-for-staging-initial-load">2.2.5 Create Table trucks_stage For Staging Initial Load</h3>

<p>Copy-and-paste the following table DDL into your <strong>trucks_stage</strong> worksheet to define a new table named trucks_stage:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>CREATE TABLE trucks_stage(driverid string, truckid string, model string, jun13_miles bigint, jun13_gas bigint, may13_miles bigint, may13_gas bigint, apr13_miles bigint, apr13_gas bigint, mar13_miles bigint, mar13_gas bigint, feb13_miles bigint, feb13_gas bigint, jan13_miles bigint, jan13_gas bigint, dec12_miles bigint, dec12_gas bigint, nov12_miles bigint, nov12_gas bigint, oct12_miles bigint, oct12_gas bigint, sep12_miles bigint, sep12_gas bigint, aug12_miles bigint, aug12_gas bigint, jul12_miles bigint, jul12_gas bigint, jun12_miles bigint, jun12_gas bigint,may12_miles bigint, may12_gas bigint, apr12_miles bigint, apr12_gas bigint, mar12_miles bigint, mar12_gas bigint, feb12_miles bigint, feb12_gas bigint, jan12_miles bigint, jan12_gas bigint, dec11_miles bigint, dec11_gas bigint, nov11_miles bigint, nov11_gas bigint, oct11_miles bigint, oct11_gas bigint, sep11_miles bigint, sep11_gas bigint, aug11_miles bigint, aug11_gas bigint, jul11_miles bigint, jul11_gas bigint, jun11_miles bigint, jun11_gas bigint, may11_miles bigint, may11_gas bigint, apr11_miles bigint, apr11_gas bigint, mar11_miles bigint, mar11_gas bigint, feb11_miles bigint, feb11_gas bigint, jan11_miles bigint, jan11_gas bigint, dec10_miles bigint, dec10_gas bigint, nov10_miles bigint, nov10_gas bigint, oct10_miles bigint, oct10_gas bigint, sep10_miles bigint, sep10_gas bigint, aug10_miles bigint, aug10_gas bigint, jul10_miles bigint, jul10_gas bigint, jun10_miles bigint, jun10_gas bigint, may10_miles bigint, may10_gas bigint, apr10_miles bigint, apr10_gas bigint, mar10_miles bigint, mar10_gas bigint, feb10_miles bigint, feb10_gas bigint, jan10_miles bigint, jan10_gas bigint, dec09_miles bigint, dec09_gas bigint, nov09_miles bigint, nov09_gas bigint, oct09_miles bigint, oct09_gas bigint, sep09_miles bigint, sep09_gas bigint, aug09_miles bigint, aug09_gas bigint, jul09_miles bigint, jul09_gas bigint, jun09_miles bigint, jun09_gas bigint, may09_miles bigint, may09_gas bigint, apr09_miles bigint, apr09_gas bigint, mar09_miles bigint, mar09_gas bigint, feb09_miles bigint, feb09_gas bigint, jan09_miles bigint, jan09_gas bigint)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
TBLPROPERTIES ("skip.header.line.count"="1");
</code></pre>
</div>

<p><img src="/assets/hello-hdp/create_trucks_stage_hello_hdp_lab2.png" alt="create_trucks_stage_table" /></p>

<h3 id="execute-the-query-and-verify-it-runs-successfully">2.2.6 Execute the Query and Verify it Runs Successfully</h3>

<p>Let’s review some aspects of the <strong>CREATE TABLE</strong> statements issued above.  If you have an SQL background this statement should seem very familiar except for the last 3 lines after the columns definition:</p>

<ul>
  <li>The <strong>ROW FORMAT</strong> clause specifies each row is terminated by the new line character.</li>
  <li>The <strong>FIELDS TERMINATED</strong> BY clause specifies that the fields associated with the table (in our case, the two csv files) are to be delimited by a comma.</li>
  <li>The <strong>STORED AS</strong> clause specifies that the table will be stored in the TEXTFILE format.</li>
</ul>

<p>For details on these clauses consult the <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL">Apache Hive Language Manual</a>.</p>

<h3 id="verify-new-tables-exist">2.2.7 Verify New Tables Exist</h3>

<p>To verify the tables were defined successfully, click the <strong>“refresh”</strong> icon in the Database Explorer. Under Databases, click default database to expand the list of table and the new tables should appear:</p>

<p><img src="/assets/hello-hdp/Lab2_7.png" alt="Lab2_7" /></p>

<h3 id="view-trucksstage-schema">2.2.8 View trucks_stage Schema</h3>

<p>Click on the <strong>trucks_stage</strong> table name to view its schema.</p>

<h3 id="load-sample-data-of-trucksstage">2.2.9 Load Sample Data of trucks_stage</h3>

<p>Click on the <strong>Load sample data</strong> icon to generate and execute a select SQL statement to query the table for a 100 rows. Notice your two new tables are currently empty.</p>

<ul>
  <li>
    <p>You can have multiple SQL statements within each editor worksheet, but each statement needs to be separated by a semicolon <strong>“;”</strong>.</p>
  </li>
  <li>
    <p>If you have multiple statements within a worksheet but you only want to run one of them just highlight the statement you want to run and then click the Execute button.</p>
  </li>
</ul>

<p><strong>A few additional commands to explore tables:</strong></p>

<ul>
  <li><code class="highlighter-rouge">show tables;</code> List the tables created in the database by looking up the list of tables from the metadata stored in HCatalogdescribe
-<code class="highlighter-rouge"><span class="p">{</span><span class="err">table_name</span><span class="p">}</span></code>;Provides a list of columns for a particular table (ie <code class="highlighter-rouge">describe geolocation_stage;</code>)</li>
  <li><code class="highlighter-rouge">show create {table_name};</code>Provides the DDL to recreate a table (ie <code class="highlighter-rouge">show create table geolocation_stage;</code>)</li>
</ul>

<p> By default, when you create a table in Hive, a directory with the same name gets created in the /apps/hive/warehouse folder in HDFS.  Using the Ambari Files User View, navigate to the /apps/hive/warehouse folder. You should see both a geolocation_stage and trucks_stage directory:</p>

<p><img src="/assets/hello-hdp/geolocation_tables_created_hive_warehouse_hello_hdp_lab2.png" alt="Lab2_8" /></p>

<ul>
  <li>The definition of a Hive table and its associated metadata (i.e., the directory the data is stored in, the file format, what Hive properties are set, etc.) are stored in the Hive metastore, which on the Sandbox is a MySQL database.</li>
</ul>

<h2 id="step-23-load-data-into-a-hive-table-a-idload-data-hive-tablea">Step 2.3: Load Data into a Hive table <a id="load-data-hive-table"></a></h2>

<h3 id="manual-approach-populate-hive-table-with-data">2.3.1 Manual Approach: Populate Hive Table with Data</h3>

<p>Let’s load some data into your two Hive tables. Populating a Hive table can be done in various ways. A simple way to populate a table is to put a file into the directory associated with the table. Using the Ambari Files User View, click on the <strong>Move</strong> icon next to the file <code class="highlighter-rouge">/tmp/maria_dev/data/geolocation.csv</code>. (Clicking on <strong>Move</strong> is similar to “cut” in cut-and-paste.)</p>

<p><img src="/assets/hello-hdp/move_geolocation_csv_file_hello_hdp_lab2.png" alt="Screen Shot 2015-07-27 at 9.45.11 PM" /></p>

<h4 id="after-clicking-on-the-move-arrow-your-screen-should-look-like-the-following">2.3.1.1 After clicking on the Move arrow, your screen should look like the following:</h4>

<p><img src="/assets/hello-hdp/cut_paste_geolocation_csv_file_hello_hdp_lab2.png" alt="Lab2_10" /></p>

<h4 id="notice-two-things-have-changed">2.3.1.2 Notice two things have changed:</h4>

<ol>
  <li>The file name geolocation.csv is grayed out.</li>
  <li>The icons associated with the operations on the files are removed. This is to indicate that this file is in a special state that is ready to be moved.</li>
</ol>

<h4 id="navigate-to-destination-path-and-paste-file">2.3.1.3 Navigate to Destination Path and Paste File</h4>

<p>Now navigate to the destination path /apps/hive/warehouse/geolocation_stage.  You might notice that as you navigate through the directories that the file is pinned at the top.  Once you get to the appropriate directory click on the <strong>Paste</strong> icon to move the file:</p>

<p><img src="/assets/hello-hdp/paste_geolocation_to_warehouse_hello_hdp_lab2.png" alt="Lab2_11" /></p>

<h4 id="load-sample-data-of-geolocationstage">2.3.1.4 Load Sample Data of geolocation_stage</h4>

<p>Go back to the Ambari Hive View and click on the <strong>Load sample data</strong> icon next to the geolocation_stage table. Notice the table is no longer empty, and you should see the first 100 rows of the table:</p>

<p><img src="/assets/hello-hdp/Lab2_12.png" alt="Lab2_12" /></p>

<h3 id="automatic-approach-populate-hive-table-with-data">2.3.2 Automatic Approach: Populate Hive Table with Data</h3>

<p>Enter the following SQL command into an empty Worksheet in the Ambari Hive User View:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>LOAD DATA INPATH '/tmp/maria_dev/data/trucks.csv' OVERWRITE INTO TABLE trucks_stage;
</code></pre>
</div>

<h4 id="load-sample-data-of-trucksstage-1">2.3.2.1 Load Sample Data of trucks_stage</h4>

<p>You should now see data in the trucks_stage table:</p>

<p><img src="/assets/hello-hdp/Lab2_13.png" alt="Lab2_13" /></p>

<h4 id="click-on-hdfs-files-view">2.3.2.2 Click on HDFS Files View</h4>

<p>Navigate to the <code class="highlighter-rouge">/tmp/maria_dev/data</code> folder. Notice the folder is empty! The <strong>LOAD DATA INPATH</strong> command moved the <code class="highlighter-rouge">trucks.csv</code> file from the <code class="highlighter-rouge">/user/maria_dev/data</code> folder to the <code class="highlighter-rouge">/apps/hive/warehouse/trucks_stage</code> folder.</p>

<h2 id="step-24-define-an-orc-table-in-hive-a-iddefine-orc-table-hivea">Step 2.4: Define an ORC Table in Hive <a id="define-orc-table-hive"></a></h2>

<p><strong>Introducing</strong> <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC"><strong>Apache ORC</strong></a></p>

<p>The Optimized Row Columnar (<a href="http://hortonworks.com/blog/apache-orc-launches-as-a-top-level-project/">new Apache ORC project</a>) file format provides a highly efficient way to store Hive data. It was designed to overcome limitations of the other Hive file formats. Using ORC files improves performance when Hive is reading, writing, and processing data.</p>

<p>To use the ORC format, specify ORC as the file format when creating the table:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>CREATE TABLE … **STORED AS ORC**
</code></pre>
</div>

<p>In this step, you will create two ORC tables (geolocation and trucks) that are created from the text data in your geolocation_stage and trucks_stage tables.</p>

<h3 id="create-table-geolocation-as-orc-from-geolocationstage-table">2.4.1 Create Table geolocation as ORC From geolocation_stage Table</h3>

<p>From the Ambari Hive User View, execute the following table DDL to define a new table named geolocation:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>CREATE TABLE geolocation STORED AS ORC AS SELECT * FROM geolocation_stage;
</code></pre>
</div>

<h3 id="verify-table-geolocation-is-in-default-database">2.4.2 Verify table geolocation is in default database</h3>

<p>Refresh the <strong>Database Explorer</strong> and verify you have a table named geolocation in the default database:</p>

<p><img src="/assets/hello-hdp/Lab2_14.png" alt="Lab2_14" /></p>

<p>1) View the contents of the geolocation table and notice it contains the same rows as geolocation_stage.</p>

<p>2) Verify geolocation is an ORC Table, execute the following query:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>describe formatted geolocation;
</code></pre>
</div>

<p>3) Scroll down to the bottom of the <strong>Results</strong> tab and you will see a section labeled <strong>Storage Information</strong>. The output should look like:</p>

<p><img src="/assets/hello-hdp/Lab2_15.png" alt="Lab2_15" /></p>

<h3 id="create-table-trucks-as-orc-from-trucksstage-table">2.4.3 Create Table trucks As ORC From trucks_stage Table</h3>

<p>Execute the following query to define a new ORC table named trucks that contains the data from trucks_stage:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>CREATE TABLE trucks STORED AS ORC TBLPROPERTIES ("orc.compress.size"="1024") AS SELECT * FROM trucks_stage;
</code></pre>
</div>

<h3 id="verify-table-was-properly-created">2.4.4 Verify Table was Properly Created</h3>

<p>Refresh the <strong>Database Explorer</strong> and view the contents of trucks:</p>

<p><img src="/assets/hello-hdp/Lab2_16.png" alt="Lab2_16" /></p>

<h3 id="enter-hive-shell">2.4.5 Enter Hive Shell</h3>

<p>If you want to try running some of these commands from the Hive Shell, follow the following steps from your terminal shell (or putty if using Windows):</p>

<p>i.  <code class="highlighter-rouge">ssh root@127.0.0.1 -p 2222</code> Root password is hadoop
    1. (for azure users enter <code class="highlighter-rouge">ssh &lt;_username_&gt;@&lt;_ipaddress_&gt; -p &lt;_port_&gt;</code>). Username is the name you gave your sandbox, and ip address is located on the dashboard.
ii.  <code class="highlighter-rouge">su hive</code><br />
iii.  <code class="highlighter-rouge">hive</code> Starts Hive shell and now you can enter commands and SQL<br />
iv.  <code class="highlighter-rouge">quit;</code> Exits out of the Hive shell.</p>

<h2 id="step-25-explore-hive-settings-a-idexplore-hive-settingsa">Step 2.5: Explore Hive Settings <a id="explore-hive-settings"></a></h2>

<h3 id="open-ambari-dashboard-in-new-tab">2.5.1 Open Ambari Dashboard in New Tab</h3>

<p>Open the Ambari Dashboard in another tab by right clicking on the Ambari icon:</p>

<p><img src="/assets/hello-hdp/Lab2_17.png" alt="Lab2_17" /></p>

<h3 id="become-familiar-with-hive-settings">2.5.2 Become Familiar with Hive Settings</h3>

<p>Go to the <strong>Hive page</strong> then select the <strong>Configs tab</strong> then click on <strong>Settings tab</strong>:</p>

<p><img src="/assets/hello-hdp/hive_settings_hello_hdp_lab2.png" alt="Lab2_18" /></p>

<p>Once you click on the Hive page you should see a page similar to above:</p>

<ol>
  <li><strong>Hive</strong> Page</li>
  <li>Hive <strong>Configs</strong> Tab</li>
  <li>Hive <strong>Settings</strong> Tab</li>
  <li>Version <strong>History</strong> of Configuration</li>
</ol>

<p>Scroll down to the <strong>Optimization Settings</strong>:</p>

<p><img src="/assets/hello-hdp/hive_optimization_settings_hello_hdp_lab2.png" alt="Lab2_19" /></p>

<p>In the above screenshot we can see:</p>

<ol>
  <li><strong>Tez</strong> is set as the optimization engine</li>
  <li><strong>Cost Based Optimizer</strong> (CBO) is turned on</li>
</ol>

<p>This shows the new HDP 2.4 <strong>Ambari Smart Configurations</strong>, which simplifies setting configurations</p>

<ul>
  <li>Hadoop is configured by a <strong>collection of XML files</strong>.</li>
  <li>In early versions of Hadoop, operators would need to do <strong>XML editing</strong> to <strong>change settings</strong>.  There was no default versioning.</li>
  <li>Early Ambari interfaces made it <strong>easier to change values</strong> by showing the settings page with <strong>dialog boxes</strong> for the various settings and allowing you to edit them.  However, you needed to know what needed to go into the field and understand the range of values.</li>
  <li>Now with Smart Configurations you can <strong>toggle binary features</strong> and use the slider bars with settings that have ranges.</li>
</ul>

<p>By default the key configurations are displayed on the first page.  If the setting you are looking for is not on this page you can find additional settings in the <strong>Advanced</strong> tab:</p>

<p><img src="/assets/hello-hdp/hive_advanced_settings.png" alt="Lab2_20" /></p>

<p>For example, if we wanted to <strong>improve SQL performance</strong>, we can use the new <strong>Hive vectorization features</strong>. These settings can be found and enabled by following these steps:</p>

<ol>
  <li>Click on the <strong>Advanced</strong> tab and scroll to find the <strong>property</strong></li>
  <li>Or, start typing in the property into the property search field and then this would filter the setting you scroll for.</li>
</ol>

<p>As you can see from the green circle above, the <code class="highlighter-rouge">Enable Vectorization and Map Vectorization</code> is turned on already.</p>

<p>Some <strong>key resources</strong> to <strong>learn more about vectorization</strong> and some of the <strong>key settings in Hive tuning:</strong></p>

<ul>
  <li>Apache Hive docs on <a href="https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution">Vectorized Query Execution</a></li>
  <li><a href="http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.9.0/bk_dataintegration/content/ch_using-hive-1a.html">HDP Docs Vectorization docs</a></li>
  <li><a href="http://hortonworks.com/blog/category/hive/">Hive Blogs</a></li>
  <li><a href="http://hortonworks.com/blog/5-ways-make-hive-queries-run-faster/">5 Ways to Make Your Hive Queries Run Faster</a></li>
  <li><a href="http://hortonworks.com/hadoop-tutorial/supercharging-interactive-queries-hive-tez/">Interactive Query for Hadoop with Apache Hive on Apache Tez</a></li>
  <li><a href="http://hortonworks.com/blog/evaluating-hive-with-tez-as-a-fast-query-engine/">Evaluating Hive with Tez as a Fast Query Engine</a></li>
</ul>

<h2 id="step-26-analyze-the-trucks-data-a-idanalyze-truck-dataa">Step 2.6: Analyze the Trucks Data <a id="analyze-truck-data"></a></h2>

<p>Next we will be using Hive, Pig and Excel to analyze derived data from the geolocation and trucks tables.  The business objective is to better understand the risk the company is under from fatigue of drivers, over-used trucks, and the impact of various trucking events on risk.   In order to accomplish this, we will apply a series of transformations to the source data, mostly though SQL, and use Pig or Spark to calculate risk.   In the last 3 labs on Data Visualization, we will be using <em>Microsoft Excel, Zeppelin or Zoomdata</em> to <strong>generate a series of charts to better understand risk</strong>.</p>

<p><img src="/assets/hello-hdp/Lab2_211.png" alt="Lab2_21" /></p>

<p>Let’s get started with the first transformation.   We want to <strong>calculate the miles per gallon for each truck</strong>. We will start with our <em>truck data table</em>.  We need to <em>sum up all the miles and gas columns on a per truck basis</em>. Hive has a series of functions that can be used to reformat a table. The keyword <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LateralView">LATERAL VIEW</a> is how we invoke things. The <strong>stack function</strong> allows us to <em>restructure the data into 3 columns</em> labeled rdate, gas and mile (ex: ‘june13’, june13_miles, june13_gas) that make up a maximum of 54 rows. We pick truckid, driverid, rdate, miles, gas from our original table and add a calculated column for mpg (miles/gas).  And then we will <strong>calculate average mileage</strong>.</p>

<h3 id="create-table-truckmileage-from-existing-trucking-data">2.6.1 Create Table truck_mileage From Existing Trucking Data</h3>

<p>Using the Ambari Hive User View, execute the following query:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>CREATE TABLE truck_mileage STORED AS ORC AS SELECT truckid, driverid, rdate, miles, gas, miles / gas mpg FROM trucks LATERAL VIEW stack(54, 'jun13',jun13_miles,jun13_gas,'may13',may13_miles,may13_gas,'apr13',apr13_miles,apr13_gas,'mar13',mar13_miles,mar13_gas,'feb13',feb13_miles,feb13_gas,'jan13',jan13_miles,jan13_gas,'dec12',dec12_miles,dec12_gas,'nov12',nov12_miles,nov12_gas,'oct12',oct12_miles,oct12_gas,'sep12',sep12_miles,sep12_gas,'aug12',aug12_miles,aug12_gas,'jul12',jul12_miles,jul12_gas,'jun12',jun12_miles,jun12_gas,'may12',may12_miles,may12_gas,'apr12',apr12_miles,apr12_gas,'mar12',mar12_miles,mar12_gas,'feb12',feb12_miles,feb12_gas,'jan12',jan12_miles,jan12_gas,'dec11',dec11_miles,dec11_gas,'nov11',nov11_miles,nov11_gas,'oct11',oct11_miles,oct11_gas,'sep11',sep11_miles,sep11_gas,'aug11',aug11_miles,aug11_gas,'jul11',jul11_miles,jul11_gas,'jun11',jun11_miles,jun11_gas,'may11',may11_miles,may11_gas,'apr11',apr11_miles,apr11_gas,'mar11',mar11_miles,mar11_gas,'feb11',feb11_miles,feb11_gas,'jan11',jan11_miles,jan11_gas,'dec10',dec10_miles,dec10_gas,'nov10',nov10_miles,nov10_gas,'oct10',oct10_miles,oct10_gas,'sep10',sep10_miles,sep10_gas,'aug10',aug10_miles,aug10_gas,'jul10',jul10_miles,jul10_gas,'jun10',jun10_miles,jun10_gas,'may10',may10_miles,may10_gas,'apr10',apr10_miles,apr10_gas,'mar10',mar10_miles,mar10_gas,'feb10',feb10_miles,feb10_gas,'jan10',jan10_miles,jan10_gas,'dec09',dec09_miles,dec09_gas,'nov09',nov09_miles,nov09_gas,'oct09',oct09_miles,oct09_gas,'sep09',sep09_miles,sep09_gas,'aug09',aug09_miles,aug09_gas,'jul09',jul09_miles,jul09_gas,'jun09',jun09_miles,jun09_gas,'may09',may09_miles,may09_gas,'apr09',apr09_miles,apr09_gas,'mar09',mar09_miles,mar09_gas,'feb09',feb09_miles,feb09_gas,'jan09',jan09_miles,jan09_gas ) dummyalias AS rdate, miles, gas;
</code></pre>
</div>

<p><img src="/assets/hello-hdp/create_truck_mileage_table_hello_hdp_lab2.png" alt="Lab2_22" /></p>

<h3 id="load-sample-data-of-truckmileage">2.6.2 Load Sample Data of truck_mileage</h3>

<p>To view the data generated by the script, click <strong>Load Sample Data</strong> icon in the Database Explorer next to truck_mileage. After clicking the next button once, you should see a table that <em>lists each trip made by a truck and driver</em>:</p>

<p><img src="/assets/hello-hdp/Lab2_23.png" alt="Lab2_23" /></p>

<h3 id="use-the-content-assist-to-build-a-query">2.6.3 Use the Content Assist to build a query</h3>

<p>1.  Create a new <strong>SQL Worksheet</strong>.</p>

<p>2.  Start typing in the <strong>SELECT SQL command</strong>, but only enter the first two letters:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>SE
</code></pre>
</div>

<p>3.  Press <strong>Ctrl+space</strong> to view the following content assist pop-up dialog window:</p>

<p><img src="/assets/hello-hdp/Lab2_24.png" alt="Lab2_24" /></p>

<p>Notice content assist shows you some options that start with an “SE”. These shortcuts will be great for when you write a lot of custom query code.</p>

<p>4. If you have created your Sandbox in Microsoft Azure, you have to make two changes in configuration. Otherwise, skip this step and move on to next one.</p>

<p>Go to the Services–&gt;Hive page, click on “Configs” tab, and make the following changes:</p>

<p>i) Scroll down to Optimization section, change Tez Container Size, increasing from 200 to 682(As recommended by Ambari) Param: “hive.tez.container.size” Value: 682</p>

<p><img src="/assets/hello-hdp/config_change1.png" alt="config_change1" /></p>

<p>ii) Click on “Advanced” tab to show extra settings, scroll down to find parameter “hive.tez.java.opts”, and change Hive-Tez Java Opts by increasing Java Heap Max size from 200MB to 512MB: Param: “hive.tez.java.opts” Value: “-server -Xmx512m -Djava.net.preferIPv4Stack=true”</p>

<p><img src="/assets/hello-hdp/config_change2.png" alt="config_change2" /></p>

<p>Click on Save after making changes and add a note on prompt.</p>

<p><img src="/assets/hello-hdp/config_change3.png" alt="config_change3" /></p>

<p>You will be prompted to restart some service components. Click on Restart drop down menu and select Restart All.</p>

<p><img src="/assets/hello-hdp/config_change4.png" alt="config_change4" /></p>

<p>Restart other services as well.</p>

<p>5.  Type in the following query, using <strong>Ctrl+space</strong> throughout your typing so that you can get an idea of what content assist can do and how it works:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>SELECT truckid, avg(mpg) avgmpg FROM truck_mileage GROUP BY truckid;
</code></pre>
</div>

<p><img src="/assets/hello-hdp/Lab2_28.png" alt="Lab2_28" /></p>

<p>6.  Click the “<strong>Save as …</strong>” button to save the query as “<strong>average mpg</strong>”:</p>

<p><img src="/assets/hello-hdp/Lab2_26.png" alt="Lab2_26" /></p>

<p>7.  Notice your query now shows up in the list of “<strong>Saved Queries</strong>”, which is one of the tabs at the top of the Hive User View.</p>

<p>8.  Execute the “<strong>average mpg</strong>” query and view its results.</p>

<h3 id="explore-explain-features-of-the-hive-query-editor">2.6.4 Explore Explain Features of the Hive Query Editor</h3>

<p>1. Now let’s <strong>explore the various explain features</strong> to better <em>understand the execution of a query</em>: Text Explain, Visual Explain and Tez Explain. Click on the <strong>Explain</strong> button:</p>

<p><img src="/assets/hello-hdp/Lab2_27.png" alt="Lab2_27" /></p>

<p>2. Add the <code class="highlighter-rouge">EXPLAIN</code> command at the beginning of the query:</p>

<p><img src="/assets/hello-hdp/Lab2_25.png" alt="Lab2_25" /></p>

<p>3. Execute the query. An alternative way to execute explain results is to press the <code class="highlighter-rouge">Explain</code> button. The results should look like the following:</p>

<p><img src="/assets/hello-hdp/query_results_from_explain_hello_hdp_lab2.png" alt="Lab2_29" /></p>

<p>4. Click on <strong>STAGE-0:</strong> to view its output, which displays the flow of the resulting Tez job:</p>

<p><img src="/assets/hello-hdp/stage_0_explain_command_hello_hdp_lab2.png" alt="Lab2_30" /></p>

<p>5. To see the Visual Explain, click on the <strong>Visual Explain icon</strong> on the right tabs. This is a much more readable summary of the explain plan:</p>

<p><img src="/assets/hello-hdp/Lab2_311.png" alt="Lab2_31" /></p>

<h3 id="explore-tez">2.6.5 Explore TEZ</h3>

<p>1. If you click on <strong>TEZ View</strong> from Ambari Views at the top, you can see <em>DAG details</em> associated with the previous hive and pig jobs.</p>

<p><img src="/assets/hello-hdp/tez_view_hello_hdp_lab2.png" alt="tez_view" /></p>

<p>2. Select the first DAG as it represents the last job that was executed.</p>

<p><img src="/assets/hello-hdp/last_hive_job_dags_hello_hdp_lab2.png" alt="all_dags" /></p>

<p>3. There are six tabs at the top right please take a few minutes to explore the various tabs and then click on the <strong>Graphical View</strong> tab and hover over one of the nodes with your cursor to get more details on the processing in that node.</p>

<p><img src="/assets/hello-hdp/tez_graphical_view_hello_hdp_lab2.png" alt="Lab2_35" /></p>

<p>4. Go back to the Hive UV and save the query by clicking the <strong>Save as …</strong> button.</p>

<h3 id="create-table-truck-avgmileage-from-existing-trucksmileage-data">2.6.6 Create Table truck avg_mileage From Existing trucks_mileage Data</h3>

<p><strong>Persist these results into a table</strong>, this is a fairly common pattern in Hive and it is called <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-CreateTableAsSelect(CTAS)">Create Table As Select</a> (CTAS ).  Paste the following script into a new Worksheet, then click the <strong>Execute</strong> button:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>CREATE TABLE avg_mileage
STORED AS ORC
AS
SELECT truckid, avg(mpg) avgmpg
FROM truck_mileage
GROUP BY truckid;
</code></pre>
</div>

<p><img src="/assets/hello-hdp/average_mile_table_hello_hdp_lab2.png" alt="average_mile_table_query" /></p>

<h3 id="load-sample-data-of-avgmileage">2.6.7 Load Sample Data of avg_mileage</h3>

<p>To view the data generated by the script, click <strong>Load sample data</strong> icon in the Database Explorer next to avg_mileage. You see our table is now a list of each trip made by a truck.</p>

<p><img src="/assets/hello-hdp/avg_mileage_table_results_hello_hdp_lab2.png" alt="results_avg_mileage_table" /></p>

<h2 id="summary-a-idsummary-lab2a">Summary <a id="summary-lab2"></a></h2>
<p>Congratulations! Let’s summarize some Hive commands we learned to process, filter and manipulate the geolocation and trucks data.
We now can create Hive tables with <strong>CREATE TABLE</strong> and load data into them using the <strong>LOAD DATA INPATH</strong> command. Additionally, we learned how to change the file format of the tables to ORC, so hive is more efficient at reading, writing and processing this data. We learned to grab parameters from our existing table using <strong>SELECT {column_name…} FROM {table_name}</strong> to create a new filtered table.</p>

<h2 id="suggested-readings-a-idsuggested-readings-lab2a">Suggested Readings <a id="suggested-readings-lab2"></a></h2>

<p>Augment your hive foundation with the following resources:</p>

<ul>
  <li><a href="http://hortonworks.com/hadoop/hive/">Apache Hive</a></li>
  <li><a href="http://www.amazon.com/Programming-Hive-Edward-Capriolo/dp/1449319335/ref=sr_1_3?ie=UTF8&amp;qid=1456009871&amp;sr=8-3&amp;keywords=apache+hive">Programming Hive</a></li>
  <li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL">Hive Language Manual</a></li>
  <li><a href="http://hortonworks.com/training/class/hadoop-2-data-analysis-pig-hive/">HDP DEVELOPER: APACHE PIG AND HIVE</a></li>
</ul>

</div>

<div id="tutorial-footer">
  <hr>
  <h2>Tutorial Q&amp;A and Reporting Issues</h2>
  <p>If you need help or have questions with this tutorial, please first check HCC for existing Answers to questions on this tutorial using the Find Answers button.  If you don't find your answer you can post a new HCC question for this tutorial using the Ask Questions button.</p>
  <p><a class="btn" href="https://community.hortonworks.com/topics/tutorial-100.html" role="button">Find Answers</a> <a class="btn pull-right" href="https://community.hortonworks.com/questions/ask.html?space=81&topics=tutorial-100&topics=hdp-2.4.0" role="button">Ask Questions</a></p>
  <p>Tutorial Name: <strong>Hello HDP An Introduction to Hadoop with Hive and Pig</strong></p>
  <p>HCC Tutorial Tag:<strong> tutorial-100</strong> and <strong>hdp-2.4.0</strong></p>
  <p>If the tutorial has multiple labs please indicate which lab your question corresponds to. Please provide any feedback related to that lab.</p>
  <p>All Hortonworks, partner and community tutorials are posted in the Hortonworks github and can be contributed via the <a href="https://github.com/hortonworks/tutorials/wiki">Hortonworks Tutorial Contribution Guide</a>.  If you are certain there is an issue or bug with the tutorial, please <a href="https://github.com/hortonworks/tutorials/wiki#issues-with-tutorials">create an issue</a> on the repository and we will do our best to resolve it!</p>
</div>
