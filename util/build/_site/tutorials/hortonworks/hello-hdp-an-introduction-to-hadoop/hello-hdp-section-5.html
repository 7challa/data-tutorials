

<div class="tutorial-content">
  <h1 id="lab-3-pig---risk-factor">Lab 3: Pig - Risk Factor</h1>

<h2 id="use-pig-to-compute-driver-risk-factor">Use Pig to compute Driver Risk Factor</h2>

<h2 id="introduction">Introduction</h2>

<p>In this tutorial, you will be introduced to <a href="http://hortonworks.com/hadoop/pig/">Apache Pig</a>. In the earlier section of lab, you learned how to load data into HDFS and then manipulate it using Hive. We are using the Truck sensor data to better understand risk associated with every driver. This section will teach you to <strong>compute risk using Apache Pig</strong>.</p>

<h2 id="pre-requisites">Pre-Requisites</h2>

<p>The tutorial is a part of series of hands on tutorial to get you started on HDP using Hortonworks sandbox. Please ensure you complete the prerequisites before proceeding with this tutorial.</p>

<ul>
  <li>Hortonworks Sandbox</li>
  <li><a href="http://hortonworks.com/hadoop-tutorial/learning-the-ropes-of-the-hortonworks-sandbox/">Learning the Ropes of the Hortonworks Sandbox</a></li>
  <li>Lab 1: Load sensor data into HDFS</li>
  <li>Lab 2: Data Manipulation with Apache Hive</li>
  <li>Allow yourself around <strong>one hour</strong> to complete this tutorial.</li>
</ul>

<h2 id="outline">Outline</h2>

<ul>
  <li><a href="#pig-basics">Pig Basics</a></li>
  <li><a href="#step3.2">Step 3.1: Create Pig Script</a></li>
  <li><a href="#step3.3">Step 3.2: Quick Recap</a></li>
  <li><a href="#step3.4">Step 3.3: Execute Pig Script on Tez</a></li>
  <li><a href="#summary-lab3">Summary</a></li>
  <li><a href="#suggested-readings-lab3">Suggested Readings</a></li>
</ul>

<h2 id="pig-basics-">Pig Basics <a id="pig-basics"></a></h2>

<p>Pig is a <strong>high-level scripting language</strong> used with Apache Hadoop. Pig enables data workers to <strong>write complex data transformations</strong> without knowing Java. Pig's <em>simple SQL-like scripting language</em> is called Pig Latin, and appeals to developers already familiar with scripting languages and SQL.</p>

<p>Pig is complete, so you can do all required data manipulations in Apache Hadoop with Pig. Through the <strong>User Defined Functions</strong>(UDF) facility in Pig, Pig can invoke code in many languages like <em>JRuby, Jython and Java</em>. You can also embed Pig scripts in other languages. The result is that you can use Pig as a component to build larger and more complex applications that tackle real business problems.</p>

<p>Pig works with data from many sources, including <strong>structured and unstructured data</strong>, and store the results into the Hadoop Data File System.</p>

<p>Pig scripts are <strong>translated into a series of MapReduce jobs</strong> that are <strong>run on the Apache Hadoop cluster</strong>.</p>

<h3 id="create-table-riskfactor-from-existing-trucks_mileage-data">Create Table riskfactor from Existing trucks_mileage Data</h3>

<p>Next, you will use Pig to compute the risk factor of each driver. <strong>Before we can run the Pig code</strong>, the <em>table must already exist in Hive</em> to satisfy one of the <em>requirements for the HCatStorer() class</em>. The Pig code expects the following structure for a table named <strong>riskfactor</strong>. Execute the following DDL command:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>CREATE TABLE riskfactor (driverid string,events bigint,totmiles bigint,riskfactor float)
STORED AS ORC;
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/riskfactor_lab3.png" alt="riskfactor_table" /></p>

<h3 id="verify-table-riskfactor-was-created-successfully">Verify Table riskfactor was Created Successfully</h3>

<p>Verify the riskfactor table was created successfully. It will be empty now, but you will populate it from a Pig script. You are now ready to compute the risk factor using Pig. Let's take a look at Pig and how to execute Pig scripts from within Ambari.</p>

<h2 id="step-31-create-pig-script-">Step 3.1: Create Pig Script <a id="step3.2"></a></h2>

<p>In this phase of the tutorial, we create and run a Pig script. We will use the Ambari Pig View. Let's get started…</p>

<h3 id="311-log-in-to-ambari-pig-user-views">3.1.1 Log in to Ambari Pig User Views</h3>

<p>To get to the Ambari Pig View, click on the Ambari Views icon at top right and select <strong>Pig</strong>:</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/ambari_pig_view_lab3.png" alt="Screen Shot 2015-07-21 at 10.12.41 AM" /></p>

<p>This will bring up the Ambari Pig User View interface. Your Pig View does not have any scripts to display, so it will look like the following:</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/Lab3_4.png" alt="Lab3_4" /></p>

<p>On the left is a list of your scripts, and on the right is a composition box for writing scripts. A <strong>special interface feature</strong> is the <em>Pig helper</em> located below the name of your script file. The <em>Pig helper</em> provides us with templates for the statements, functions, I/O statements, HCatLoader() and Python user defined functions. At the very bottom are status areas that will show the results of our script and log files.</p>

<p>The following screenshot shows and describes the various components and features of the Pig View:</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/pig_user_view_components_hello_hdp.png" alt="Lab3_5" /></p>

<h3 id="312-create-a-new-script">3.1.2 Create a New Script</h3>

<p>Let's enter a Pig script. Click the <strong>New Script</strong> button in the upper-right corner of the view:</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/new_script_hello_hdp_lab3.png" alt="Lab3_6" /></p>

<p>Name the script <strong>riskfactor.pig</strong>, then click the <strong>Create</strong> button:</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/Lab3_7.png" alt="Lab3_7" /></p>

<h3 id="313-load-data-in-pig-using-hcatalog">3.1.3 Load Data in Pig using Hcatalog</h3>

<p>We will use <strong>HCatalog</strong> to <em>load data into Pig</em>. HCatalog allows us to <em>share schema across tools</em> and users within our Hadoop environment. It also allows us to <em>factor out schema</em> and <em>location information from</em> our <em>queries and scripts</em> and <em>centralize them in a common repository</em>. Since it is in HCatalog we can use the <strong>HCatLoader() function</strong>. Pig allows us to give the table a name or alias and not have to worry about allocating space and defining the structure. We just have to worry about how we are processing the table.</p>

<ul>
  <li>We can use the Pig helper located below the name of your script file to give us a template for the line. Click on the <strong>Pig helper -&gt; HCatalog -&gt; LOAD</strong> template</li>
  <li>The entry <strong>%TABLE%</strong> is highlighted in red for us. Type the name of the table which is geolocation.</li>
  <li>Remember to add the <strong>a =</strong> before the template. This saves the results into a. Note the <strong>'='</strong> has to have a space before and after it.</li>
  <li>Our completed line of code will look like:</li>
</ul>

<div class="highlighter-rouge"><pre class="highlight"><code>a = LOAD 'geolocation' USING org.apache.hive.hcatalog.pig.HCatLoader();
</code></pre>
</div>

<p>The script above loads data, in our case, from a file named <strong>geolocation</strong> using the <em>HCatLoader()</em> function. Copy-and-paste the above Pig code into the riskfactor.pig window.</p>

<blockquote>
  <p>Note: Refer to <a href="http://pig.apache.org/docs/r0.14.0/basic.html#load">Pig Latin Basics - load</a> to learn more about the <strong>load</strong> operator.</p>
</blockquote>

<h3 id="314-filter-your-data-set">3.1.4 Filter your data set</h3>

<p>The next step is to <strong>select a subset of the records</strong>, so we have the records of drivers <em>for which the event is not normal</em>. To do this in Pig we <strong>use the Filter operator</strong>. We <strong>instruct Pig to Filter</strong> our table and keep <em>all records where event !="normal"</em> and store this in b. With this one simple statement, Pig will look at each record in the table and filter out all the ones that do not meet our criteria.</p>

<ul>
  <li>We can use Pig Help again by clicking on the <strong>Pig helper-&gt; Relational Operators -&gt; FILTER</strong> template</li>
  <li>We can replace <strong>%VAR%</strong> with <strong>"a"</strong> (hint: tab jumps you to the next field)</li>
  <li>Our <strong>%COND%</strong> is "<strong>event !='normal';</strong> " (note: single quotes are needed around normal and don't forget the trailing semi-colon)
hdp-2.5</li>
  <li>Complete line of code will look like:</li>
</ul>

<div class="highlighter-rouge"><pre class="highlight"><code>b = filter a by event != 'normal';
</code></pre>
</div>

<p>Copy-and-paste the above Pig code into the riskfactor.pig window.</p>

<blockquote>
  <p>Note: Refer to <a href="http://pig.apache.org/docs/r0.14.0/basic.html#filter">Pig Latin Basics - filter</a> to learn more about the <strong>filter</strong> operator.</p>
</blockquote>

<h3 id="315-iterate-your-data-set">3.1.5 Iterate your data set</h3>

<p>Since we have the right set of records, let's iterate through them. We use the <strong>"foreach"</strong> operator on the grouped data to iterate through all the records. We would also like to <strong>know the number of non normal events associated with a driver</strong>, so to achieve this we <em>add '1' to every row</em> in the data set.</p>

<ul>
  <li>Use Pig Help again by clicking on the <strong>Pig helper -&gt; Relational Operators -&gt; FOREACH</strong> template</li>
  <li>Our <strong>%DATA%</strong> is <strong>b</strong> and the second <strong>%NEW_DATA%</strong> is "<strong>driverid, event, (int) '1' as occurance;</strong>"</li>
  <li>Complete line of code will look like:</li>
</ul>

<div class="highlighter-rouge"><pre class="highlight"><code>c = foreach b generate driverid, event, (int) '1' as occurance;
</code></pre>
</div>

<p>Copy-and-paste the above Pig code into the riskfactor.pig window:</p>

<blockquote>
  <p>Note: Refer to <a href="http://pig.apache.org/docs/r0.14.0/basic.html#foreach">Pig Latin Basics - foreach</a> to learn more about the <strong>foreach</strong> operator.</p>
</blockquote>

<h3 id="316-calculate-the-total-non-normal-events-for-each-driver">3.1.6 Calculate the total non normal events for each driver</h3>

<p>The <strong>group</strong> statement is important because it <em>groups the records by one or more relations</em>. In our case, we want to group by driver id and iterate over each row again to sum the non normal events.</p>

<ul>
  <li>Use the template <strong>Pig helper -&gt; Relational Operators -&gt; GROUP %VAR% BY %VAR%</strong></li>
  <li>First <strong>%VAR%</strong> takes <strong>"c"</strong> and second <strong>%VAR%</strong> takes "<strong>driverid;</strong>"</li>
  <li>Complete line of code will look like:</li>
</ul>

<div class="highlighter-rouge"><pre class="highlight"><code>d = group c by driverid;
</code></pre>
</div>

<p>Copy-and-paste the above Pig code into the riskfactor.pig window.</p>

<ul>
  <li>Next use Foreach statement again to add the occurance.</li>
</ul>

<div class="highlighter-rouge"><pre class="highlight"><code>e = foreach d generate group as driverid, SUM(c.occurance) as t_occ;
</code></pre>
</div>

<blockquote>
  <p>Note: Refer to <a href="http://pig.apache.org/docs/r0.14.0/basic.html#group">Pig Latin Basics - group</a> to learn more about the <strong>group</strong> operator.</p>
</blockquote>

<h3 id="317-load-drivermileage-table-and-perform-a-join-operation">3.1.7 Load drivermileage Table and Perform a Join Operation</h3>

<p>In this section, we will load drivermileage table into Pig using <strong>Hcatlog</strong> and perform a <strong>join</strong> operation on driverid. The <strong>resulting data</strong> set will <em>give us total miles and total non normal events</em> for a particular driver.</p>

<ul>
  <li>Load drivermileage using HcatLoader()</li>
</ul>

<div class="highlighter-rouge"><pre class="highlight"><code>g = LOAD 'drivermileage' using org.apache.hive.hcatalog.pig.HCatLoader();
</code></pre>
</div>

<ul>
  <li>Use the template <strong>Pig helper -&gt;Relational Operators-&gt;JOIN %VAR% BY</strong></li>
  <li>Replace <strong>%VAR%</strong> by '<strong>e</strong>' and after <strong>BY</strong> put '<strong>driverid, g by driverid;</strong>'</li>
  <li>Complete line of code will look like:</li>
</ul>

<div class="highlighter-rouge"><pre class="highlight"><code>h = join e by driverid, g by driverid;
</code></pre>
</div>

<p>Copy-and-paste the above two Pig codes into the riskfactor.pig window.</p>

<blockquote>
  <p>Note: Refer to <a href="http://pig.apache.org/docs/r0.14.0/basic.html#join">Pig Latin Basics - join</a> to learn more about the <strong>join</strong> operator.</p>
</blockquote>

<h3 id="318-compute-driver-risk-factor">3.1.8 Compute Driver Risk factor</h3>

<p>In this section, we will associate a driver risk factor with every driver. To <strong>calculate driver risk factor</strong>, <em>divide total miles travelled by non normal event occurrences</em>.</p>

<ul>
  <li>We will use <strong>Foreach</strong> statement again to compute driver risk factor for each driver.</li>
  <li>Use the following code and paste it into your Pig script.</li>
</ul>

<div class="highlighter-rouge"><pre class="highlight"><code>final_data = foreach h generate $0 as driverid, $1 as events, $3 as totmiles, (float) $3/$1 as riskfactor;
</code></pre>
</div>

<ul>
  <li>As a final step, <strong>store the data</strong> into a table <em>using Hcatalog</em>.</li>
</ul>

<div class="highlighter-rouge"><pre class="highlight"><code>store final_data into 'riskfactor' using org.apache.hive.hcatalog.pig.HCatStorer();
</code></pre>
</div>
<p>Here is the final code and what it will look like once you paste it into the editor.</p>

<blockquote>
  <p>Note: Refer to <a href="http://pig.apache.org/docs/r0.14.0/basic.html#store">Pig Latin Basics - store</a> to learn more about the <strong>store</strong> operator.</p>
</blockquote>

<p><strong>Geolocation has data stored in ORC format</strong></p>

<div class="highlighter-rouge"><pre class="highlight"><code>a = LOAD 'geolocation' using org.apache.hive.hcatalog.pig.HCatLoader();
b = filter a by event != 'normal';
c = foreach b generate driverid, event, (int) '1' as occurance;
d = group c by driverid;
e = foreach d generate group as driverid, SUM(c.occurance) as t_occ;
g = LOAD 'drivermileage' using org.apache.hive.hcatalog.pig.HCatLoader();
h = join e by driverid, g by driverid;
final_data = foreach h generate $0 as driverid, $1 as events, $3 as totmiles, (float) $3/$1 as riskfactor;
store final_data into 'riskfactor' using org.apache.hive.hcatalog.pig.HCatStorer();
</code></pre>
</div>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/riskfactor_computation_script_lab3.png" alt="Lab3_8" /></p>

<p>Save the file riskfactor.pig by clicking the <strong>Save</strong> button in the left-hand column.</p>

<h2 id="step-32-quick-recap-">Step 3.2: Quick Recap <a id="step3.3"></a></h2>

<p>Before we execute the code, let's review the code again:</p>

<ul>
  <li>The line <code class="highlighter-rouge">a=</code> loads the geolocation table from HCatalog.</li>
  <li>The line <code class="highlighter-rouge">b=</code> filters out all the rows where the event is not 'Normal'.</li>
  <li>Then we add a column called occurrence and assign it a value of 1.</li>
  <li>We then group the records by driverid and sum up the occurrences for each driver.</li>
  <li>At this point we need the miles driven by each driver, so we load the table we created using Hive.</li>
  <li>To get our final result, we join by the driverid the count of events in e with the mileage data in g.</li>
  <li>Now it is real simple to calculate the risk factor by dividing the miles driven by the number of events</li>
</ul>

<p>You need to configure the Pig Editor to use HCatalog so that the Pig script can load the proper libraries. In the Pig arguments text box, enter <strong>-useHCatalog</strong> and click the <strong>Add</strong> button:</p>

<blockquote>
  <p><strong>Note</strong> this argument is <strong>case sensitive</strong>. It should be typed exactly "-useHCatalog".</p>
</blockquote>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/Lab3_9.png" alt="Lab3_9" /></p>

<p>The <strong>Arguments</strong> section of the Pig View should now look like the following:<br />
<img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/Lab3_10.png" alt="Lab3_10" /></p>

<h2 id="step-33-execute-pig-script-on-tez-">Step 3.3: Execute Pig Script on Tez <a id="step3.4"></a></h2>

<h3 id="331-execute-pig-script">3.3.1 Execute Pig Script</h3>

<p>Click <strong>Execute on Tez</strong> checkbox and finally hit the blue <strong>Execute</strong> button to submit the job. Pig job will be submitted to the cluster. This will generate a new tab with a status of the running of the Pig job and at the top you will find a progress bar that shows the job status.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/execute_pig_script_compute_riskfactor_hello_hdp_lab3.png" alt="Lab3_11" /></p>

<h3 id="332-view-results-section">3.3.2 View Results Section</h3>

<p>Wait for the job to complete. The output of the job is displayed in the <strong>Results</strong> section. Notice your script does not output any result – it stores the result into a Hive table – so your Results section will be empty.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/running_script_riskfactor_hello_hdp_lab3.png" alt="Lab3_12" /></p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/completed_riskfactor_script_hello_hdp_lab3.png" alt="Lab3_13" /></p>

<p>Click on the <strong>Logs</strong> dropdown menu to see what happened when your script ran. Errors will appear here.</p>

<h3 id="333-view-logs-section-debugging-practice">3.3.3 View Logs section (Debugging Practice)</h3>

<p><strong>Why are Logs important?</strong></p>

<p>The logs section is helpful when debugging code after expected output does not happen. For instance, say in the next section, we load the sample data from our <strong>riskfactor</strong> table and nothing appears. Logs will tell us why the job failed. A common issue that could happen is that pig does not successfully read data from the <strong>geolocation</strong> table or <strong>drivermileage</strong> table. Therefore, we can effectively address the issue.</p>

<p>Let's verify pig read from these tables successfully and stored the data into our <strong>riskfactor</strong> table. You should receive similar output:</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/debug_through_logs_lab3.png" alt="debug_through_logs_lab3" /></p>

<p>What results do our logs show us about our Pig Script?</p>

<ul>
  <li>Read 8000 records from our <strong>geolocation</strong> table</li>
  <li>Read 100 records from our <strong>drivermileage</strong> table</li>
  <li>Stored 99 records into our <strong>riskfactor</strong> table</li>
</ul>

<p>Can you think of scenarios in which these results if different would help us debug our script?
For example, say 0 records were read from the <strong>geolocation</strong> table, how would you solve the problem?</p>

<h3 id="334-verify-pig-script-successfully-populated-hive-table">3.3.4 Verify Pig Script Successfully Populated Hive Table</h3>

<p>Go back to the Ambari Hive User View and browse the data in the <strong>riskfactor</strong> table to verify that your Pig job successfully populated this table. Here is what is should look like:</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/pig_populated_riskfactor_table_hello_hdp_lab3.png" alt="Lab3_14" /></p>

<p>At this point we now have our truck miles per gallon table and our risk factor table. The next step is to pull this data into Excel to create the charts for the visualization step.</p>

<h2 id="summary-">Summary <a id="summary-lab3"></a></h2>
<p>Congratulations! Let's summarize the Pig commands we learned in this tutorial to compute risk factor analysis on the geolocation and truck data. We learned to use Pig to access the data from Hive using the <strong>LOAD {hive_table} …HCatLoader()</strong> script. Therefore, we were able to perform the <strong>filter</strong>, <strong>foreach</strong>, <strong>group</strong>, <strong>join</strong>, and <strong>store {hive_table} …HCatStorer()</strong> scripts to manipulate, transform and process this data. To review these bold pig latin operators, view the <a href="http://pig.apache.org/docs/r0.14.0/basic.html">Pig Latin Basics</a>, which contains documentation on each operator.</p>

<h2 id="suggested-readings-">Suggested Readings <a id="suggested-readings-lab3"></a></h2>

<p>Strengthen your foundation of pig latin and reinforce why this scripting platform is benficial for processing and analyzing massive data sets with these resources:</p>

<ul>
  <li>To practice more pig programming, visit <a href="http://hortonworks.com/hadoop/pig/#tutorials">Pig Tutorials</a></li>
  <li><a href="http://hortonworks.com/hadoop/pig/">Apache Pig</a></li>
  <li><a href="http://www.amazon.com/Programming-Pig-Alan-Gates/dp/1449302645/ref=sr_1_2?ie=UTF8&amp;qid=1455994738&amp;sr=8-2&amp;keywords=pig+latin&amp;refinements=p_72%3A2661618011">Programming Pig</a></li>
  <li>To learn more about the various Pig operators, refer to<a href="http://pig.apache.org/docs/r0.14.0/basic.html">Pig Latin Basics</a></li>
  <li><a href="http://hortonworks.com/training/class/hadoop-2-data-analysis-pig-hive/">HDP DEVELOPER: APACHE PIG AND HIVE</a></li>
</ul>

</div>

<div id="tutorial-footer">
  <hr>
  <h2>Tutorial Q&amp;A and Reporting Issues</h2>
  <p>If you need help or have questions with this tutorial, please first check HCC for existing Answers to questions on this tutorial using the Find Answers button.  If you don't find your answer you can post a new HCC question for this tutorial using the Ask Questions button.</p>
  <p><a class="btn" href="https://community.hortonworks.com/topics/tutorial-100.html" role="button">Find Answers</a> <a class="btn pull-right" href="https://community.hortonworks.com/questions/ask.html?space=81&topics=tutorial-100&topics=hdp-2.5.0" role="button">Ask Questions</a></p>
  <p>Tutorial Name: <strong>Hello HDP An Introduction to Hadoop with Hive and Pig</strong></p>
  <p>HCC Tutorial Tag:<strong> tutorial-100</strong> and <strong>hdp-2.5.0</strong></p>
  <p>If the tutorial has multiple labs please indicate which lab your question corresponds to. Please provide any feedback related to that lab.</p>
  <p>All Hortonworks, partner and community tutorials are posted in the Hortonworks github and can be contributed via the <a href="https://github.com/hortonworks/tutorials/wiki">Hortonworks Tutorial Contribution Guide</a>.  If you are certain there is an issue or bug with the tutorial, please <a href="https://github.com/hortonworks/tutorials/wiki#issues-with-tutorials">create an issue</a> on the repository and we will do our best to resolve it!</p>
</div>
