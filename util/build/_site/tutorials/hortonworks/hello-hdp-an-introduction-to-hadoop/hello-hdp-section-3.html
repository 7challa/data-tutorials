

<div class="tutorial-content">
  <h1 id="lab-1-hdfs---loading-data">Lab 1: HDFS - Loading Data</h1>

<h2 id="loading-sensor-data-into-hdfs">Loading Sensor Data into HDFS</h2>

<h2 id="introduction">Introduction</h2>

<p>In this section, you will download the sensor data and load that into HDFS using Ambari User Views. You will get introduced to the Ambari Files User View to manage files. You can perform tasks like create directories, navigate file systems and upload files to HDFS.  In addition, you'll perform a few other file-related tasks as well.  Once you get the basics, you will create two directories and then load two files into HDFS using the Ambari Files User View.</p>

<h2 id="pre-requisites">Pre-Requisites</h2>

<p>The tutorial is a part of series of hands on tutorial to get you started on HDP using Hortonworks sandbox. Please ensure you complete the prerequisites before proceeding with this tutorial.</p>

<ul>
  <li>Downloaded and Installed <a href="http://hortonworks.com/downloads/#sandbox">Hortonworks Sandbox</a></li>
  <li><a href="http://hortonworks.com/hadoop-tutorial/learning-the-ropes-of-the-hortonworks-sandbox/">Learning the Ropes of the Hortonworks Sandbox</a>[Optional]</li>
  <li>Allow yourself around <strong>20 minutes</strong> to complete this tutorial.</li>
</ul>

<h2 id="outline">Outline</h2>

<ul>
  <li><a href="#hdfs-backdrop">HDFS backdrop</a></li>
  <li><a href="#step1.1">Step 1.1: Download data</a> – <a href="https://app.box.com/HadoopCrashCourseData"><strong>Geolocation.zip</strong></a></li>
  <li><a href="#step1.2">Step 1.2: Load Data into HDFS</a></li>
  <li><a href="#summary-lab1">Summary</a></li>
  <li><a href="#suggested-reading">Suggested Reading</a></li>
</ul>

<h2 id="hdfs-backdrop-">HDFS backdrop <a id="hdfs-backdrop"></a></h2>

<p>A single physical machine gets saturated with its storage capacity as the data grows. This growth drives the need to partition your data across separate machines. This type of File system that manages storage of data across a network of machines is called Distributed File Systems. <a href="http://hortonworks.com/blog/thinking-about-the-hdfs-vs-other-storage-technologies/">HDFS</a> is a core component of Apache Hadoop and is designed to store large files with streaming data access patterns, running on clusters of commodity hardware. With Hortonworks Data Platform HDP 2.2, HDFS is now expanded to support <a href="http://hortonworks.com/blog/heterogeneous-storage-policies-hdp-2-2/">heterogeneous storage</a>  media within the HDFS cluster.</p>

<h3 id="step-11-download-and-extract-the-sensor-data-files-">Step 1.1: Download and Extract the Sensor Data Files <a id="step1.1"></a></h3>

<p>1.  You can download the sample sensor data contained in a compressed (.zip) folder here:  <a href="https://app.box.com/HadoopCrashCourseData"><strong>Geolocation.zip</strong></a></p>

<p>2.   Save the Geolocation.zip file to your computer, then extract the files. You should see a Geolocation folder that contains the following files:
    *   geolocation.csv – This is the collected geolocation data from the trucks. It contains <strong>records</strong> showing <em>truck location, date, time, type of event, speed, etc</em>.
    *   trucks.csv – This is data was exported from a relational database and it shows <strong>info</strong> on <em>truck models, driverid, truckid, and aggregated mileage info</em>.</p>

<h3 id="step-12-load-the-sensor-data-into-hdfs-">Step 1.2: Load the Sensor Data into HDFS <a id="step1.2"></a></h3>

<p>1.   Go to Ambari Dashboard and open the <strong>HDFS Files</strong> view. Click on the 9 square Ambari User Views icon next to the username button and select the <strong>HDFS Files</strong> menu item.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/files_view_lab1.png" alt="Screen Shot 2015-07-21 at 10.17.21 AM" /></p>

<p>2.  Start from the top root of the HDFS file system, you will see all the files the logged in user (<em>maria_dev</em> in this case) has access to see:</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/root_files_view_folder_lab1.png" alt="Lab2_2" /></p>

<p>3. Navigate to <code class="highlighter-rouge">/user/maria_dev</code> directory by clicking on the directory links.</p>

<p>4.  Let's create a data directory to upload the data that we are going to use for this use case.  Click the <img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/new_folder_icon_lab1.png" alt="Lab2_3" /> button to create the data directory inside the <code class="highlighter-rouge">maria_dev</code> directory. Now navigate into the <code class="highlighter-rouge">data</code> directory.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/add_new_folder_data_lab1.png" alt="add_new_folder_data_lab1" /></p>

<h3 id="121-upload-geolocation-and-trucks-csv-files-to-data-folder">1.2.1 Upload Geolocation and Trucks CSV Files to data Folder</h3>

<p>4.   If you're not already in your newly created directory path <code class="highlighter-rouge">/user/maria_dev/data</code>, go to the <strong>data</strong> folder. Then  click on the <img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/upload_icon_lab1.png" alt="upload_icon_lab1" /> button to upload the corresponding <strong>geolocation.csv</strong> and <strong>trucks.csv</strong> files into it.</p>

<p>5. An <strong>Upload file</strong> window will appear, click on the cloud symbol.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/upload_file_lab1.png" alt="upload_file_lab1" /></p>

<p>6. Another window will appear, navigate to the destination the two csv files were downloaded. Click on one at a time, press open to complete the upload. Repeat the process until both files are uploaded.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/upload_file_window_lab1.png" alt="upload_file_window_lab1" /></p>

<p>Both files are uploaded to HDFS as shown in the Files View UI:</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/uploaded_files_lab1.png" alt="uploaded_files_lab1" /></p>

<p>You can also perform the following operations on a file or folder by clicking on the entity's row: <strong>Open</strong>, <strong>Rename</strong>, <strong>Permissions</strong>, <strong>Delete</strong>, <strong>Copy</strong>, <strong>Move</strong>, <strong>Download</strong> and <strong>concatenate</strong>.</p>

<h3 id="122-set-write-permissions-to-write-to-data-folder">1.2.2 Set Write Permissions to Write to data Folder</h3>

<p>1. click on the <code class="highlighter-rouge">data</code> folder's row, which is contained within the directory path <code class="highlighter-rouge">/user/maria_dev</code>. Click <strong>Permissions</strong>. Make sure that the background of all the <strong>write</strong> boxes are checked (<strong>blue</strong>). Refer to image for a visual explanation.</p>

<p><img src="https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.5/assets/hello-hdp/edit_permissions_lab1.png" alt="edit_permissions_lab1" /></p>

<h2 id="summary-">Summary <a id="summary-lab1"></a></h2>

<p>Congratulations! Let's summarize the skills and knowledge we acquired from this tutorial. We learned <strong>Hadoop Distributed File System (HDFS)</strong> was built to manage storing data across multiple machines. Now we can upload data into the HDFS using Ambari's HDFS <strong>Files view</strong>.</p>

<h2 id="suggested-reading-">Suggested Reading <a id="suggested-reading"></a></h2>
<ul>
  <li><a href="http://hortonworks.com/hadoop/hdfs/">HDFS</a></li>
  <li><a href="http://hortonworks.com/hadoop-tutorial/using-commandline-manage-files-hdfs/">Manage Files on HDFS with Command Line: Hands-on Tutorial</a></li>
  <li><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html">HDFS User Guide</a></li>
  <li>Build your HDFS Architecture Knowledge <a href="https://hadoop.apache.org/docs/r1.0.4/hdfs_design.html">HDFS Architecture Guide</a></li>
  <li><a href="http://hortonworks.com/training/class/hdp-operations-hadoop-administration-fundamentals/">HDP OPERATIONS: HADOOP ADMINISTRATION</a></li>
</ul>

</div>

<div id="tutorial-footer">
  <hr>
  <h2>Tutorial Q&amp;A and Reporting Issues</h2>
  <p>If you need help or have questions with this tutorial, please first check HCC for existing Answers to questions on this tutorial using the Find Answers button.  If you don't find your answer you can post a new HCC question for this tutorial using the Ask Questions button.</p>
  <p><a class="btn" href="https://community.hortonworks.com/topics/tutorial-100.html" role="button">Find Answers</a> <a class="btn pull-right" href="https://community.hortonworks.com/questions/ask.html?space=81&topics=tutorial-100&topics=hdp-2.5.0" role="button">Ask Questions</a></p>
  <p>Tutorial Name: <strong>Hello HDP An Introduction to Hadoop with Hive and Pig</strong></p>
  <p>HCC Tutorial Tag:<strong> tutorial-100</strong> and <strong>hdp-2.5.0</strong></p>
  <p>If the tutorial has multiple labs please indicate which lab your question corresponds to. Please provide any feedback related to that lab.</p>
  <p>All Hortonworks, partner and community tutorials are posted in the Hortonworks github and can be contributed via the <a href="https://github.com/hortonworks/tutorials/wiki">Hortonworks Tutorial Contribution Guide</a>.  If you are certain there is an issue or bug with the tutorial, please <a href="https://github.com/hortonworks/tutorials/wiki#issues-with-tutorials">create an issue</a> on the repository and we will do our best to resolve it!</p>
</div>
