

<div class="tutorial-content">
  <h2 id="lab-3-real-time-data-ingestion-in-hbase-and-hive-using-storm">Lab 3: Real Time data Ingestion in Hbase and Hive using Storm</h2>

<h3 id="introduction">Introduction</h3>

<p>In this tutorial, we will build a solution to ingest real time streaming data into HBase and HDFS.</p>

<p>In previous tutorial we have explored generating and processing streaming data with <a href="http://hortonworks.com/hadoop-tutorial/simulating-transporting-realtime-events-stream-apache-kafka/">Apache Kafka</a> and <a href="http://hortonworks.com/hadoop-tutorial/ingesting-processing-real-time-events-apache-storm/">Apache Storm</a>. In this tutorial we will create HDFS Bolt &amp; HBase Bolt to read the streaming data from the Kafka Spout and persist in Hive &amp; HBase tables.</p>

<h2 id="pre-requisites">Pre-Requisites</h2>

<ul>
  <li><a href="http://hortonworks.com/hadoop-tutorial/simulating-transporting-realtime-events-stream-apache-kafka/">Tutorial #1</a> Simulate and Transport Real Time Events with Apache Kafka</li>
  <li><a href="http://hortonworks.com/hadoop-tutorial/ingesting-processing-real-time-events-apache-storm/">Tutorial #2</a> Ingest and Process Real Time Events with Apache Storm</li>
  <li>Downloaded and Installed the latest <a href="http://hortonworks.com/products/hortonworks-sandbox/#install">Hortonworks Sandbox</a></li>
  <li><a href="http://hortonworks.com/hadoop-tutorial/learning-the-ropes-of-the-hortonworks-sandbox/">Learning the Ropes of the Hortonworks Sandbox</a></li>
</ul>

<h2 id="outline">Outline</h2>

<ul>
  <li><a href="#hbase-concept-lab3">Hbase</a></li>
  <li><a href="#apache-storm-concept-lab3">Apache Storm</a></li>
  <li><a href="#step-1-start-hbase-lab3">Step 1: Start Hbase</a></li>
  <li><a href="#step2-create-tables-hdfs-hbase-lab3">Step 2: Create tables in HDFS and Hbase</a></li>
  <li><a href="#step3-launch-new-storm-topology-lab3">Step 3: Launch new Storm topology</a></li>
  <li><a href="#step4-generate-events-verify-data-hdfs-hbase-lab3">Step 4: Generate events and verify data in HDFS and HBase</a></li>
  <li><a href="#step5-code-description-lab3">Step 5: Code description</a></li>
  <li><a href="#update-tutorials-master-project-lab3">Appendix A: Update Tutorials Master Project</a></li>
  <li><a href="#enable-remote-desktop-setup-topology-lab3">Appendix B: Enable Remote Desktop and Set up Storm Topology as an Eclipse Project</a></li>
  <li><a href="#further-reading-lab3">Further Reading</a></li>
</ul>

<h2 id="hbase-a-idhbase-concept-lab3a">HBase <a id="hbase-concept-lab3"></a></h2>

<p>HBase provides near real-time, random read and write access to tables (or to be more accurate ‘maps’) storing billions of rows and millions of columns.</p>

<p>In this case, once we store this rapidly and continuously growing dataset from Internet of Things (IoT), we will be able to perform a swift lookup for analytics regardless of the data size.</p>

<h2 id="apache-storm-a-idapache-storm-concept-lab3a">Apache Storm <a id="apache-storm-concept-lab3"></a></h2>

<p>Apache Storm is an Open Source distributed, reliable, fault–tolerant system for real time processing of large volume of data. Spout and Bolt are the two main components in Storm, which work together to process streams of data.</p>

<ul>
  <li><strong>Spout</strong>: Works on the source of data streams. In the “Truck Events” use case, Spout will read data from Kafka topics.</li>
  <li><strong>Bolt</strong>: Spout passes streams of data to Bolt which processes and persists  it to a data store or sends it downstream to another Bolt.</li>
</ul>

<p>In this tutorial, you will learn the following topics:</p>

<ul>
  <li>Configure Storm Bolt.</li>
  <li>Store Persisting data in HBase and Hive.</li>
  <li>Verify the data in HDFS and HBase.</li>
</ul>

<h3 id="step-1-start-hbase-a-idstep-1-start-hbase-lab3a">Step 1: Start HBase <a id="step-1-start-hbase-lab3"></a></h3>

<p>1.  <strong>View the HBase Services page</strong></p>

<p>Started by logging into Ambari as an admin user. From the previous tutorials: HDFS, Hive, YARN, Kafka and Storm should already be running but HBase may be down. From the Dashboard page of Ambari, click on HBase from the list of installed services.</p>

<p><img src="/assets/realtime-event-processing/t3-update/installed_services_iot_t3.png" alt="Screen Shot 2015-06-04 at 6.26.48 PM.png" /></p>

<p>2. Start HBase</p>

<p>From the HBase page, click on Service Actions -&gt; Start</p>

<p><img src="/assets/realtime-event-processing/t3-update/start_hbase_iot_t3.png" alt="Screen Shot 2015-06-04 at 6.32.07 PM.png" /></p>

<p>Check the box and click on Confirm Start:</p>

<p><img src="/assets/realtime-event-processing/t3-update/image09.png" alt="Screen Shot 2015-06-04 at 6.33.21 PM.png" /></p>

<p>Wait for HBase to start (It may take a few minutes to turn green)</p>

<p><img src="/assets/realtime-event-processing/t3-update/started_hbase_iot_t3.png" alt="Screen Shot 2015-06-04 at 6.41.39 PM.png" /></p>

<p>You can use the Ambari dashboard to check status of other components too. If <strong>HDFS, Hive, YARN, Kafka, Storm or HBase</strong> are down, you can start them in the same way: by selecting the service and then using the Service Actions to start it. The remaining components do not have to be up. (Oozie can be stopped to save memory, as it is not needed for this tutorial)</p>

<h3 id="step-2-create-tables-in-hdfs--hbase-a-idstep2-create-tables-hdfs-hbase-lab3a">Step 2: Create tables in HDFS &amp; HBase <a id="step2-create-tables-hdfs-hbase-lab3"></a></h3>

<ul>
  <li>Create HBase tables</li>
</ul>

<p>We will be working with 2 Hbase tables in this tutorial.</p>

<p>The first table stores all events generated and the second stores the ‘driverId’ and non-normal events count.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>[root@sandbox ~]$ su hbase

[hbase@sandbox root]$ hbase shell

hbase(main):001:0&gt; create 'truck_events', 'events'    
hbase(main):002:0&gt; create 'driver_dangerous_events', 'count'    
hbase(main):003:0&gt; list    
hbase(main):004:0&gt; exit 
</code></pre>
</div>

<p><img src="/assets/realtime-event-processing/t3-update/create_hbase_tables_iot_t3.png" alt="Screen Shot 2015-06-04 at 7.03.00 PM.png" /></p>

<p>Next, we will create Hive tables.</p>

<ul>
  <li>Create Hive tables</li>
</ul>

<p>Open the Hive view in Ambari in a browser, copy the below script into the query editor and click Execute: <a href="http://localhost:8080/#/main/views/HIVE/1.0.0/AUTO_HIVE_INSTANCE">http://localhost:8080/#/main/views/HIVE/1.0.0/Hive</a></p>

<div class="language-sql highlighter-rouge"><pre class="highlight"><code><span class="k">create</span> <span class="k">table</span> <span class="n">truck_events_text_partition</span> <span class="err"> </span>  
<span class="p">(</span><span class="n">driverId</span> <span class="n">string</span><span class="p">,</span> <span class="err"> </span>  
 <span class="n">truckId</span> <span class="n">string</span><span class="p">,</span> <span class="err"> </span>  
 <span class="n">eventTime</span> <span class="k">timestamp</span><span class="p">,</span> <span class="err"> </span>  
 <span class="n">eventType</span> <span class="n">string</span><span class="p">,</span> <span class="err"> </span>  
 <span class="n">longitude</span> <span class="n">double</span><span class="p">,</span> <span class="err"> </span>  
 <span class="n">latitude</span> <span class="n">double</span><span class="p">)</span> <span class="err"> </span>  
<span class="n">partitioned</span> <span class="k">by</span> <span class="p">(</span><span class="n">eventDate</span> <span class="n">string</span><span class="p">)</span> <span class="err"> </span>  
<span class="k">ROW</span> <span class="n">FORMAT</span> <span class="n">DELIMITED</span> <span class="err"> </span>  
<span class="n">FIELDS</span> <span class="n">TERMINATED</span> <span class="k">BY</span> <span class="s1">','</span><span class="p">;</span>
</code></pre>
</div>

<p>This hive query creates the Hive table to persist all events generated. The table is partitioned by date.</p>

<p><img src="/assets/realtime-event-processing/t3-update/create_truck_events_text_partition_hive_table_iot_t3.png" alt="" /></p>

<p>Verify that the table has been properly created by refreshing the Database Explorer. Under Databases, click default to expand this table and the new table should appear. Clicking on the List icon next to truck_events_text_partition shows that the table was created but empty.</p>

<p><img src="/assets/realtime-event-processing/t3-update/verify_truck_events_text_partition_created_iot_t3.png" alt="Screen Shot 2015-06-04 at 7.13.29 PM.png" /></p>

<ul>
  <li>Create ORC ‘truckevent’ Hive tables</li>
</ul>

<p>The Optimized Row Columnar (ORC) file format provides a highly efficient way to store Hive data. It was designed to overcome limitations of the other Hive file formats. Using ORC files improves performance when Hive is reading, writing, and processing data.</p>

<p>Syntax for ORC tables:</p>

<p><code class="highlighter-rouge">CREATE TABLE … STORED AS ORC</code></p>

<p><code class="highlighter-rouge">ALTER TABLE … [PARTITION partition_spec] SET FILEFORMAT ORC</code></p>

<p><strong>Note</strong>: This statement only works on partitioned tables. If you apply it to flat tables, it may cause query errors.</p>

<p>Next let’s create the ‘truckevent’ table as per the above syntax. Paste the below into the worksheet of the Hive view and click Execute</p>

<div class="language-sql highlighter-rouge"><pre class="highlight"><code><span class="k">create</span> <span class="k">table</span> <span class="n">truck_events_text_partition_orc</span>
<span class="p">(</span><span class="n">driverId</span> <span class="n">string</span><span class="p">,</span>
<span class="n">truckId</span> <span class="n">string</span><span class="p">,</span>
<span class="n">eventTime</span> <span class="k">timestamp</span><span class="p">,</span>
<span class="n">eventType</span> <span class="n">string</span><span class="p">,</span>
<span class="n">longitude</span> <span class="n">double</span><span class="p">,</span>
<span class="n">latitude</span> <span class="n">double</span><span class="p">)</span>
<span class="n">partitioned</span> <span class="k">by</span> <span class="p">(</span><span class="n">eventDate</span> <span class="n">string</span><span class="p">)</span>
<span class="k">ROW</span> <span class="n">FORMAT</span> <span class="n">DELIMITED</span>
<span class="n">FIELDS</span> <span class="n">TERMINATED</span> <span class="k">BY</span> <span class="s1">','</span>
<span class="n">STORED</span> <span class="k">AS</span> <span class="n">orc</span> 
<span class="n">TBLPROPERTIES</span> <span class="p">(</span><span class="nv">"orc.compress"</span><span class="o">=</span><span class="nv">"NONE"</span><span class="p">);</span>
</code></pre>
</div>

<p>Refresh the Database Explorer and you should see the new table appear under default:</p>

<p><img src="/assets/realtime-event-processing/t3-update/create_truck_events_partition_orc_iot_t3.png" alt="" /></p>

<p>The data in ‘truck_events_text_partition_orc’ table can be stored with ZLIB, Snappy, LZO compression options. This can be set by changing tblproperties (“orc.compress”=”NONE”)option in the query above.</p>

<ul>
  <li>
    <p>Set permissions on <code class="highlighter-rouge">/tmp/hive</code></p>

    <p><code class="highlighter-rouge">chmod -R 777 /tmp/hive/</code></p>
  </li>
</ul>

<h3 id="step-3-launch-new-storm-topology-a-idstep3-launch-new-storm-topology-lab3a">Step 3: Launch new Storm topology <a id="step3-launch-new-storm-topology-lab3"></a></h3>

<p>Recall that the source code is under <code class="highlighter-rouge">/opt/TruckEvents/Tutorials-master/src</code> directory. The pre-compiled jars are under the <code class="highlighter-rouge">/opt/TruckEvents/Tutorials-master/target</code> directory.</p>

<p><strong>(Optional)</strong> If you would like to modify/run the code:</p>

<ul>
  <li>refer to <a href="#update-tutorials-master-project">Appendix A</a> for the steps to run maven to compile the jars to the target subdir from terminal command line</li>
  <li>refer to <a href="#enable-remote-desktop-setup-topology">Appendix B</a> for the steps to enable VNC (i.e. ‘remote desktop’) access on your sandbox and open/compile the code using Eclipse</li>
</ul>

<p>If its still running, deactivate or kill the previous Storm topology using the Storm UI as shown in the screenshots below:</p>

<p><img src="/assets/realtime-event-processing/t3-update/deactivate_storm_topology_storm_ui_iot_t3.png" alt="storm UI" /></p>

<p><img src="/assets/realtime-event-processing/t3-update/deactivate_and_kill_iot_t3.png" alt="deactivate and kill" /></p>

<p><img src="/assets/realtime-event-processing/t3-update/deactivate_topology_iot_t3.png" alt="Deactivate" /></p>

<p><img src="/assets/realtime-event-processing/t3-update/kill_topology_iot_t3.png" alt="kill" /></p>

<p>The Storm topology can be deactivated/killed from the Storm UI as above or via shell:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>storm <span class="nb">kill </span>TruckEventProcessingTopology
</code></pre>
</div>

<ul>
  <li>Load new Storm topology.</li>
</ul>

<p>Execute the Storm <strong>jar</strong> command to create a new Topology from Tutorial #3 after the code has been compiled.</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="o">[</span>root@sandbox ~]# <span class="nb">cd</span> /opt/TruckEvents/Tutorials-master/

<span class="o">[</span>root@sandbox Tutorials-master]# storm jar target/Tutorial-1.0-SNAPSHOT.jar com.hortonworks.tutorials.tutorial3.TruckEventProcessingTopology  
</code></pre>
</div>

<p>You should see that it successfully submitted the topology:</p>

<p><img src="/assets/realtime-event-processing/t3-update/storm_topology_submitted_iot_t3.png" alt="Screen Shot 2015-06-04 at 7.55.23 PM.png" /></p>

<p>The topology should also show up on the Storm UI as in the image below. After a few minutes, you should see that the number of emitted and transferred tuples is increasing. This increase shows that the messages are process in real time.</p>

<p><img src="/assets/realtime-event-processing/t3-update/topology_show_up_on_stormui_iot_t3.png" alt="Topology Summary" /></p>

<p>Under Topology Visualization: You shall see here that Kafka Spout has started writing to hdfs and hbase.</p>

<p><img src="/assets/realtime-event-processing/t3-update/kafka_spout_writing_hdfs_hbase_iot_t3.png" alt="" /></p>

<p><img src="/assets/realtime-event-processing/t3-update/bolts_section_hbase_hdfs_iot_t3.png" alt="" /></p>

<h3 id="step-4-generate-events-and-verify-data-in-hdfs-and-hbase-a-idstep4-generate-events-verify-data-hdfs-hbase-lab3a">Step 4: Generate Events and Verify Data in HDFS and HBase <a id="step4-generate-events-verify-data-hdfs-hbase-lab3"></a></h3>

<ul>
  <li>Start the ‘TruckEventsProducer’ Kafka Producer and verify that the data has been persisted by using the Storm Topology view.</li>
</ul>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="o">[</span>root@sandbox Tutorials-master]# java -cp target/Tutorial-1.0-SNAPSHOT.jar com.hortonworks.tutorials.tutorial1.TruckEventsProducer sandbox.hortonworks.com:6667 sandbox.hortonworks.com:2181
</code></pre>
</div>

<p>Verify in the Storm UI or Storm User View to verify the Bolt section that HDFS/HBase tuples are being executed and acked</p>

<p><img src="/assets/realtime-event-processing/t3-update/verify_hdfs_hbase_tuples_executed_iot_t3.png" alt="stormview-hbase-hdfs.png" /></p>

<p><img src="/assets/realtime-event-processing/t3-update/bolts_section_statistics_tuples_executed_iot_t3.png" alt="bolts-section-tuples-executed" /></p>

<ul>
  <li>Verify that the data is in HDFS by opening the Ambari Files view: <code class="highlighter-rouge">http://localhost:8080/#/main/views/FILES/0.1.0/MyFiles</code></li>
</ul>

<p>With the default settings for HDFS, users will see the data written to HDFS once in every few minutes.</p>

<p><img src="/assets/realtime-event-processing/t3-update/data_written_to_hdfs_few_minutes_iot_t3.png" alt="Screen Shot 2015-06-04 at 9.10.39 PM.png" /></p>

<p>Drill down into <code class="highlighter-rouge">/truck-events-v4/staging</code> dir in HDFS</p>

<p><img src="/assets/realtime-event-processing/t3-update/image00.png" alt="" /></p>

<p>Press Control-C in the terminal to stop the Kafka producer. Now go back to the staging directory, click on one of the txt files and confirm that it contains the events:</p>

<p><img src="/assets/realtime-event-processing/t3-update/image05.png" alt="Screen Shot 2015-06-04 at 9.20.24 PM.png" /></p>

<blockquote>
  <p><strong>Note:</strong> It may take a 5-10 minutes, before you can access the txt files to see the data.</p>
</blockquote>

<ul>
  <li>Verify data in Hive by navigating to the Hive view, expanding the default database and clicking the List icon next to truck_events_text_partition table</li>
</ul>

<p><img src="/assets/realtime-event-processing/t3-update/verify_data_hive_truckevents_iot_t3.png" alt="Screen Shot 2015-06-04 at 9.13.23 PM.png" /></p>

<ul>
  <li>If you haven’t done so, you can press Control-C to stop the Kafka producer (i.e keep Control key pressed and then press C)</li>
  <li>Verify that the data is in HBase by executing the following commands in HBase shell:</li>
</ul>

<div class="highlighter-rouge"><pre class="highlight"><code>[root@sandbox Tutorials-master]# hbase shell
    
hbase(main):001:0&gt; list
hbase(main):002:0&gt; count 'truck_events'
    366 row(s) in 0.3900 seconds
    =&gt; 366
hbase(main):003:0&gt; count 'driver_dangerous_events'
    3 row(s) in 0.0130 seconds
    =&gt; 3
hbase(main):004:0&gt; exit
</code></pre>
</div>

<p>The <code class="highlighter-rouge">driver_dangerous_events</code> table is updated upon every violation.</p>

<p><img src="/assets/realtime-event-processing/t3-update/verify_data_in_hbase_iot_t3.png" alt="Screen Shot 2015-06-04 at 9.09.29 PM.png" /></p>

<ul>
  <li>Next let’s populate the data into ORC table for interactive query by Excel (or any BI tool) via ODBC over Hive/Tez. Open the Hive view and enter the below and click Execute.</li>
</ul>

<div class="language-sql highlighter-rouge"><pre class="highlight"><code><span class="k">INSERT</span> <span class="n">OVERWRITE</span> <span class="k">TABLE</span> <span class="n">truck_events_text_partition_orc</span> 
<span class="n">partition</span> <span class="p">(</span><span class="n">eventDate</span><span class="p">)</span>
<span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">truck_events_text_partition</span><span class="p">;</span>
</code></pre>
</div>

<p><img src="/assets/realtime-event-processing/t3-update/populate_data_orc_table_excel_iot_t3.png" alt="" /></p>

<p>Notice that this launches a Tez job in the background. You can get more details on this using the Yarn resource manager UI. You can find for this under the link under Ambari -&gt; Yarn -&gt; Quick links but will be similar to <code class="highlighter-rouge">http://localhost:8088/cluster</code></p>

<p><img src="/assets/realtime-event-processing/t3-update/image19.png" alt="Screen Shot 2015-06-06 at 8.00.27 PM.png" /></p>

<p>Now query the ORC table by clicking the List icon next to it under Databases and notice it is also now populated</p>

<p><img src="/assets/realtime-event-processing/t3-update/view_orc_table_data_now_populated_iot_t3.png" alt="" /></p>

<ul>
  <li>Once done, stop the Storm topology</li>
</ul>

<p>The Storm topology can be deactivated/killed from the Storm UI or</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>storm <span class="nb">kill </span>TruckEventProcessingTopology
</code></pre>
</div>

<h2 id="conclusion">Conclusion</h2>

<p>This completes the tutorial #3. You have seen how to store streaming data into multiple sources for persistence.</p>

<h3 id="step-5-code-description-a-idstep5-code-description-lab3a">Step 5: Code Description <a id="step5-code-description-lab3"></a></h3>

<p>1.BaseTruckEventTopology.java</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">topologyConfig</span><span class="o">.</span><span class="na">load</span><span class="o">(</span><span class="n">ClassLoader</span><span class="o">.</span><span class="na">getSystemResourceAsStream</span><span class="o">(</span><span class="n">configFileLocation</span><span class="o">));</span>  
</code></pre>
</div>

<p>This is the base class, where the topology configuration is initialized from the <code class="highlighter-rouge">/resource/truck_event_topology.properties</code> files.</p>

<p>2.FileTimeRotationPolicy.java</p>

<p>This implements the file rotation policy after a certain duration.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="kd">public</span> <span class="nf">FileTimeRotationPolicy</span><span class="o">(</span><span class="kt">float</span> <span class="n">count</span><span class="o">,</span> <span class="n">Units</span> <span class="n">units</span><span class="o">)</span> <span class="o">{</span>

    <span class="k">this</span><span class="o">.</span><span class="na">maxMilliSeconds</span> <span class="o">=</span> <span class="o">(</span><span class="kt">long</span><span class="o">)</span> <span class="o">(</span><span class="n">count</span> <span class="o">*</span> <span class="n">units</span><span class="o">.</span><span class="na">getMilliSeconds</span><span class="o">());</span>

<span class="o">}</span>

<span class="nd">@Override</span> 

<span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">mark</span><span class="o">(</span><span class="n">Tuple</span> <span class="n">tuple</span><span class="o">,</span> <span class="kt">long</span> <span class="n">offset</span><span class="o">)</span> <span class="o">{</span> <span class="err"> </span>  
    <span class="c1">// The offsett is not used here as we are rotating based on time    </span>
    <span class="kt">long</span> <span class="n">diff</span> <span class="o">=</span> <span class="o">(</span><span class="k">new</span> <span class="n">Date</span><span class="o">()).</span><span class="na">getTime</span><span class="o">()</span> <span class="o">-</span> <span class="k">this</span><span class="o">.</span><span class="na">lastCheckpoint</span><span class="o">;</span> <span class="err"> </span>  
    <span class="k">return</span> <span class="n">diff</span> <span class="o">&gt;=</span> <span class="k">this</span><span class="o">.</span><span class="na">maxMilliSeconds</span><span class="o">;</span> <span class="err"> </span>  
<span class="o">}</span>  
</code></pre>
</div>

<p>3.LogTruckEventsBolt.java</p>

<p>LogTruckEvent Spout logs the Kafka messages received from the Kafka Spout to the log files under /var/log/storm/worker-*.log</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="kd">public</span> <span class="kt">void</span> <span class="nf">execute</span><span class="o">(</span><span class="n">Tuple</span> <span class="n">tuple</span><span class="o">)</span> <span class="err"> </span>  
<span class="o">{</span>  
    <span class="n">LOG</span><span class="o">.</span><span class="na">info</span><span class="o">(</span><span class="n">tuple</span><span class="o">.</span><span class="na">getStringByField</span><span class="o">(</span><span class="n">TruckScheme</span><span class="o">.</span><span class="na">FIELD_DRIVER_ID</span><span class="o">)</span> <span class="o">+</span> <span class="s">","</span> <span class="o">+</span> <span class="err"> </span>  
    <span class="n">tuple</span><span class="o">.</span><span class="na">getStringByField</span><span class="o">(</span><span class="n">TruckScheme</span><span class="o">.</span><span class="na">FIELD_TRUCK_ID</span><span class="o">)</span> <span class="o">+</span> <span class="s">","</span> <span class="o">+</span> <span class="err"> </span>  
    <span class="n">tuple</span><span class="o">.</span><span class="na">getValueByField</span><span class="o">(</span><span class="n">TruckScheme</span><span class="o">.</span><span class="na">FIELD_EVENT_TIME</span><span class="o">)</span> <span class="o">+</span> <span class="s">","</span> <span class="o">+</span> <span class="err"> </span>  
    <span class="n">tuple</span><span class="o">.</span><span class="na">getStringByField</span><span class="o">(</span><span class="n">TruckScheme</span><span class="o">.</span><span class="na">FIELD_EVENT_TYPE</span><span class="o">)</span> <span class="o">+</span> <span class="s">","</span> <span class="o">+</span> <span class="err"> </span>  
    <span class="n">tuple</span><span class="o">.</span><span class="na">getStringByField</span><span class="o">(</span><span class="n">TruckScheme</span><span class="o">.</span><span class="na">FIELD_LATITUDE</span><span class="o">)</span> <span class="o">+</span> <span class="s">","</span> <span class="o">+</span> <span class="err"> </span>  
    <span class="n">tuple</span><span class="o">.</span><span class="na">getStringByField</span><span class="o">(</span><span class="n">TruckScheme</span><span class="o">.</span><span class="na">FIELD_LONGITUDE</span><span class="o">));</span> <span class="err"> </span>  
<span class="o">}</span>  
</code></pre>
</div>

<p>4.<code class="highlighter-rouge">TruckScheme.java</code></p>

<p>This is the deserializer provided to the Kafka Spout to deserialize Kafka’s byte message streams to Values objects.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="kd">public</span> <span class="n">List</span><span class="o">&lt;</span><span class="n">Object</span><span class="o">&gt;</span> <span class="nf">deserialize</span><span class="o">(</span><span class="kt">byte</span><span class="o">[]</span> <span class="n">bytes</span><span class="o">)</span> <span class="err"> </span>  
<span class="o">{</span>  
    <span class="k">try</span> <span class="err"> </span>  
    <span class="o">{</span>  
        <span class="n">String</span> <span class="n">truckEvent</span> <span class="o">=</span> <span class="k">new</span> <span class="n">String</span><span class="o">(</span><span class="n">bytes</span><span class="o">,</span> <span class="s">"UTF-8"</span><span class="o">);</span> <span class="err"> </span>  
        <span class="n">String</span><span class="o">[]</span> <span class="n">pieces</span> <span class="o">=</span> <span class="n">truckEvent</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="s">"\\|"</span><span class="o">);</span>  

        <span class="n">Timestamp</span> <span class="n">eventTime</span> <span class="o">=</span> <span class="n">Timestamp</span><span class="o">.</span><span class="na">valueOf</span><span class="o">(</span><span class="n">pieces</span><span class="o">[</span><span class="mi">0</span><span class="o">]);</span> <span class="err"> </span>  
        <span class="n">String</span> <span class="n">truckId</span> <span class="o">=</span> <span class="n">pieces</span><span class="o">[</span><span class="mi">1</span><span class="o">];</span> <span class="err"> </span>  
        <span class="n">String</span> <span class="n">driverId</span> <span class="o">=</span> <span class="n">pieces</span><span class="o">[</span><span class="mi">2</span><span class="o">];</span> <span class="err"> </span>  
        <span class="n">String</span> <span class="n">eventType</span> <span class="o">=</span> <span class="n">pieces</span><span class="o">[</span><span class="mi">3</span><span class="o">];</span> <span class="err"> </span>  
        <span class="n">String</span> <span class="n">longitude</span><span class="o">=</span> <span class="n">pieces</span><span class="o">[</span><span class="mi">4</span><span class="o">];</span> <span class="err"> </span>  
        <span class="n">String</span> <span class="n">latitude</span> <span class="err"> </span><span class="o">=</span> <span class="n">pieces</span><span class="o">[</span><span class="mi">5</span><span class="o">];</span> <span class="err"> </span>  
        <span class="k">return</span> <span class="k">new</span> <span class="nf">Values</span><span class="o">(</span><span class="n">cleanup</span><span class="o">(</span><span class="n">driverId</span><span class="o">),</span> <span class="n">cleanup</span><span class="o">(</span><span class="n">truckId</span><span class="o">),</span> <span class="err"> </span>  
                                <span class="n">eventTime</span><span class="o">,</span> <span class="n">cleanup</span><span class="o">(</span><span class="n">eventType</span><span class="o">),</span> <span class="n">cleanup</span><span class="o">(</span><span class="n">longitude</span><span class="o">),</span> <span class="n">cleanup</span><span class="o">(</span><span class="n">latitude</span><span class="o">));</span>  
    <span class="o">}</span> <span class="err"> </span>  
    <span class="k">catch</span> <span class="o">(</span><span class="n">UnsupportedEncodingException</span> <span class="n">e</span><span class="o">)</span> <span class="err"> </span>  
    <span class="o">{</span>  
        <span class="n">LOG</span><span class="o">.</span><span class="na">error</span><span class="o">(</span><span class="n">e</span><span class="o">);</span> <span class="err"> </span>  
        <span class="k">throw</span> <span class="k">new</span> <span class="nf">RuntimeException</span><span class="o">(</span><span class="n">e</span><span class="o">);</span> <span class="err"> </span>  
    <span class="o">}</span>  
<span class="o">}</span>  
</code></pre>
</div>

<p>5.HiveTablePartitionAction.java</p>

<p>This creates Hive partitions based on timestamp and loads the data by executing the Hive DDL statements.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="kd">public</span> <span class="kt">void</span> <span class="nf">loadData</span><span class="o">(</span><span class="n">String</span> <span class="n">path</span><span class="o">,</span> <span class="n">String</span> <span class="n">datePartitionName</span><span class="o">,</span> <span class="n">String</span> <span class="n">hourPartitionName</span> <span class="o">)</span> <span class="err"> </span>  
<span class="o">{</span>  
    <span class="n">String</span> <span class="n">partitionValue</span> <span class="o">=</span> <span class="n">datePartitionName</span> <span class="o">+</span> <span class="s">"-"</span> <span class="o">+</span> <span class="n">hourPartitionName</span><span class="o">;</span>  

    <span class="n">LOG</span><span class="o">.</span><span class="na">info</span><span class="o">(</span><span class="s">"About to add file["</span><span class="o">+</span> <span class="n">path</span> <span class="o">+</span> <span class="s">"] to a partitions["</span><span class="o">+</span><span class="n">partitionValue</span> <span class="o">+</span> <span class="s">"]"</span><span class="o">);</span>  

    <span class="n">StringBuilder</span> <span class="n">ddl</span> <span class="o">=</span> <span class="k">new</span> <span class="n">StringBuilder</span><span class="o">();</span> <span class="err"> </span>  
    <span class="n">ddl</span><span class="o">.</span><span class="na">append</span><span class="o">(</span><span class="s">" load data inpath "</span><span class="o">)</span> <span class="err"> </span>  
       <span class="o">.</span><span class="na">append</span><span class="o">(</span><span class="s">" '"</span><span class="o">).</span><span class="na">append</span><span class="o">(</span><span class="n">path</span><span class="o">).</span><span class="na">append</span><span class="o">(</span><span class="s">"' "</span><span class="o">)</span> <span class="err"> </span>  
       <span class="o">.</span><span class="na">append</span><span class="o">(</span><span class="s">" into table "</span><span class="o">)</span> <span class="err"> </span>  
       <span class="o">.</span><span class="na">append</span><span class="o">(</span><span class="n">tableName</span><span class="o">)</span> <span class="err"> </span>  
       <span class="o">.</span><span class="na">append</span><span class="o">(</span><span class="s">" partition "</span><span class="o">).</span><span class="na">append</span><span class="o">(</span><span class="s">" (date='"</span><span class="o">).</span><span class="na">append</span><span class="o">(</span><span class="n">partitionValue</span><span class="o">).</span><span class="na">append</span><span class="o">(</span><span class="s">"')"</span><span class="o">);</span>  

    <span class="n">startSessionState</span><span class="o">(</span><span class="n">sourceMetastoreUrl</span><span class="o">);</span>  
</code></pre>
</div>

<p>The data is stored in the partitioned ORC tables using the following method.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="n">String</span> <span class="n">ddlORC</span> <span class="o">=</span> <span class="s">"INSERT OVERWRITE TABLE "</span> <span class="o">+</span> <span class="n">tableName</span> <span class="o">+</span> <span class="s">"_orc SELECT * FROM "</span> <span class="o">+</span><span class="n">tableName</span><span class="o">;</span>  

    <span class="k">try</span> <span class="o">{</span> <span class="err"> </span>  
        <span class="n">execHiveDDL</span><span class="o">(</span><span class="s">"use "</span> <span class="o">+</span> <span class="n">databaseName</span><span class="o">);</span> <span class="err"> </span>  
        <span class="n">execHiveDDL</span><span class="o">(</span><span class="n">ddl</span><span class="o">.</span><span class="na">toString</span><span class="o">());</span> <span class="err"> </span>  
        <span class="n">execHiveDDL</span><span class="o">(</span><span class="n">ddlORC</span><span class="o">.</span><span class="na">toString</span><span class="o">());</span> <span class="err"> </span>  
    <span class="o">}</span> 
    <span class="k">catch</span> <span class="o">(</span><span class="n">Exception</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span> <span class="err"> </span>  
        <span class="n">String</span> <span class="n">errorMessage</span> <span class="o">=</span> <span class="s">"Error exexcuting query["</span><span class="o">+</span><span class="n">ddl</span><span class="o">.</span><span class="na">toString</span><span class="o">()</span> <span class="o">+</span> <span class="s">"]"</span><span class="o">;</span> <span class="err"> </span>  
        <span class="n">LOG</span><span class="o">.</span><span class="na">error</span><span class="o">(</span><span class="n">errorMessage</span><span class="o">,</span> <span class="n">e</span><span class="o">);</span> <span class="err"> </span>  
        <span class="k">throw</span> <span class="k">new</span> <span class="nf">RuntimeException</span><span class="o">(</span><span class="n">errorMessage</span><span class="o">,</span> <span class="n">e</span><span class="o">);</span> <span class="err"> </span>  
    <span class="o">}</span>  
<span class="o">}</span>  
</code></pre>
</div>

<p>6. <code class="highlighter-rouge">TruckEventProcessingTopology.java</code></p>

<p>This creates a connection to HBase tables and access data within the <code class="highlighter-rouge">prepare()</code> function.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="kd">public</span> <span class="kt">void</span> <span class="nf">prepare</span><span class="o">(</span><span class="n">Map</span> <span class="n">stormConf</span><span class="o">,</span> <span class="n">TopologyContext</span> <span class="n">context</span><span class="o">,</span> <span class="n">OutputCollector</span> <span class="n">collector</span><span class="o">)</span> <span class="err"> </span>  
<span class="o">{</span>  
    <span class="o">...</span>  
    <span class="k">this</span><span class="o">.</span><span class="na">connection</span> <span class="o">=</span> <span class="n">HConnectionManager</span><span class="o">.</span><span class="na">createConnection</span><span class="o">(</span><span class="n">constructConfiguration</span><span class="o">());</span> <span class="err"> </span>  
    <span class="k">this</span><span class="o">.</span><span class="na">eventsCountTable</span> <span class="o">=</span> <span class="n">connection</span><span class="o">.</span><span class="na">getTable</span><span class="o">(</span><span class="n">EVENTS_COUNT_TABLE_NAME</span><span class="o">);</span> <span class="err"> </span>  
    <span class="k">this</span><span class="o">.</span><span class="na">eventsTable</span> <span class="o">=</span> <span class="n">connection</span><span class="o">.</span><span class="na">getTable</span><span class="o">(</span><span class="n">EVENTS_TABLE_NAME</span><span class="o">);</span> <span class="err"> </span>  
<span class="o">}</span>  

    <span class="o">...</span>  
<span class="o">}</span>  
</code></pre>
</div>

<p>Data to be stored is prepared in the <code class="highlighter-rouge">constructRow()</code> function using <code class="highlighter-rouge">put.add()</code>.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="kd">private</span> <span class="n">Put</span> <span class="nf">constructRow</span><span class="o">(</span><span class="n">String</span> <span class="n">columnFamily</span><span class="o">,</span> <span class="n">String</span> <span class="n">driverId</span><span class="o">,</span> <span class="n">String</span> <span class="n">truckId</span><span class="o">,</span> <span class="err"> </span>  
<span class="n">Timestamp</span> <span class="n">eventTime</span><span class="o">,</span> <span class="n">String</span> <span class="n">eventType</span><span class="o">,</span> <span class="n">String</span> <span class="n">latitude</span><span class="o">,</span> <span class="n">String</span> <span class="n">longitude</span><span class="o">)</span> <span class="err"> </span>  
<span class="o">{</span>  
    <span class="n">String</span> <span class="n">rowKey</span> <span class="o">=</span> <span class="n">consructKey</span><span class="o">(</span><span class="n">driverId</span><span class="o">,</span> <span class="n">truckId</span><span class="o">,</span> <span class="n">eventTime</span><span class="o">);</span> <span class="err"> </span>  
    <span class="o">...</span>   
    <span class="n">put</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">CF_EVENTS_TABLE</span><span class="o">,</span> <span class="n">COL_DRIVER_ID</span><span class="o">,</span> <span class="n">Bytes</span><span class="o">.</span><span class="na">toBytes</span><span class="o">(</span><span class="n">driverId</span><span class="o">));</span> <span class="err"> </span>  
    <span class="n">put</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="n">CF_EVENTS_TABLE</span><span class="o">,</span> <span class="n">COL_TRUCK_ID</span><span class="o">,</span> <span class="n">Bytes</span><span class="o">.</span><span class="na">toBytes</span><span class="o">(</span><span class="n">truckId</span><span class="o">));</span>  
    <span class="o">...</span>   
<span class="o">}</span>  
</code></pre>
</div>

<p>This executes the <code class="highlighter-rouge">getInfractionCountForDriver()</code> to get the count of events for a driver using driverID and stores the data in HBase with <code class="highlighter-rouge">constructRow()</code> function.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="kd">public</span> <span class="kt">void</span> <span class="nf">execute</span><span class="o">(</span><span class="n">Tuple</span> <span class="n">tuple</span><span class="o">)</span> <span class="err"> </span>  
<span class="o">{</span>  
    <span class="o">...</span> <span class="err"> </span>  
    <span class="kt">long</span> <span class="n">incidentTotalCount</span> <span class="o">=</span> <span class="n">getInfractionCountForDriver</span><span class="o">(</span><span class="n">driverId</span><span class="o">);</span>  
    <span class="o">...</span>  
        <span class="n">Put</span> <span class="n">put</span> <span class="o">=</span> <span class="n">constructRow</span><span class="o">(</span><span class="n">EVENTS_TABLE_NAME</span><span class="o">,</span> <span class="n">driverId</span><span class="o">,</span> <span class="n">truckId</span><span class="o">,</span> <span class="n">eventTime</span><span class="o">,</span> <span class="n">eventType</span><span class="o">,</span> <span class="err"> </span>  
                            <span class="n">latitude</span><span class="o">,</span> <span class="n">longitude</span><span class="o">);</span> <span class="err"> </span>  
        <span class="k">this</span><span class="o">.</span><span class="na">eventsTable</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="n">put</span><span class="o">);</span>  
    <span class="o">...</span> <span class="err"> </span>  
            <span class="n">incidentTotalCount</span> <span class="o">=</span> <span class="k">this</span><span class="o">.</span><span class="na">eventsCountTable</span><span class="o">.</span><span class="na">incrementColumnValue</span><span class="o">(</span><span class="n">Bytes</span><span class="o">.</span><span class="na">toBytes</span><span class="o">(</span><span class="n">driverId</span><span class="o">),</span> <span class="n">CF_EVENTS_COUNT_TABLE</span><span class="o">,</span> <span class="err"> </span>  
    <span class="o">...</span> <span class="err"> </span>  
<span class="o">}</span>  
</code></pre>
</div>

<p>7. <code class="highlighter-rouge">TruckEventProcessingTopology.java</code></p>

<p>HDFS and HBase Bolt configurations created within configureHDFSBolt() and configureHBaseBolt() respectively.</p>

<div class="language-java highlighter-rouge"><pre class="highlight"><code><span class="kd">public</span> <span class="kt">void</span> <span class="nf">configureHDFSBolt</span><span class="o">(</span><span class="n">TopologyBuilder</span> <span class="n">builder</span><span class="o">)</span> <span class="err"> </span>  
<span class="o">{</span>  
    <span class="n">HdfsBolt</span> <span class="n">hdfsBolt</span> <span class="o">=</span> <span class="k">new</span> <span class="n">HdfsBolt</span><span class="o">()</span> <span class="err"> </span>  
                    <span class="o">.</span><span class="na">withFsUrl</span><span class="o">(</span><span class="n">fsUrl</span><span class="o">)</span> <span class="err"> </span>  
            <span class="o">.</span><span class="na">withFileNameFormat</span><span class="o">(</span><span class="n">fileNameFormat</span><span class="o">)</span> <span class="err"> </span>  
            <span class="o">.</span><span class="na">withRecordFormat</span><span class="o">(</span><span class="n">format</span><span class="o">)</span> <span class="err"> </span>  
            <span class="o">.</span><span class="na">withRotationPolicy</span><span class="o">(</span><span class="n">rotationPolicy</span><span class="o">)</span> <span class="err"> </span>  
            <span class="o">.</span><span class="na">withSyncPolicy</span><span class="o">(</span><span class="n">syncPolicy</span><span class="o">)</span> <span class="err"> </span>  
            <span class="o">.</span><span class="na">addRotationAction</span><span class="o">(</span><span class="n">hivePartitionAction</span><span class="o">);</span>  
<span class="o">}</span> <span class="err"> </span>  

<span class="kd">public</span> <span class="kt">void</span> <span class="nf">configureHBaseBolt</span><span class="o">(</span><span class="n">TopologyBuilder</span> <span class="n">builder</span><span class="o">)</span> <span class="err"> </span>  
<span class="o">{</span>  
    <span class="n">TruckHBaseBolt</span> <span class="n">hbaseBolt</span> <span class="o">=</span> <span class="k">new</span> <span class="n">TruckHBaseBolt</span><span class="o">(</span><span class="n">topologyConfig</span><span class="o">);</span> <span class="err"> </span>  
    <span class="n">builder</span><span class="o">.</span><span class="na">setBolt</span><span class="o">(</span><span class="n">HBASE_BOLT_ID</span><span class="o">,</span> <span class="n">hbaseBolt</span><span class="o">,</span> <span class="mi">2</span><span class="o">).</span><span class="na">shuffleGrouping</span><span class="o">(</span><span class="n">KAFKA_SPOUT_ID</span><span class="o">);</span> <span class="err"> </span>  
<span class="o">}</span>  
</code></pre>
</div>

<h3 id="appendix-a-update-tutorials-master-project-a-idupdate-tutorials-master-project-lab3a">Appendix A: Update Tutorials-master Project <a id="update-tutorials-master-project-lab3"></a></h3>

<ul>
  <li>Copy /etc/hbase/conf/hbase-site.xml to src/main/resources/ directory</li>
</ul>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="o">[</span>root@sandbox ~]# <span class="nb">cd</span> /opt/TruckEvents/Tutorials-master
<span class="o">[</span>root@sandbox ~]# cp /etc/hbase/conf/hbase-site.xml src/main/resources/
</code></pre>
</div>

<ul>
  <li>Check pom.xml to ensure it includes the below dependencies (check after <strong>line 104</strong>)</li>
</ul>

<div class="language-html highlighter-rouge"><pre class="highlight"><code>    <span class="nt">&lt;dependency&gt;</span>
      <span class="nt">&lt;groupId&gt;</span>xerces<span class="nt">&lt;/groupId&gt;</span>
      <span class="nt">&lt;artifactId&gt;</span>xercesImpl<span class="nt">&lt;/artifactId&gt;</span>
      <span class="nt">&lt;version&gt;</span>2.9.1<span class="nt">&lt;/version&gt;</span>
    <span class="nt">&lt;/dependency&gt;</span>
    
    <span class="nt">&lt;dependency&gt;</span>
      <span class="nt">&lt;groupId&gt;</span>xalan<span class="nt">&lt;/groupId&gt;</span>
      <span class="nt">&lt;artifactId&gt;</span>xalan<span class="nt">&lt;/artifactId&gt;</span>
      <span class="nt">&lt;version&gt;</span>2.7.1<span class="nt">&lt;/version&gt;</span>
    <span class="nt">&lt;/dependency&gt;</span>
    
    <span class="nt">&lt;dependency&gt;</span>
      <span class="nt">&lt;groupId&gt;</span>org.htrace<span class="nt">&lt;/groupId&gt;</span>
      <span class="nt">&lt;artifactId&gt;</span>htrace-core<span class="nt">&lt;/artifactId&gt;</span>
      <span class="nt">&lt;version&gt;</span>3.0.4<span class="nt">&lt;/version&gt;</span>
    <span class="nt">&lt;/dependency&gt;</span>
    
    <span class="nt">&lt;dependency&gt;</span>
      <span class="nt">&lt;groupId&gt;</span>org.apache.hadoop<span class="nt">&lt;/groupId&gt;</span>
      <span class="nt">&lt;artifactId&gt;</span>hadoop-hdfs<span class="nt">&lt;/artifactId&gt;</span>
      <span class="nt">&lt;version&gt;</span>2.6.0<span class="nt">&lt;/version&gt;</span>
    <span class="nt">&lt;/dependency&gt;</span>
</code></pre>
</div>

<ul>
  <li>recompile the Maven project. This may run for 10+ min</li>
</ul>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="o">[</span>root@sandbox ~]# mvn clean package
</code></pre>
</div>

<p><img src="/assets/realtime-event-processing/t3-update/image15.png" alt="update project" /></p>

<p>The maven build should succeed</p>

<h3 id="appendix-b-enable-remote-desktop-on-sandbox-and-set-up-storm-topology-as-eclipse-project-a-idenable-remote-desktop-setup-topology-lab3a">Appendix B: Enable remote desktop on sandbox and set up Storm topology as Eclipse project <a id="enable-remote-desktop-setup-topology-lab3"></a></h3>

<ol>
  <li>Setup Ambari VNC service on the sandbox to enable remote desktop via VNC and install eclipse using steps here <a href="https://github.com/hortonworks-gallery/ambari-vnc-service%23setup-vnc-service">https://github.com/hortonworks-gallery/ambari-vnc-service#setup-vnc-service</a></li>
  <li>Import code as Eclipse project using steps here:</li>
</ol>

<p><a href="https://github.com/hortonworks-gallery/ambari-vnc-service%23getting-started-with-storm-and-maven-in-eclipse-environment">https://github.com/hortonworks-gallery/ambari-vnc-service#getting-started-with-storm-and-maven-in-eclipse-environment</a></p>

<h2 id="further-reading-a-idfurther-reading-lab3a">Further Reading <a id="further-reading-lab3"></a></h2>
<ul>
  <li><a href="http://hortonworks.com/hadoop/hbase/">Apache HBase</a></li>
  <li><a href="https://hbase.apache.org/book.html#quickstart">Getting Started with HBase</a></li>
  <li><a href="http://storm.apache.org/documentation/storm-hive.html">Storm Hive Integration</a></li>
</ul>

</div>

<div id="tutorial-footer">
  <hr>
  <h2>Tutorial Q&amp;A and Reporting Issues</h2>
  <p>If you need help or have questions with this tutorial, please first check HCC for existing Answers to questions on this tutorial using the Find Answers button.  If you don't find your answer you can post a new HCC question for this tutorial using the Ask Questions button.</p>
  <p><a class="btn" href="https://community.hortonworks.com/topics/tutorial-180.html" role="button">Find Answers</a> <a class="btn pull-right" href="https://community.hortonworks.com/questions/ask.html?space=81&topics=tutorial-180&topics=hdp-2.4.0" role="button">Ask Questions</a></p>
  <p>Tutorial Name: <strong>Real time Data Ingestion in HBase & Hive using Storm Bolt</strong></p>
  <p>HCC Tutorial Tag:<strong> tutorial-180</strong> and <strong>hdp-2.4.0</strong></p>
  <p>If the tutorial has multiple labs please indicate which lab your question corresponds to. Please provide any feedback related to that lab.</p>
  <p>All Hortonworks, partner and community tutorials are posted in the Hortonworks github and can be contributed via the <a href="https://github.com/hortonworks/tutorials/wiki">Hortonworks Tutorial Contribution Guide</a>.  If you are certain there is an issue or bug with the tutorial, please <a href="https://github.com/hortonworks/tutorials/wiki#issues-with-tutorials">create an issue</a> on the repository and we will do our best to resolve it!</p>
</div>
