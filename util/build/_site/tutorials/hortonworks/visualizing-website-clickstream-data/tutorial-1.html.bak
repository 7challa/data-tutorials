

<div class="tutorial-content">
  <h2 id="lab-1-perform-web-log-analytics-with-hive">Lab 1: Perform Web Log Analytics with Hive</h2>

<h2 id="introduction">Introduction</h2>

<p>This tutorial describes how to ingest clickstream data into HDFS, then use HCatalog to create tables and perform queries on those tables with Hive to analyze the web logs from that data. By the end of the tutorial, we will have a better understanding of how to perform web log analysis on clickstream data, so we can better understand the habits of our customers.</p>

<h3 id="overview">Overview</h3>

<p>To load data into the Hortonworks sandbox, you will:</p>

<ul>
  <li>Download sample data to your computer.</li>
  <li>Upload the data files into the sandbox</li>
  <li>View and refine the data in the sandbox.</li>
</ul>

<h3 id="prerequisites">Prerequisites:</h3>

<ul>
  <li>Hortonworks Sandbox 2.5 (installed and running)</li>
</ul>

<h2 id="outline">Outline</h2>
<ul>
  <li><a href="clickstream-data">Clickstream Data</a></li>
  <li><a href="uses-clickstream-data">Potential Uses of Clickstream Data</a></li>
  <li><a href="download-data">Step 1: Download the Sample Data</a></li>
  <li><a href="upload-data-files-sandbox">Step 2: Upload the Data Files into the Sandbox</a></li>
  <li><a href="create-hive-tables">Step 3: Create Hive Tables</a></li>
  <li><a href="load-data-to-tables">Step 4: Load data into new tables</a></li>
  <li><a href="view-refine-data">Step 5: View and Refine the Data in the Sandbox</a></li>
  <li><a href="summary-lab1">Summary</a></li>
  <li><a href="further-reading-lab1">Further Reading</a></li>
</ul>

<h3 id="clickstream-data-">Clickstream Data <a id="clickstream-data"></a></h3>

<p>Clickstream data is an information trail a user leaves behind while visiting a website. It is typically captured in semi-structured website log files.</p>

<p>These website log files contain data elements such as a date and time stamp, the visitor’s IP address, the destination URLs of the pages visited, and a user ID that uniquely identifies the website visitor.</p>

<h3 id="potential-uses-of-clickstream-data-">Potential Uses of Clickstream Data <a id="uses-clickstream-data"></a></h3>

<p>One of the original uses of Hadoop at Yahoo was to store and process their massive volume of clickstream data. Now enterprises of all types can use Hadoop and the Hortonworks Data Platform (HDP) to refine and analyze clickstream data. They can then answer business questions such as:</p>

<ul>
  <li>What is the most efficient path for a site visitor to research a product, and then buy it?</li>
  <li>What products do visitors tend to buy together, and what are they most likely to buy in the future?</li>
  <li>Where should I spend resources on fixing or enhancing the user experience on my website?</li>
</ul>

<p>In this tutorial, we will focus on the “path optimization” use case. Specifically: how can we improve our website to reduce bounce rates and improve conversion?</p>

<hr />

<h3 id="step-1-download-the-sample-data-">Step 1: Download the Sample Data <a id="download-data"></a></h3>

<p>A set of sample data contained in a compressed (.zip) folder can be downloaded here:</p>

<p><a href="https://s3.amazonaws.com/hw-sandbox/tutorial8/RefineDemoData.zip">RefineDemoData.zip</a></p>

<p>Save the sample data .zip file to your computer, then extract the files and unzip <code class="highlighter-rouge">Omniture.0.tsv.gz</code>, <code class="highlighter-rouge">user.tsv.gz</code> and <code class="highlighter-rouge">products.tsv.gz</code>.</p>

<p><strong>Note</strong>: The extracted data files should have a .tsv file extension at the end.</p>

<h3 id="step-2-upload-the-data-files-into-the-sandbox-">Step 2: Upload the Data Files into the Sandbox <a id="upload-data-files-sandbox"></a></h3>

<p>First Log in to the Ambari interface at <a href="http://localhost:8080">http://localhost:8080</a>. You can log in with the username <code class="highlighter-rouge">maria_dev</code> and the password <code class="highlighter-rouge">maria_dev</code></p>

<p>Select the <code class="highlighter-rouge">Files view</code> from the <code class="highlighter-rouge">vies menu</code> <img src="/assets/clickstream/lab1/views_menu.png" alt="views_menu" /> at the top. The HDFS Files view enables users to view Hortonworks Data Platform(HDP) file store. The HDP file system is separate from the local file system.</p>

<p>We navigate to <code class="highlighter-rouge">/tmp</code>, create a <strong>maria</strong> folder</p>

<p><img src="/assets/clickstream/lab1/files_view.png" alt="files_view" /></p>

<p>click on the row of <code class="highlighter-rouge">maria</code> and select <strong>Permissions</strong>:</p>

<p>Now we check the <code class="highlighter-rouge">Write buttons</code> and press save.</p>

<p><img src="/assets/clickstream/lab1/modify_permissions.png" alt="modify_permissions" /></p>

<p>Verify that the permissions look now like this:</p>

<p><img src="/assets/clickstream/lab1/verify_permissions.png" alt="verify_permissions" /></p>

<p>Now, we navigate to <code class="highlighter-rouge">/tmp/maria</code>, click on upload and browse the <code class="highlighter-rouge">Omniture.0.tsv</code>.</p>

<p>Repeat this procedure for <code class="highlighter-rouge">users.tsv</code> and <code class="highlighter-rouge">products.tsv</code>.</p>

<h3 id="step-3-create-hive-tables-">Step 3: Create Hive Tables <a id="create-hive-tables"></a></h3>

<p>Let’s open the <code class="highlighter-rouge">Hive View</code> by clicking on the Hive button from the <code class="highlighter-rouge">views menu</code> <img src="/assets/clickstream/lab1/views_menu.png" alt="views_menu" />.</p>

<p>Let’s create the tables: <strong>users</strong>, <strong>products</strong> and <strong>omniture</strong>.</p>

<h3 id="31-create-users-table">3.1 Create users Table</h3>

<div class="language-sql highlighter-rouge"><pre class="highlight"><code><span class="k">create</span> <span class="k">table</span> <span class="n">users</span> <span class="p">(</span><span class="n">swid</span> <span class="n">STRING</span><span class="p">,</span> <span class="n">birth_dt</span> <span class="n">STRING</span><span class="p">,</span> <span class="n">gender_cd</span> <span class="n">CHAR</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="k">ROW</span> <span class="n">FORMAT</span> <span class="n">DELIMITED</span>
<span class="n">FIELDS</span> <span class="n">TERMINATED</span> <span class="k">by</span> <span class="s1">'</span><span class="se">\t</span><span class="s1">'</span>
<span class="n">stored</span> <span class="k">as</span> <span class="n">textfile</span>
<span class="n">tblproperties</span> <span class="p">(</span><span class="nv">"skip.header.line.count"</span><span class="o">=</span><span class="nv">"1"</span><span class="p">);</span>
</code></pre>
</div>

<p><img src="/assets/clickstream/lab1/users_hive_table_create.png" alt="users_hive_table_create" /></p>

<h3 id="32-create-products-table">3.2 Create products Table</h3>

<div class="language-sql highlighter-rouge"><pre class="highlight"><code><span class="k">create</span> <span class="k">table</span> <span class="n">products</span> <span class="p">(</span><span class="n">url</span> <span class="n">STRING</span><span class="p">,</span> <span class="n">category</span> <span class="n">STRING</span><span class="p">)</span>
<span class="k">ROW</span> <span class="n">FORMAT</span> <span class="n">DELIMITED</span>
<span class="n">FIELDS</span> <span class="n">TERMINATED</span> <span class="k">by</span> <span class="s1">'</span><span class="se">\t</span><span class="s1">'</span>
<span class="n">stored</span> <span class="k">as</span> <span class="n">textfile</span>
<span class="n">tblproperties</span> <span class="p">(</span><span class="nv">"skip.header.line.count"</span><span class="o">=</span><span class="nv">"1"</span><span class="p">);</span>
</code></pre>
</div>

<h3 id="33-create-omniturelogs-table">3.3 Create omniturelogs Table</h3>

<div class="language-sql highlighter-rouge"><pre class="highlight"><code><span class="k">create</span> <span class="k">table</span> <span class="n">omniturelogs</span> <span class="p">(</span><span class="n">col_1</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_2</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_3</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_4</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_5</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_6</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_7</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_8</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_9</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_10</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_11</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_12</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_13</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_14</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_15</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_16</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_17</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_18</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_19</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_20</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_21</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_22</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_23</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_24</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_25</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_26</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_27</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_28</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_29</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_30</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_31</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_32</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_33</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_34</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_35</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_36</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_37</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_38</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_39</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_40</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_41</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_42</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_43</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_44</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_45</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_46</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_47</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_48</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_49</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_50</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_51</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_52</span> <span class="n">STRING</span><span class="p">,</span><span class="n">col_53</span> <span class="n">STRING</span><span class="p">)</span>
<span class="k">ROW</span> <span class="n">FORMAT</span> <span class="n">DELIMITED</span>
<span class="n">FIELDS</span> <span class="n">TERMINATED</span> <span class="k">by</span> <span class="s1">'</span><span class="se">\t</span><span class="s1">'</span>
<span class="n">stored</span> <span class="k">as</span> <span class="n">textfile</span>
<span class="n">tblproperties</span> <span class="p">(</span><span class="nv">"skip.header.line.count"</span><span class="o">=</span><span class="nv">"1"</span><span class="p">);</span>
</code></pre>
</div>

<h3 id="step-4-load-data-into-new-tables-">Step 4: Load data into new tables <a id="load-data-to-tables"></a></h3>

<p>Let’s execute the following queries to load the data into the tables.</p>

<div class="language-sql highlighter-rouge"><pre class="highlight"><code><span class="k">LOAD</span> <span class="k">DATA</span> <span class="n">INPATH</span> <span class="s1">'/tmp/maria/products.tsv'</span> <span class="n">OVERWRITE</span> <span class="k">INTO</span> <span class="k">TABLE</span> <span class="n">products</span><span class="p">;</span>
<span class="k">LOAD</span> <span class="k">DATA</span> <span class="n">INPATH</span> <span class="s1">'/tmp/maria/users.tsv'</span> <span class="n">OVERWRITE</span> <span class="k">INTO</span> <span class="k">TABLE</span> <span class="n">users</span><span class="p">;</span>
<span class="k">LOAD</span> <span class="k">DATA</span> <span class="n">INPATH</span> <span class="s1">'/tmp/maria/Omniture.0.tsv'</span> <span class="n">OVERWRITE</span> <span class="k">INTO</span> <span class="k">TABLE</span> <span class="n">omniturelogs</span><span class="p">;</span>
</code></pre>
</div>

<p><img src="/assets/clickstream/lab1/load_dataset_hive_tables.png" alt="load_dataset_hive_tables" /></p>

<h3 id="41-verify-data-loaded-correctly">4.1 Verify data loaded correctly</h3>

<p>To check if the data was loaded, click on the <strong>load sample data</strong> icon <img src="/assets/clickstream/lab1/load_sample_data_icon.png" alt="load_sample_data_icon" /> next to the table name. It executes a sample query.</p>

<p><img src="/assets/clickstream/lab1/load_sample_data_users_table.png" alt="load_sample_data_users_table" /></p>

<blockquote>
  <p>Note: repeat the procedure to verify that the dataset was loaded into the products and omniturelogs tables.</p>
</blockquote>

<h3 id="step-5-view-and-refine-the-data-in-the-sandbox-">Step 5: View and Refine the Data in the Sandbox <a id="view-refine-data"></a></h3>

<p>In the previous section, we created sandbox tables from uploaded data files. Now let’s take a closer look at that data.</p>

<p>Here’s a summary of the data we’re working with:</p>

<p>Switch to your local machine, navigate to the location of the datasets before sending them to HDP, then open omniturelogs in the text editor or spreadsheet of your choice:</p>

<p><strong>omniturelogs</strong> – website logs containing information such as URL, timestamp, IP address, geocoded IP, and session ID.</p>

<p><img src="/assets/clickstream/lab1/omniturelogs_dataset_part1.png" alt="omniturelogs_dataset" />
<img src="/assets/clickstream/lab1/omniturelogs_dataset_part2.png" alt="omniturelogs_dataset" /></p>

<p><strong>users</strong> – CRM user data listing SWIDs (Software User IDs) along with date of birth and gender.</p>

<p><img src="/assets/clickstream/lab1/users_dataset.png" alt="users_dataset" /></p>

<p><strong>products</strong> – CMS data that maps product categories to website URLs.</p>

<p><img src="/assets/clickstream/lab1/products_dataset.png" alt="products_dataset" /></p>

<p>Now let’s use a Hive script to generate an “omniture” view that contains a subset of the data in the Omniture log table.</p>

<div class="language-sql highlighter-rouge"><pre class="highlight"><code><span class="k">CREATE</span> <span class="k">VIEW</span> <span class="n">omniture</span> <span class="k">AS</span>
<span class="k">SELECT</span> <span class="n">col_2</span> <span class="n">ts</span><span class="p">,</span> <span class="n">col_8</span> <span class="n">ip</span><span class="p">,</span> <span class="n">col_13</span> <span class="n">url</span><span class="p">,</span> <span class="n">col_14</span> <span class="n">swid</span><span class="p">,</span> <span class="n">col_50</span> <span class="n">city</span><span class="p">,</span> <span class="n">col_51</span> <span class="n">country</span><span class="p">,</span> <span class="n">col_53</span> <span class="k">state</span>
<span class="k">FROM</span> <span class="n">omniturelogs</span>
</code></pre>
</div>

<p>Click <strong>Save as…</strong>. On the “Saving item” pop-up, type “omniture” in the box, then click OK.</p>

<p><img src="/assets/clickstream/lab1/save_as_hive_script.png" alt="save_as_hive_script" /></p>

<p>You can see your saved query now by clicking on the “Save Queries” button at the top.</p>

<p><img src="/assets/clickstream/lab1/save_queries.png" alt="save_queries" /></p>

<p>Click <strong>Execute</strong> to run the script.</p>

<p>To view the data generated by the saved script, click on the icon next to the view’s name at the Database Explorer.
The query results will appear, and you can see that the results include the data from the omniturelogs table that were specified in the query.</p>

<p><img src="/assets/clickstream/lab1/omniture_sample_data.png" alt="" /></p>

<p>Finally, we’ll create a script that <strong>joins</strong> the omniture website log data to the CRM data (registered users) and CMS data (products). Click Query Editor, then paste the following text in the Query box:</p>

<div class="language-sql highlighter-rouge"><pre class="highlight"><code><span class="k">create</span> <span class="k">table</span> <span class="n">webloganalytics</span> <span class="k">as</span>
<span class="k">select</span> <span class="n">to_date</span><span class="p">(</span><span class="n">o</span><span class="p">.</span><span class="n">ts</span><span class="p">)</span> <span class="n">logdate</span><span class="p">,</span> <span class="n">o</span><span class="p">.</span><span class="n">url</span><span class="p">,</span> <span class="n">o</span><span class="p">.</span><span class="n">ip</span><span class="p">,</span> <span class="n">o</span><span class="p">.</span><span class="n">city</span><span class="p">,</span> <span class="k">upper</span><span class="p">(</span><span class="n">o</span><span class="p">.</span><span class="k">state</span><span class="p">)</span> <span class="k">state</span><span class="p">,</span>
<span class="n">o</span><span class="p">.</span><span class="n">country</span><span class="p">,</span> <span class="n">p</span><span class="p">.</span><span class="n">category</span><span class="p">,</span> <span class="k">CAST</span><span class="p">(</span><span class="n">datediff</span><span class="p">(</span> <span class="n">from_unixtime</span><span class="p">(</span> <span class="n">unix_timestamp</span><span class="p">()</span> <span class="p">),</span>
<span class="n">from_unixtime</span><span class="p">(</span> <span class="n">unix_timestamp</span><span class="p">(</span><span class="n">u</span><span class="p">.</span><span class="n">birth_dt</span><span class="p">,</span> <span class="s1">'dd-MMM-yy'</span><span class="p">)))</span> <span class="o">/</span> <span class="mi">365</span>  <span class="k">AS</span> <span class="n">INT</span><span class="p">)</span> <span class="n">age</span><span class="p">,</span> <span class="n">u</span><span class="p">.</span><span class="n">gender_cd</span>
<span class="k">from</span> <span class="n">omniture</span> <span class="n">o</span>
<span class="k">inner</span> <span class="k">join</span> <span class="n">products</span> <span class="n">p</span>     
<span class="k">on</span> <span class="n">o</span><span class="p">.</span><span class="n">url</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">url</span>
<span class="k">left</span> <span class="k">outer</span> <span class="k">join</span> <span class="n">users</span> <span class="n">u</span>
<span class="k">on</span> <span class="n">o</span><span class="p">.</span><span class="n">swid</span> <span class="o">=</span> <span class="n">concat</span><span class="p">(</span><span class="s1">'{'</span><span class="p">,</span> <span class="n">u</span><span class="p">.</span><span class="n">swid</span> <span class="p">,</span> <span class="s1">'}'</span><span class="p">)</span>
</code></pre>
</div>

<p>Save this script as “webloganalytics” and execute the script.</p>

<p>Let’s view the data generated by the script with the procedure we learned in the previous steps.</p>

<p><img src="/assets/clickstream/lab1/view_webloganalytics_data.png" alt="view_webloganalytics_data" /></p>

<p>Now that you have loaded data into the Hortonworks Platform, you can use Business Intelligence (BI) applications or Data Science Notebooks such as Microsoft Excel or Apache Zeppelin to access and analyze the data.</p>

<h2 id="summary-">Summary <a id="summary-lab1"></a></h2>
<p>Congratulations! We uploaded similar datasets for users, products and omniturelogs into HDFS. We then used Hive view to create Hive scripts that create tables and load them with datasets. We then learned to perform ETL operations on the datasets, such as joining our datasets and then further filtering down the data we wanted for our use case of webloganalytics pertaining to when, how and by whom our web server was visited. Thus, we were able to extract the following attributes: logdate, url, ip, city, state, country, category, age and gender into a new table about our customers, so we can better understand them.</p>

<h2 id="further-reading-">Further Reading <a id="further-reading-lab1"></a></h2>
<ul>
  <li>Explore Apache Hive in <a href="https://community.hortonworks.com/repos/51932/retail-analytics-demo.html">Retail Analytics Demo</a> along with other components from the HDP stack.</li>
  <li><a href="http://hortonworks.com/wp-content/uploads/2013/05/hql_cheat_sheet.pdf">SQL to HQL Cheat Sheet</a></li>
  <li><a href="http://hortonworks.com/apache/hive/#blog">Hive in the Blog</a></li>
  <li><a href="http://hortonworks.com/apache/hive/#webinars">Hive Webinars &amp; Presentations</a></li>
</ul>

</div>

<div id="tutorial-footer">
  <hr>
  <h2>Tutorial Q&amp;A and Reporting Issues</h2>
  <p>If you need help or have questions with this tutorial, please first check HCC for existing Answers to questions on this tutorial using the Find Answers button.  If you don't find your answer you can post a new HCC question for this tutorial using the Ask Questions button.</p>
  <p><a class="btn" href="https://community.hortonworks.com/topics/tutorial-250.html" role="button">Find Answers</a> <a class="btn pull-right" href="https://community.hortonworks.com/questions/ask.html?space=81&topics=tutorial-250&topics=hdp-2.5.0" role="button">Ask Questions</a></p>
  <p>Tutorial Name: <strong>How to Visualize Website Clickstream Data</strong></p>
  <p>HCC Tutorial Tag:<strong> tutorial-250</strong> and <strong>hdp-2.5.0</strong></p>
  <p>If the tutorial has multiple labs please indicate which lab your question corresponds to. Please provide any feedback related to that lab.</p>
  <p>All Hortonworks, partner and community tutorials are posted in the Hortonworks github and can be contributed via the <a href="https://github.com/hortonworks/tutorials/wiki">Hortonworks Tutorial Contribution Guide</a>.  If you are certain there is an issue or bug with the tutorial, please <a href="https://github.com/hortonworks/tutorials/wiki#issues-with-tutorials">create an issue</a> on the repository and we will do our best to resolve it!</p>
</div>
