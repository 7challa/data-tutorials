

<div class="tutorial-content">
  <h2 id="introduction">Introduction</h2>

<p>In this tutorial, you will strengthen your foundation of technologies used in real-time event processing. You will learn in detail how Apache Kafka sends messages, the process Apache Storm undergoes to collect that data and the process involved for HBase to read that streaming data.</p>

<h2 id="pre-requisites">Pre-Requisites</h2>
<ul>
  <li>Downloaded and Installed the latest <a href="http://hortonworks.com/products/hortonworks-sandbox/#install">Hortonworks Sandbox</a></li>
  <li><a href="http://hortonworks.com/hadoop-tutorial/learning-the-ropes-of-the-hortonworks-sandbox/">Learning the Ropes of the Hortonworks Sandbox</a></li>
</ul>

<h2 id="outline">Outline</h2>
<ul>
  <li><a href="#concepts-apache-nifi">1st Concept: Apache NiFi</a></li>
  <li><a href="#concepts-apache-kafka">2nd Concept: Apache Kafka</a></li>
  <li><a href="#concepts-apache-storm">3rd Concept: Apache Storm</a></li>
  <li><a href="#concepts-kafka-on-storm">5th Concept: Kafka on Storm</a></li>
  <li><a href="#further-reading-concepts">Further Reading</a></li>
</ul>

<h2 id="1st-concept-apache-nifi-">1st Concept: Apache NiFi <a id="concepts-apache-nifi"></a></h2>

<h2 id="introduction-1">Introduction:</h2>

<p>NiFi works with Apache Kafka, Apache Storm, Apache HBase and Spark for real-time distributed messaging of streaming data. NiFi is an excellent platform for ingesting real-time streaming data sources, such as the internet of things, sensors and transactional systems. If the data that comes in is garbage data, NiFi offers tools to filter out the undesired data. Additionally, NiFi can also act as a messenger and send data to the desired location.</p>

<h2 id="goals-of-this-module">Goals of this module:</h2>

<ul>
  <li>Understand how Apache NiFi works</li>
</ul>

<h2 id="how-nifi-works">How NiFi Works</h2>

<p>NiFi’s system design can be thought of as an Automated Teller Machine, where incoming data is securely processed and written sequentially to disk. There are four main components involved in moving data in and out of NiFi:</p>

<ul>
  <li>FlowFile</li>
  <li>Processor</li>
  <li>Connections</li>
  <li>Flow Controller</li>
</ul>

<p><img src="/assets/realtime-event-processing-with-hdf/concepts/image_of_nifi_flow.png" alt="Image of NiFi Flow" /></p>

<p>In NiFi, a <strong>FlowFile</strong> is data brought into the flow from any data source and moves through the dataflow. <strong>Connections</strong> are linkages between components that enable FlowFiles to move throughout the dataflow. A <strong>Flow Controller</strong> regulates the exchange of FlowFiles between processors. <strong>Processors</strong> are actions taken on the FlowFiles to process their content and attributes to ensure desired data moves throughout the dataflow to eventually be stored at a secure location. Therefore, NiFi acts as a Producer to publish messages to one or more topics. So, at a high level, producers send messages over the network to the Kafka cluster.</p>

<h2 id="2nd-concept-apache-kafka-">2nd Concept: Apache kafka <a id="concepts-apache-kafka"></a></h2>

<h2 id="introduction-2">Introduction:</h2>

<p>In a modern data architecture built on YARN-enabled Apache Hadoop, Kafka works in combination with Apache Storm, Apache HBase and Apache Spark for real-time distributed messaging of streaming data. Kafka is an excellent low latency messaging platform for real-time streaming data sources, such as the internet of things, sensors, and transactional systems.  Whatever the industry or use case, Kafka brokers massive message streams for low-latency analysis in Enterprise Apache Hadoop.
Kafka is fully supported and included in HDP today.</p>

<h2 id="goals-of-this-module-1">Goals of this module:</h2>

<ul>
  <li>Understand Apache Kafka Architecture</li>
  <li>Understand how Apache Kafka works</li>
</ul>

<h2 id="what-kafka-does">What Kafka Does</h2>

<p>Apache Kafka supports a wide range of use cases as a general-purpose messaging system for scenarios where high throughput, reliable delivery, and horizontal scalability are important. Apache Storm and Apache Spark both work very well in combination with Kafka. Common use cases include:</p>
<ul>
  <li>Stream Processing</li>
  <li>Website Activity Tracking</li>
  <li>Metrics Collection and Monitoring</li>
  <li>Log Aggregation
Some of the important characteristics that make Kafka such an attractive option for these use cases include the following:</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Feature</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Scalability</td>
      <td>Distributed messaging system scales easily with no downtime</td>
    </tr>
    <tr>
      <td>Durability</td>
      <td>Persists messages on disk, and provides intra-cluster replication</td>
    </tr>
    <tr>
      <td>Reliability</td>
      <td>Replicates data, supports multiple consumers, and automatically balances consumers in case of failure</td>
    </tr>
    <tr>
      <td>Performance</td>
      <td>High throughput for both publishing and subscribing, with disk structures that provide constant performance even with many terabytes of stored messages</td>
    </tr>
  </tbody>
</table>

<h2 id="how-kafka-works">How Kafka Works</h2>

<p>Kafka’s system design can be thought of as that of a distributed commit log, where incoming data is written sequentially to disk. There are four main components involved in moving data in and out of Kafka:</p>

<ul>
  <li>Topics</li>
  <li>Producers</li>
  <li>Consumers</li>
  <li>Brokers</li>
</ul>

<p><img src="/assets/realtime-event-processing-with-hdf/concepts/01_kafka_cluster.png" alt="Image of Kafka Flow" /></p>

<p>In Kafka, a <strong>Topic</strong> is a user-defined category to which messages are published. NiFi will act in the role of Producers to publish messages to one or more topics and <strong>Consumers</strong> subscribe to topics and process the published messages. At a high level, producers send messages over the network to the Kafka cluster, which in turn serves them up to consumers. Finally, a Kafka cluster consists of one or more servers, called <strong>Brokers</strong> that manage the persistence and replication of message data (i.e. the commit log).</p>

<p><img src="/assets/realtime-event-processing-with-hdf/concepts/02_kafka_partitions.png" alt="Image of Kafka Partitions" /></p>

<p>One of the keys to Kafka’s high performance is the simplicity of the brokers’ responsibilities. In Kafka, topics consist of one or more Partitions that are ordered, immutable sequences of messages. Since writes to a partition are sequential, this design greatly reduces the number of hard disk seeks (with their resulting latency).
Another factor contributing to Kafka’s performance and scalability is the fact that Kafka brokers are not responsible for keeping track of what messages have been consumed – that responsibility falls on the consumer. In traditional messaging systems, such as JMS, the broker bore this responsibility, severely limiting the system’s ability to scale as the number of consumers increased.</p>

<p><img src="/assets/realtime-event-processing-with-hdf/concepts/03_kafka_zookeeper.png" alt="Image of Brokers w/ Zookeeper" /></p>

<p>For Kafka consumers, keeping track of which messages have been consumed (processed) is simply a matter of keeping track of an <strong>Offset</strong>, which is a sequential id number that uniquely identifies a message within a partition. Because Kafka retains all messages on disk (for a configurable amount of time), consumers can rewind or skip to any point in a partition simply by supplying an offset value. Finally, this design eliminates the potential for back-pressure when consumers process messages at different rates.</p>

<h2 id="2nd-concept-apache-storm-">2nd Concept: Apache Storm <a id="concepts-apache-storm"></a></h2>

<h2 id="introduction-3">Introduction:</h2>

<p>Apache Storm is a distributed real-time computation system for processing large volumes of high-velocity data in parallel and at scale. Storm is to realtime data processing as Apache Hadoop and MapReduce are to batch data processing. With its simple programming interface, Storm allows application developers to write applications that analyze streams of tuples of data; a tuple may can contain object of any type.</p>

<p>At the core of Storm’s data stream processing is a computational topology, which is discussed below. This topology of nodes dictates how tuples are processed, transformed,aggregated, stored, or re-emitted to other nodes in the topology for further processing.</p>

<h2 id="storm-on-apache-hadoop-yarn">Storm on Apache Hadoop YARN</h2>

<p>Storm on YARN is powerful for scenarios requiring  continuous analytics, real-time predictions, and continuous monitoring of operations. Eliminating a need to have dedicated silos, enterprises using Storm on YARN benefit on cost savings (by accessing the same datasets as other engines and applications on the same cluster) and on security, data governance, and operations (by employing the same compute resources managed by YARN.</p>

<h2 id="storm-in-the-enterprise">Storm in the Enterprise</h2>

<p>Some of the specific new business opportunities include: real-time customer service management, data monetization, operational dashboards, or cyber security analytics and threat detection.</p>

<p>Storm is extremely fast, with the ability to process over a million records per second per node on a cluster of modest size. Enterprises harness this speed and combine it with other data access applications in Hadoop to prevent undesirable events or to optimize positive outcomes.</p>

<p>Here are some typical “prevent” and “optimize” use cases for Storm.</p>

<table>
  <thead>
    <tr>
      <th>—</th>
      <th>“Prevent” Use Cases</th>
      <th>“Optimize” Use Cases</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Financial Services</td>
      <td>Securities fraud, Operational risks &amp; compliance violations</td>
      <td>Order routing, Pricing</td>
    </tr>
    <tr>
      <td>Telecom</td>
      <td>Security breaches, Network outages</td>
      <td>Bandwidth allocation, Customer service</td>
    </tr>
    <tr>
      <td>Retail</td>
      <td>Shrinkage, Stock outs</td>
      <td>Offers, Pricing</td>
    </tr>
    <tr>
      <td>Manufacturing</td>
      <td>Preventative maintenance, Quality assurance</td>
      <td>Supply chain optimization, Reduced plant downtime</td>
    </tr>
    <tr>
      <td>Transportation</td>
      <td>Driver monitoring, Predictive maintenance</td>
      <td>Routes, Pricing</td>
    </tr>
    <tr>
      <td>Web</td>
      <td>Application failures, Operational issues</td>
      <td>Personalized content</td>
    </tr>
  </tbody>
</table>

<p>Now with Storm on YARN, a Hadoop cluster can efficiently process a full range of workloads from real-time to interactive to batch. <a href="http://storm.apache.org/about/simple-api.html">Storm</a> is simple, and developers can write Storm topologies using <a href="http://storm.apache.org/about/multi-language.html">any programming language</a>.
Five characteristics make Storm ideal for real-time data processing workloads. Storm is:</p>

<ul>
  <li><strong>Fast</strong> – benchmarked as processing one million 100 byte messages per second per node</li>
  <li><strong>Scalable</strong> – with parallel calculations that run across a cluster of machines</li>
  <li><strong>Fault-tolerant</strong> – when workers die, Storm will automatically restart them. If a node dies, the worker will be restarted on another node.</li>
  <li><strong>Reliable</strong> – Storm guarantees that each unit of data (tuple) will be processed at least once or exactly once. Messages are only replayed when there are failures.
Easy to operate – standard configurations are suitable for production on day one. Once deployed, Storm is easy to operate.</li>
</ul>

<h2 id="how-storm-works">How Storm Works</h2>

<p><em>Storm Cluster Components</em><br />
A storm cluster has three sets of nodes:</p>

<ul>
  <li><strong>Nimbus</strong> node (master node, similar to the Hadoop JobTracker):
    <ul>
      <li>Uploads computations for execution</li>
      <li>Distributes code across the cluster</li>
      <li>Launches workers across the cluster</li>
      <li>Monitors computation and reallocates workers as needed</li>
    </ul>
  </li>
  <li><strong>ZooKeeper</strong> nodes – coordinates the Storm cluster</li>
  <li><strong>Supervisor</strong> nodes – communicates with Nimbus through Zookeeper, starts and stops workers according to signals from Nimbus</li>
</ul>

<p><img src="/assets/realtime-event-processing-with-hdf/concepts/04_storm_layout.png" alt="Image of Storm-Zookeeper" /></p>

<p>Five key abstractions help to understand how Storm processes data:</p>

<ul>
  <li><strong>Tuples</strong>– an ordered list of elements. For example, a “4-tuple” might be (7, 1, 3, 7)</li>
  <li><strong>Streams</strong> – an unbounded sequence of tuples.</li>
  <li><strong>Spouts</strong> –sources of streams in a computation (e.g. a Twitter API)</li>
  <li><strong>Bolts</strong> – process input streams and produce output streams. They can run functions, filter, aggregate, or join data, or talk to databases.</li>
  <li><strong>Topologies</strong> – the overall calculation, represented visually as a network of spouts and bolts (as in the following diagram)</li>
</ul>

<p><img src="/assets/realtime-event-processing-with-hdf/concepts/05_spouts_and_bolts.png" alt="Img of Spouts and Bolts" /></p>

<p>Storm users define topologies for how to process the data when it comes streaming in from the spout. When the data comes in, it is processed and the results are passed onto to other bolts or stored in Hadoop.</p>

<p>Learn more about how the community is working to <a href="http://hortonworks.com/labs/storm">integrate Storm with Hadoop</a> and improve its readiness for the enterprise.</p>

<h2 id="storm-topologies">Storm Topologies</h2>

<p>A Storm cluster is similar to a Hadoop cluster. Whereas on Hadoop you run “MapReduce jobs,” on Storm you run “topologies.” “Jobs” and “topologies” are different – one key difference is that a MapReduce job eventually finishes, whereas a topology processes messages forever (or until you kill it).</p>

<p>There are two kinds of nodes on a Storm cluster: the master node and the worker nodes. The master node runs a daemon called “Nimbus” that is similar to Hadoop’s “JobTracker”. Nimbus is responsible for distributing code around the cluster, assigning tasks to machines, and monitoring for failures.</p>

<p>Each worker node runs a daemon called the “Supervisor.” It  listens for work assigned to its machine and starts and stops worker processes as dictated by  Nimbus. Each worker process executes a subset of a topology; a running topology consists of many worker processes spread across many machines.</p>

<p>All coordination between Nimbus and the Supervisors is done through a <a href="http://zookeeper.apache.org/">Zookeeper</a> cluster. Additionally, the Nimbus daemon and Supervisor daemons are fail-fast and stateless; all state is kept in Zookeeper or on local disk. This means you can kill -9 Nimbus or the Supervisors and they’ll start back up like nothing happened. Hence, Storm clusters are stable and fault-tolerant</p>

<h2 id="streams-within-storm-topologies">Streams Within Storm Topologies</h2>

<p>The core abstraction in Storm is the “stream.” It is  an unbounded sequence of tuples. Storm provides the primitives for transforming a stream into a new stream in a distributed and reliable way. For example, you may transform a stream of tweets into a stream of trending topics.</p>

<p>The basic primitives Storm provides for doing stream transformations are “spouts” and “bolts.” Spouts and bolts have interfaces that you, as an application developer, implement to run your application-specific logic.</p>

<p>A spout is a source of streams. For example, a spout may read tuples off of a Kafka Topics and emit them as a stream. Or a spout may connect to the Twitter API and emit a stream of tweets.</p>

<p>A bolt consumes any number of input streams, does some processing, and possibly emits new streams. Complex stream transformations, like computing a stream of trending topics from a stream of tweets, require multiple steps and thus multiple bolts. Bolts can do anything from run functions, filter tuples, do streaming aggregations, do streaming joins, talk to databases, and more.</p>

<p>Networks of spouts and bolts are packaged into a “topology,” which is the top-level abstraction that you submit to Storm clusters for execution. A topology is a graph of stream transformations where each node is a spout or bolt. Edges in the graph indicate which bolts are subscribing to which streams. When a spout or bolt emits a tuple to a stream, it sends the tuple to every bolt that subscribed to that stream.</p>

<p><img src="/assets/realtime-event-processing-with-hdf/concepts/06_more_spouts_and_bolts.png" alt="Another Img of spouts and bolts" /></p>

<p>Links between nodes in your topology indicate how tuples should be passed around. For example, if there is a link between Spout A and Bolt B, a link from Spout A to Bolt C, and a link from Bolt B to Bolt C, then every time Spout A emits a tuple, it will send the tuple to both Bolt B and Bolt C. All of Bolt B’s output tuples will go to Bolt C as well.</p>

<p>Each node in a Storm topology executes in parallel. In your topology, you can specify how much parallelism you want for each node, and then Storm will spawn that number of threads across the cluster to do the execution.</p>

<p>A topology runs forever, or until you kill it. Storm will automatically reassign any failed tasks. Additionally, Storm guarantees that there will be no data loss, even if machines go down and messages are dropped.</p>

<h2 id="3rd-concept-kafka-on-storm-">3rd Concept: Kafka on Storm <a id="concepts-kafka-on-storm"></a></h2>

<h2 id="introduction-4">Introduction:</h2>

<p>Hortonworks Data Platform’s YARN-based architecture enables multiple applications to share a common cluster and data set while ensuring consistent levels of response made possible by a centralized architecture. Hortonworks led the efforts to on-board open source data processing engines, such as <a href="http://hortonworks.com/hadoop/hive">Apache Hive</a>, <a href="http://hortonworks.com/hadoop/hbase">HBase</a>, <a href="http://hortonworks.com/hadoop/accumulo">Accumulo</a>, <a href="http://hortonworks.com/hadoop/spark">Spark</a>,<a href="http://hortonworks.com/hadoop/storm">Storm</a> and others, on <a href="http://hortonworks.com/hadoop/yarn/">Apache Hadoop YARN</a>.</p>

<p>In this tutorial, we will focus on one of those data processing engines—<a href="http://hortonworks.com/hadoop/storm">Apache Storm</a>—and its relationship with <a href="http://hortonworks.com/hadoop/kafka">Apache Kafka</a>. I will describe how Storm and Kafka form a multi-stage event processing pipeline, discuss some use cases, and explain Storm topologies.</p>

<h2 id="goals-of-this-tutorial">Goals of this tutorial:</h2>

<ul>
  <li>Understand Relationship between Apache Kafka and Apache Storm</li>
  <li>Understand Storm topologies</li>
</ul>

<h2 id="kafka-on-storm">Kafka on Storm:</h2>

<p>An oil refinery takes crude oil, distills it, processes it and refines it into useful finished products such as the gas that we buy at the pump. We can think of Storm with Kafka as a similar refinery, but data is the input. A real-time data refinery converts raw streaming data into finished data products, enabling new use cases and innovative business models for the modern enterprise.</p>

<p>Apache Storm is a distributed real-time computation engine that reliably processes unbounded streams of data. While Storm processes stream data at scale, Apache Kafka processes messages at scale. Kafka is a distributed pub-sub real-time messaging system that provides strong durability and fault tolerance guarantees.
Storm and Kafka naturally complement each other, and their powerful cooperation enables real-time streaming analytics for fast-moving big data. HDP 2.4 contains the results of Hortonworks’ continuing focus on making the Storm-Kafka union even more powerful for stream processing.</p>

<p><img src="/assets/realtime-event-processing-with-hdf/concepts/07_hadoop_cluster.png" alt="Img of Cluster Layout" /></p>

<p>Conceptual Reference Architecture for Real-Time Processing in HDP 2.2</p>

<p><img src="/assets/realtime-event-processing-with-hdf/concepts/08_event_processing_pipeline.png" alt="Image of Event Processing Pipeline" /></p>

<p>Conceptual Introduction to the Event Processing Pipeline</p>

<p>In an event processing pipeline, we can view each stage as a purpose-built step that performs some real-time processing against upstream event streams for downstream analysis. This produces increasingly richer event streams, as data flows through the pipeline:</p>

<ul>
  <li>raw events stream from many sources,</li>
  <li>those are processed to create events of interest, and</li>
  <li>events of interest are analyzed to detect significant events.</li>
</ul>

<p>Here are some typical uses for this event processing pipeline:</p>

<ul>
  <li>a. High Speed Filtering and Pattern Matching</li>
  <li>b. Contextual Enrichment on the Fly</li>
  <li>c. Real-time KPIs, Statistical Analytics, Baselining and Notification</li>
  <li>d. Predictive Analytics</li>
  <li>e. Actions and Decisions</li>
</ul>

<h2 id="build-the-data-refinery-with-topologies">Build the Data Refinery with Topologies</h2>

<p>To perform real-time computation on Storm, we create “topologies.” A topology is a graph of a computation, containing a network of nodes called “Spouts” and “Bolts.” In a Storm topology, a Spout is the source of data streams and a Bolt holds the business logic for analyzing and processing those streams.</p>

<p><img src="/assets/realtime-event-processing-with-hdf/concepts/09_storm_topologies.png" alt="Storm and Kafka Pipelines" /></p>

<p>Hortonworks’ focus for Apache Storm and Kafka has been to make it easier for developers to ingest and publish data streams from Storm topologies. The first topology ingests raw data streams from Kafka and fans out to HDFS, which serves as persistent store for raw events. Next, a filter Bolt emits the enriched event to a downstream Kafka Bolt that publishes it to a Kafka Topic. As events flow through these stages, the system can keep track of data lineage that allows drill-down from aggregated events to its constituents and can be used for forensic analysis. In a multi-stage pipeline architecture, providing right cluster resources to most intense part of the data processing stages is very critical, an “Isolation Scheduler” in Storm provides the ability to easily and safely share a cluster among many topologies.
In summary, refinery style data processing architecture enables you to:</p>

<ul>
  <li>Incrementally add more topologies/use cases</li>
  <li>Tap into raw or refined data streams at any stage of the processing</li>
  <li>Modularize your key cluster resources to most intense processing phase of the pipeline</li>
</ul>

<h2 id="further-reading-">Further Reading <a id="further-reading-concepts"></a></h2>
<ul>
  <li><a href="https://nifi.apache.org/docs/nifi-docs/">Apache NiFi</a></li>
  <li><a href="http://storm.apache.org/">Apache Storm</a></li>
  <li><a href="http://kafka.apache.org/">Apache Kafka</a></li>
  <li><a href="http://storm.apache.org/documentation/storm-kafka.html">Storm Kafka Integration</a></li>
</ul>

</div>

<div id="tutorial-footer">
  <hr>
  <h2>Tutorial Q&amp;A and Reporting Issues</h2>
  <p>If you need help or have questions with this tutorial, please first check HCC for existing Answers to questions on this tutorial using the Find Answers button.  If you don't find your answer you can post a new HCC question for this tutorial using the Ask Questions button.</p>
  <p><a class="btn" href="https://community.hortonworks.com/topics/tutorial-220.html" role="button">Find Answers</a> <a class="btn pull-right" href="https://community.hortonworks.com/questions/ask.html?space=81&topics=tutorial-220&topics=hdp-2.5.0" role="button">Ask Questions</a></p>
  <p>Tutorial Name: <strong>Real Time Data Processing in Hadoop</strong></p>
  <p>HCC Tutorial Tag:<strong> tutorial-220</strong> and <strong>hdp-2.5.0</strong></p>
  <p>If the tutorial has multiple labs please indicate which lab your question corresponds to. Please provide any feedback related to that lab.</p>
  <p>All Hortonworks, partner and community tutorials are posted in the Hortonworks github and can be contributed via the <a href="https://github.com/hortonworks/tutorials/wiki">Hortonworks Tutorial Contribution Guide</a>.  If you are certain there is an issue or bug with the tutorial, please <a href="https://github.com/hortonworks/tutorials/wiki#issues-with-tutorials">create an issue</a> on the repository and we will do our best to resolve it!</p>
</div>
