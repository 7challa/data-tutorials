

<div class="tutorial-content">
  <h1 id="refine-and-visualize-server-log-data">Refine and Visualize Server Log Data</h1>

<p>Security breaches happen. And when they do, your server logs may be your best line of defense. Hadoop takes server-log analysis to the next level by speeding and improving security forensics and providing a low cost platform to show compliance.</p>

<p>In this demo, we demonstrate how an enterprise security breach analysis and response might be performed.</p>

<!--<iframe width="700" height="394" src="https://www.youtube.com/embed/BPC_mClNSXk?feature=oembed" frameborder="0" allowfullscreen="" id="player0"></iframe>-->

<h3 id="in-this-tutorial-learn-how-to">In this tutorial, learn how to:</h3>

<ul>
  <li>Stream server logs into Hadoop with <a href="http://hortonworks.com/hdf/">Hortonworks Dataflow</a> powered by <strong>Apache NiFi</strong></li>
  <li>Use <a href="http://hortonworks.com/hadoop/hive">Hive</a> to build a relational view of the data</li>
  <li>Use <a href="http://hortonworks.com/hadoop/pig">Pig</a> to query and refine the data</li>
  <li>Use Elastic Search for high-level visualization</li>
  <li>Import the data into Microsoft Excel with the <a href="http://hortonworks.com/products/hdp-2/#add_ons">ODBC connector</a></li>
  <li>Visualize data with Powerview</li>
  <li>Use <a href="http://hortonworks.com/hadoop/oozie">Oozie</a> to automatically update a firewall</li>
  <li>Visualize the data with <a href="http://hortonworks.com/hadoop/zeppelin">Apache Zeppelin</a></li>
</ul>

<p>This Hadoop tutorial can be performed with the <a href="http://hortonworks.com/products/sandbox">Hortonworks Sandbox</a> – a single-node Hadoop cluster running in a virtual machine. Download to run this and other tutorials in the series. The tutorial presented here is for Sandbox v2.0</p>

<h2 id="background">Background</h2>

<h3 id="server-log-data">Server Log Data</h3>

<p>Server logs are computer-generated log files that capture network and server operations data. They are useful for managing network operations, especially for security and regulatory compliance.</p>

<h3 id="potential-uses-of-server-log-data">Potential Uses of Server Log Data</h3>

<p>IT organizations use server log analysis to answer questions about:</p>

<ul>
  <li><strong>Security</strong> – For example, if we suspect a security breach, how can we use server log data to identify and repair the vulnerability?</li>
  <li><strong>Compliance</strong> – Large organizations are bound by regulations such as HIPAA and Sarbanes-Oxley. How can IT administrators prepare for system audits?</li>
</ul>

<p>In this tutorial, we will focus on a network security use case. Specifically, we will look at how Apache Hadoop can help the administrator of a large enterprise network diagnose and respond to a distributed denial-of-service attack.</p>

<h3 id="what-is-hortonworks-dataflow-and-apache-nifi">What Is Hortonworks Dataflow and Apache NiFi?</h3>

<p><strong>Apache NiFi</strong> is a secure integrated platform for real time data collection, simple event processing, transport and delivery from source to storage. It is useful for moving distributed data to and from your Hadoop cluster. NiFi has lots of distributed processing capability to help reduce processing cost and get real-time insights from many different data sources across many large systems and can help aggregate that data into a single, or many different places.</p>

<p><strong>NiFi</strong> lets users get the most value from their data. Specifically NiFi allows users to:</p>

<ul>
  <li>Stream data from multiple source</li>
  <li>Collect high volumes of data in real time</li>
  <li>Guarantee delivery of data</li>
  <li>Scale horizontally across many machines</li>
</ul>

<p><strong>How NiFi Works</strong>. NiFi’s high-level architecture is focused on delivering a streamlined interface that is easy to use and easy to set up. There is a little bit of terminology that are an integral part to understanding how NiFi works.</p>

<ul>
  <li><strong>Processor</strong>: Processors in NiFi are what makes the data move. Processors can help generate data, run commands, move data, convert data, and many many more. NiFi’s architecture and feature set is designed to be extended these processors. They are at the very core of NiFi’s functionality.</li>
  <li><strong>Processing Group</strong>: When data flows get very complex, it can be very useful to group different parts together which perform certain functions. NiFi abstracts this concept and calls them processing groups.</li>
  <li><strong>FlowFile</strong>: A FlowFile in NiFi represents just a single piece of data. It is made of different parts. <strong>Attributes</strong> and <strong>Contents</strong>. Attributes help give the data context which are made of key-value pairs. Typically there are 3 attributes which are present on all FlowFiles: <strong>uuid</strong>, <strong>filename</strong>, and <strong>path</strong></li>
  <li><strong>Connections</strong> and <strong>Relationships</strong>: NiFi allows users to simply drag and drop connections between processors which controls how the data will flow. Each connection will be assigned to different types of relationships for the FlowFiles (such as successful processing, or a failure to process)</li>
</ul>

<p>A FlowFile can originate from a processor in NiFi. Processors can also receive the flowfiles and transmit them to many other processors. These processors can then drop the data in the flowfile into various places depending on the function of the processor.</p>

<h3 id="prerequisites">Prerequisites:</h3>

<ul>
  <li>Hortonworks Sandbox (installed and running)</li>
  <li>A copy of Hortonworks DataFlow - <a href="http://hortonworks.com">Download here</a></li>
  <li>Hortonworks ODBC driver installed and configured – See Tutorial “Installing and Configuring the Hortonworks ODBC Driver”</li>
  <li>Microsoft Excel 2013 Professional Plus (optional)
    <ul>
      <li>Note, Excel 2013 is not available on a Mac. However, you can still connect the Sandbox to your version of Excel via the ODBC driver, and you can explore the data through the standard charting capabilities of Excel.</li>
    </ul>
  </li>
  <li>If you’d like to use Tableau to explore the data, please see this HOWTO on the Hortonworks website: <a href="http://hortonworks.com/kb/how-to-connect-tableau-to-hortonworks-sandbox/">HOWTO: Connect Tableau to the Hortonworks Sandbox</a></li>
  <li>Server log tutorial files (included in this tutorial)</li>
</ul>

<p><strong>Notes:</strong></p>

<ul>
  <li>In this tutorial, the Hortonworks Sandbox is installed on an Oracle VirtualBox virtual machine (VM).</li>
  <li>Install the ODBC driver that matches the version of Excel you are using (32-bit or 64-bit).</li>
  <li>In this tutorial, we will use the Power View feature in Excel 2013 to visualize the server log data. Power View is currently only available in Microsoft Office Professional Plus and Microsoft Office 365 Professional Plus.</li>
  <li>We’re going to install Hortonworks DataFlow (HDF) on the Sandbox, so you’ll need to download the latest HDF release</li>
</ul>

<h3 id="overview">Overview</h3>

<p>To refine and visualize server log data, we will:</p>

<ul>
  <li>Download and configure the script which will generate our server log data</li>
  <li>Install, configure, and start Hortonworks DataFlow</li>
  <li>Generate the server log data.</li>
  <li>Import the server log data into Excel.</li>
  <li>Visualize the server log data using Excel Power View and Apache Zeppelin.</li>
</ul>

<hr />

<h2 id="step-1-download-and-the-script-to-generate-log-data">Step 1: Download and the Script to Generate Log Data</h2>

<p>We’ll be using a python script to generate the server log data. SSH into the sandbox with the command</p>

<div class="highlighter-rouge"><pre class="highlight"><code>ssh root@localhost -p 2222
</code></pre>
</div>

<p><strong>Default Sandbox Login</strong></p>

<table>
  <thead>
    <tr>
      <th>username</th>
      <th>password</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>root</td>
      <td>hadoop</td>
    </tr>
  </tbody>
</table>

<p>Or you can choose to use the Sandbox’s built-in Web-based SSH terminal <strong>Shell-In-A-Box</strong> which can be accessed at <a href="http://sandbox.hortonworks.com:4200">http://sandbox.hortonworks.com:4200</a></p>

<p>Remember the username is <code class="highlighter-rouge">root</code> and the password is <code class="highlighter-rouge">hadoop</code>.</p>

<p>After you log in, the command prompt will appear with the prefix <code class="highlighter-rouge">[root@Sandbox \~]\#:</code></p>

<p>Then execute:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>wget https://raw.githubusercontent.com/hortonworks/tutorials/%2370-revamp-refine-analyze-server-log-data/assets/server-logs/scripts/generate_logs.py
</code></pre>
</div>

<p>An exmaple for the output of these commands is below</p>

<p><img src="../../../assets/server-logs/script-download.png" alt="Example Output" /></p>

<hr />

<h2 id="step-2--configure-and-install-hortonworks-dataflow">Step 2 – Configure and Install Hortonworks DataFlow</h2>

<p>First thing’s you’ll need to do is to make sure you’ve <a href="http://hortonworks.com/hdp/downloads/#hdf">downloaded the gzipped version of Hortonworks DataFlow</a></p>

<p>Once you’ve downloaded HDF let’s get it on the sandbox. If you’re on a Mac or Unix system with the scp command available on your terminal you can simply run</p>

<div class="highlighter-rouge"><pre class="highlight"><code>scp -P 2222 $HDF_DOWNLOAD root@localhost:/root/
</code></pre>
</div>

<p>If you’re on a windows system you can use the program <a href="https://winscp.net/eng/index.php">WinSCP</a> to transfer files to the Sandbox.</p>

<p>After sending the HDF file to the Sandbox make sure you SSH into the Sandbox <strong>using the instructions from step 1</strong>.</p>

<p><img src="../../../assets/server-logs/scp-hdf-sandbox.png" alt="" /></p>

<p>Now that we have SSH’d into the sandbox we can run the following set of commands to set up and install HDF.</p>

<p>You can copy and paste these commands below, just make sure to <strong>first set the correct <code class="highlighter-rouge">HDF_FILE</code> and <code class="highlighter-rouge">HDF_VERSION</code> environment variables</strong> for the version of HDF that you downloaded.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>export HDF_FILE=HDF-1.2.0.0-91.tar.gz
export HDF_VERSION=HDF-1.2.0.0
cd /root
mkdir hdf
mv $HDF_FILE ./hdf
cd hdf
tar -xvf $HDF_FILE
cd $HDF_VERSION/nifi
sed -i s/nifi.web.http.port=8080/nifi.web.http.port=6434/g conf/nifi.properties
cd bin/
sh nifi.sh install
cd ~
</code></pre>
</div>

<p>Great! HDF is now set up for our needs. You can now start NiFi with the following command:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>service nifi start
</code></pre>
</div>

<p>First, we’ll need to open up the NiFi interface in our web browser. During installation we set the port that NiFi listens on to <code class="highlighter-rouge">6434</code>. You’ll need to forward this port in the virtual machine settings.</p>

<p>For a guide on forwarding a port on your VM please <a href="http://hortonworks.com/hadoop-tutorial/how-to-refine-and-visualize-sentiment-data/">see the guide in this tutorial</a></p>

<p>After forwarding port <code class="highlighter-rouge">6434</code> for NiFi you should be able to access the interface at <a href="https://localhost:6434/nifi">https://localhost:6434/nifi</a></p>

<p>It should look something like below:</p>

<p><img src="../../../assets/server-logs/nifi-interface.png" alt="Nifi Interface" /></p>

<h2 id="step-3-import-the-flow">Step 3: Import the Flow</h2>

<p>We’re going to import a pre-made data flow from a template which you can <a href="/assets/server-logs/ServerLogGenerator.xml"><strong>download here</strong></a>.</p>

<p>Use the NiFi inteface to upload the flow, and then drag it onto your workspace.</p>

<p><img src="../../../assets/server-logs/upload-template-1.png" alt="Upload NiFi Template" /></p>

<p><img src="../../../assets/server-logs/upload-template-2.png" alt="Upload NiFi Template" /></p>

<p>Once you’ve uploaded the template into NiFi you can instantiate it by dragging the template icon onto the screen. It will ask you to select your template’s name and the flow will appear as in the image below.</p>

<p><img src="../../../assets/server-logs/instantiate-template.png" alt="Instantiate NiFi Template" /></p>

<hr />

<h2 id="step-4-generate-the-server-log-data">Step 4: Generate the Server Log Data</h2>

<p>Now that you’ve imported the data flow and everything it set up, simply click the <strong>Run</strong> at the top of the screen. (Make you you haven’t selected a specific processor, or  else only one of the processors will start)</p>

<p><img src="../../../assets/server-logs/start-flow.png" alt="Instantiate NiFi Template" /></p>

<p>Now that everything is running we can check in the places where we see the data being deposited in HDFS.</p>

<p>Log into the Ambari interface which can be found at <a href="http://localhost:8080">http://localhost:8080</a></p>

<p>Open up the <strong>HDFS Files</strong> view, and then navigate to <code class="highlighter-rouge">/tmp/server-logs/</code>. Files should start appearing a few seconds after you start the flow. You can click on them to view the content.</p>

<p><img src="../../../assets/server-logs/explore-output-files.png" alt="Explore Output" /></p>

<ul>
  <li>Next we will create an Hive table from the log file.</li>
</ul>

<p>Open the Ambari UI and head to the views dropdown list. Select <strong>Hive</strong> and then past the following query.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>CREATE TABLE FIREWALL_LOGS(time STRING, ip STRING, country STRING, success BOOLEAN)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '|' 
LOCATION '/tmp/server-logs';
</code></pre>
</div>

<p><strong>Note</strong> if the query doesn’t run successfully due to a permissions error you then you might need to update the permission on the directory. Run the following commands over SSH on the Sandbox</p>

<div class="highlighter-rouge"><pre class="highlight"><code>sudo -u hdfs hadoop fs -chmod -R 777 /tmp
sudo -u hdfs hadoop fs -chown -R admin /tmp
</code></pre>
</div>

<p>When the table has been created you should now be able to query the data table for data using a query like</p>

<div class="highlighter-rouge"><pre class="highlight"><code>Select * from FIREWALL_LOGS LIMIT 100;
</code></pre>
</div>

<p><img src="../../../assets/server-logs/test-query-results.png" alt="" /></p>

<hr />

<h2 id="step-5-import-the-server-log-data-into-excel">Step 5: Import the Server Log Data into Excel</h2>

<p>In this section, we will use Excel Professional Plus 2013 to access the generated server log data. Note: if you do not have Excel 2013, you can still bring the data into other versions of Excel and explore the data through other charts. The screens may be slightly different in your version, but the actions are the same. You can complete Step 5 and then explore the data on your own.</p>

<ul>
  <li>In Windows, open a new Excel workbook, then select <strong>Data &gt; From Other Sources &gt; From Microsoft Query</strong>.</li>
</ul>

<p><img src="../../../assets/server-logs/17_open_query.jpg" alt="" /></p>

<ul>
  <li>
    <p>On the Choose Data Source pop-up, select the Hortonworks ODBC data source you installed previously, then click <strong>OK</strong>.</p>

    <p>The Hortonworks ODBC driver enables you to access Hortonworks data with Excel and other Business Intelligence (BI) applications that support ODBC.</p>
  </li>
</ul>

<p><img src="../../../assets/server-logs/18_choose_data_source.jpg" alt="" /></p>

<ul>
  <li>After the connection to the Sandbox is established, the Query Wizard appears. Select the “firewall_logs” table in the Available tables and columns box, then click the right arrow button to add the entire “firewall_logs” table to the query. Click <strong>Next</strong> to continue.</li>
</ul>

<p><img src="../../../assets/server-logs/19_query_wizard1.jpg" alt="" /></p>

<ul>
  <li>On the Filter Data screen, click <strong>Next</strong> to continue without filtering the data.</li>
</ul>

<p><img src="../../../assets/server-logs/20_query_wizard2.jpg" alt="" /></p>

<ul>
  <li>On the Sort Order screen, click <strong>Next</strong> to continue without setting a sort order.</li>
</ul>

<p><img src="../../../assets/server-logs/21_query_wizard3.jpg" alt="" /></p>

<ul>
  <li>Click <strong>Finish</strong> on the Query Wizard Finish screen to retrieve the query data from the Sandbox and import it into Excel.</li>
</ul>

<p><img src="../../../assets/server-logs/22_query_wizard4.jpg" alt="" /></p>

<ul>
  <li>On the Import Data dialog box, click <strong>OK</strong> to accept the default settings and import the data as a table.</li>
</ul>

<p><img src="../../../assets/server-logs/23_import_data.jpg" alt="" /></p>

<ul>
  <li>The imported query data should then appear in the Excel workbook.</li>
</ul>

<p>Now that we have successfully imported Hortonworks Sandbox data into Microsoft Excel, we can use the Excel Power View feature to analyze and visualize the data.</p>

<hr />

<h2 id="step-6-visualize-the-sentiment-data-using-excel-power-view">Step 6: Visualize the Sentiment Data Using Excel Power View</h2>

<p>Data visualization can help you analyze network data and determine effective responses to network issues. In this section, we will analyze data for a denial-of-service attack:</p>

<ul>
  <li>Review the network traffic by country</li>
  <li>Zoom in on one particular country</li>
  <li>Generate a list of attacking IP addresses</li>
</ul>

<p>We’ll start by reviewing the network traffic by country.</p>

<ul>
  <li>In the Excel worksheet with the imported “<firewall_logs>" table, select **Insert &gt; Power View** to open a new Power View report. (note: if this is your first time running PowerView it will prompt you to [install Silverlight](#Excel%20configuration%20for%20PowerView).</firewall_logs></li>
</ul>

<p><img src="../../../assets/server-logs/25_open_powerview_firewall_logs.jpg" alt="" /></p>

<ul>
  <li>
    <p>The Power View Fields area appears on the right side of the window, with the data table displayed on the left.</p>

    <p>Drag the handles or click the Pop Out icon to maximize the size of the data table, and close the Filters area.</p>
  </li>
</ul>

<p><img src="../../../assets/server-logs/26_powerview_firewall_logs.jpg" alt="" /></p>

<ul>
  <li>In the Power View Fields area, clear checkboxes next to the <strong>ip</strong> and <strong>time</strong> fields, then click <strong>Map</strong> on the Design tab in the top menu. (<strong>Note:</strong> If you do not get data plotted on your map look at <a href="#Geolocation%20of%20data%20using%20Bing">Geolocation of data using Bing</a>)</li>
</ul>

<p><img src="../../../assets/server-logs/27_open_map.jpg" alt="" /></p>

<ul>
  <li>Drag the <strong>status</strong> field into the <strong>SIZE</strong> box.</li>
</ul>

<p><img src="../../../assets/server-logs/28_status_to_size.jpg" alt="" /></p>

<ul>
  <li>The map view displays a global view of the network traffic by country. The color orange represents successful, authorized network connections. Blue represents connections from unauthorized sources.</li>
</ul>

<p><img src="../../../assets/server-logs/29_network_traffic_by_country.jpg" alt="" /></p>

<ul>
  <li>Let’s assume that recent denial-of-service attacks have originated in Pakistan. We can use the map controls to zoom in and take a closer look at traffic from that country.</li>
</ul>

<p><img src="../../../assets/server-logs/30_network_traffic_pakistan.jpg" alt="" /></p>

<div class="highlighter-rouge"><pre class="highlight"><code>It's obvious that this is a coordinated attack, originating from many countries. Now we can use Excel to generate a list of the unauthorized IP addresses.
</code></pre>
</div>

<ul>
  <li>
    <p>Use the tabs at the bottom of the Excel window to navigate back to the Excel worksheet with the imported “<firewall_logs>" table.</firewall_logs></p>

    <p>Click the arrow next to the <strong>status</strong> column header. Clear the <strong>Select all</strong> check box, select the <strong>ERROR</strong> check box, then click <strong>OK</strong>.</p>
  </li>
</ul>

<p><img src="../../../assets/server-logs/31_excel_error_list.jpg" alt="" /></p>

<ul>
  <li>Now that we have a list of the unauthorized IP addresses, we can update the network firewall to deny requests from those attacking IP addresses.</li>
</ul>

<p>We’ve shown how the Hortonworks Data Platform can help system administrators capture, store, and analyze server log data. With real-time access to massive amounts of data on the Hortonworks Data Platform, we were able to block unauthorized access, restore VPN access to authorized users.</p>

<p>With log data flowing continuously into the Hortonworks Data Platform “data lake,” we can protect the company network from similar attacks in the future. The data can be refreshed frequently and accessed to respond to security threats, or to prepare for compliance audits.</p>

<hr />

<h3 id="visualize-server-log-data-with-apache-zeppelin">Visualize Server Log Data with Apache Zeppelin</h3>

<p>First, make sure that the Apache Zeppelin service is started in Ambari. Then use the <strong>Views Dropdown Menu</strong> and select the Zeppelin View.</p>

<p>You should be greeted by the following screen where you can choose to view notes, or create a new one.</p>

<p><img src="../../../assets/server-logs/zeppelin_create_note.png" alt="" /></p>

<p>You can choose to import the note from this tutorial using the following URL:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>https://raw.githubusercontent.com/hortonworks/tutorials/hdp-2.4/data/zeppelin-notes/FlumeServerLogs.json
</code></pre>
</div>

<p>Once you’ve opened the note you can use the following commands to generate charts to visualize the data</p>

<div class="highlighter-rouge"><pre class="highlight"><code>%hive
select country from firewall_logs
</code></pre>
</div>

<p>and</p>

<div class="highlighter-rouge"><pre class="highlight"><code>%hive
select time, country from firewall_logs
</code></pre>
</div>

<p><img src="../../../assets/server-logs/sample_zeppelin_charts.png" alt="" /></p>

</div>

<div id="tutorial-footer">
  <hr>
  <h2>Tutorial Q&amp;A and Reporting Issues</h2>
  <p>If you need help or have questions with this tutorial, please first check HCC for existing Answers to questions on this tutorial using the Find Answers button.  If you don't find your answer you can post a new HCC question for this tutorial using the Ask Questions button.</p>
  <p><a class="btn" href="https://community.hortonworks.com/topics/tutorial-200.html" role="button">Find Answers</a> <a class="btn pull-right" href="https://community.hortonworks.com/questions/ask.html?space=81&topics=tutorial-200&topics=hdp-2.4.0" role="button">Ask Questions</a></p>
  <p>Tutorial Name: <strong>How to Refine and Visualize Server Log Data</strong></p>
  <p>HCC Tutorial Tag:<strong> tutorial-200</strong> and <strong>hdp-2.4.0</strong></p>
  <p>If the tutorial has multiple labs please indicate which lab your question corresponds to. Please provide any feedback related to that lab.</p>
  <p>All Hortonworks, partner and community tutorials are posted in the Hortonworks github and can be contributed via the <a href="https://github.com/hortonworks/tutorials/wiki">Hortonworks Tutorial Contribution Guide</a>.  If you are certain there is an issue or bug with the tutorial, please <a href="https://github.com/hortonworks/tutorials/wiki#issues-with-tutorials">create an issue</a> on the repository and we will do our best to resolve it!</p>
</div>
