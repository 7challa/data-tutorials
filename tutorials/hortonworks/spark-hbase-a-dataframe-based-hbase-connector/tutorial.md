---
layout: tutorial
title: Spark HBase - A Dataframe Based HBase Connector
tutorial-id: 369
tutorial-series: Spark
tutorial-version: hdp-2.5.0
intro-page: true
components: [ Spark, HBase ]
---

<span style="font-weight: 400;">We are proud to announce the technical preview of</span>[ <span style="font-weight: 400;">Spark-HBase Connector</span>](https://github.com/hortonworks/shc.git)<span style="font-weight: 400;">, developed by Hortonworks working with Bloomberg.</span> <span style="font-weight: 400;">The Spark-HBase connector leverages</span> [<span style="font-weight: 400;">Data Source API</span>](https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html) <span style="font-weight: 400;">(</span>[<span style="font-weight: 400;">SPARK-3247</span>](https://issues.apache.org/jira/browse/SPARK-3247)<span style="font-weight: 400;">) introduced in Spark-1.2.0\.  It bridges the gap between the simple HBase Key Value store and complex relational SQL queries and enables users to perform complex data analytics on top of HBase using Spark. An HBase DataFrame is a standard Spark DataFrame, and is able to interact with any other data sources such as Hive, ORC, Parquet, JSON, etc.</span>

## Background

<span style="font-weight: 400;">There are several open source Spark HBase connectors available either as Spark packages, as independent projects or in HBase trunk.</span> <span style="font-weight: 400;">Spark has moved to the Dataset/DataFrame APIs, which provides built-in query plan optimization. Now, end users prefer to use DataFrames/Datasets based interface.</span> <span style="font-weight: 400;">The HBase connector in the HBase trunk has a rich support at the RDD level, e.g. BulkPut, etc, but its DataFrame support is not as rich. HBase trunk connector relies on the standard HadoopRDD with HBase built-in TableInputFormat has some performance limitations. In addition, BulkGet performed in the the driver may be a single point of failure.</span> <span style="font-weight: 400;">There are some other alternative implementations. Take</span> [**Spark-SQL-on-HBase**](https://github.com/Huawei-Spark/Spark-SQL-on-HBase) <span style="font-weight: 400;">as an example. It applies very advanced custom optimization techniques by embedding its own query optimization plan inside the standard Spark Catalyst engine, ships the RDD to HBase and performs complicated tasks, such as partial aggregation, inside the HBase coprocessor. This approach is able to achieve high performance, but it difficult to maintain due to its complexity and the rapid evolution of Spark. Also allowing arbitrary code to run inside a coprocessor may pose security risks.</span> <span style="font-weight: 400;">The Spark-on-HBase Connector (SHC) has been developed to overcome these potential bottlenecks and weaknesses. It implements the standard Spark Datasource API, and leverages the Spark Catalyst engine for query optimization.  In parallel, the RDD is constructed from scratch instead of using</span> _<span style="font-weight: 400;">TableInputFormat</span>_ <span style="font-weight: 400;">in order to achieve high performance. With this customized RDD, all critical techniques can be applied and fully implemented, such as partition pruning, column pruning, predicate pushdown and data locality. The design makes the maintenance very easy, while achieving a good tradeoff between performance and simplicity.</span>

## Architecture

<span style="font-weight: 400;">We assume Spark and HBase are deployed in the same cluster, and Spark executors are co-located with region servers, as illustrated in the figure below.</span> ![age](http://hortonworks.com/wp-content/uploads/2016/06/age.png) <span style="font-weight: 400;">Figure 1\. Spark-on-HBase Connector Architecture</span> <span style="font-weight: 400;">At a high-level, the connector treats both Scan and Get in a similar way, and both actions are performed in the executors. The driver processes the query, aggregates scans/gets based on the region’s metadata, and generates tasks per region. The tasks are sent to the preferred executors co-­located with the region server, and are performed in parallel in the executors to achieve better data locality and concurrency. If a region does not hold the data required, that region server is not assigned any task. A task may consist of multiple Scans and BulkGets, and the data requests by a task is retrieved from only one region server, and this region server will also be the locality preference for the task. Note that the driver is not involved in the real job execution except for scheduling tasks. This avoids the driver being the bottleneck.</span>

## Table Catalog

<span style="font-weight: 400;">To bring the HBase table as a relational table into Spark, we define a mapping between HBase and Spark tables, called Table Catalog. There are two critical parts of this catalog. One is the rowkey definition and the other is the mapping between table column in Spark and the column family and column qualifier in HBase. Please refer to the Usage section for details.</span>

## Native Avro support

<span style="font-weight: 400;">The connector supports the Avro format natively, as it is a very common practice to persist structured data into HBase as a byte array. User can persist the Avro record into HBase directly. Internally, the Avro schema is converted to a native Spark Catalyst data type automatically. Note that both key-value parts in an HBase table can be defined in Avro format. Please refer to the examples/test cases in the repo for exact usage.</span>

## Predicate Pushdown

<span style="font-weight: 400;">The connector only retrieves required columns from region server to reduce network overhead and avoid redundant processing in Spark Catalyst engine. Existing standard HBase filters are used to perform predicate push-down without leveraging the coprocessor capability. Because HBase is not aware of the data type except for byte array, and the order inconsistency between Java primitive types and byte array,  we have to preprocess the filter condition before setting the filter in the Scan operation to avoid any data loss. Inside the region server, records not matching the query condition are filtered out.</span>

## Partition Pruning

<span style="font-weight: 400;">By extracting the row key from the predicates, we split the Scan/BulkGet into multiple non-overlapping ranges, only the region servers that has the requested data will perform Scan/BulkGet. Currently, the partition pruning is performed on the first dimension of the row keys. For example, if a row key is “key1:key2:key3”, the partition pruning will be based on “key1” only. Note that the WHERE conditions need to be defined carefully. Otherwise, the partition pruning may not take effect. For example, WHERE rowkey1 > "abc" OR column = "xyz" (where rowkey1 is the first dimension of the rowkey, and column is a regular hbase column) will result in a full scan, as we have to cover all the ranges because of the</span> **OR** <span style="font-weight: 400;">logic.</span>

## Data Locality

<span style="font-weight: 400;">When a Spark executor is co-located with HBase region servers, data locality is achieved by identifying the region server location, and makes best effort to co-locate the task with the region server. Each executor performs Scan/BulkGet on the part of the data co-located on the same host.</span>

## Scan and BulkGet

<span style="font-weight: 400;">These two operators are exposed to users by specifying WHERE CLAUSE, e.g.,</span> _<span style="font-weight: 400;">WHERE column > x and column < y</span>_ <span style="font-weight: 400;">for scan and</span> _<span style="font-weight: 400;">WHERE column = x</span>_ <span style="font-weight: 400;">for get. The operations are performed in the executors, and the driver only constructs these operations. Internally they are converted to scan and/or get, and Iterator[Row] is returned to catalyst engine for upper layer processing.</span>

## Usage

<span style="font-weight: 400;">The following illustrates the basic procedure on how to use the connector. For more details and advanced use case, such as Avro and composite key support, please refer to the</span> [<span style="font-weight: 400;">examples</span>](https://github.com/hortonworks/shc/tree/master/src/main/scala/org/apache/spark/sql/execution/datasources/hbase/examples) <span style="font-weight: 400;">in the repository.</span> **1) Define the catalog for the schema mapping:** def catalog = s"""{ <span style="font-weight: 400;">       |"table":{"namespace":"default", "name":"table1"},</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;">       |"rowkey":"key",</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;">       |"columns":{</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;">         |"col0":{"cf":"rowkey", "col":"key", "type":"string"},</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;">         |"col1":{"cf":"cf1", "col":"col1", "type":"boolean"},</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;">         |"col2":{"cf":"cf2", "col":"col2", "type":"double"},</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;">         |"col3":{"cf":"cf3", "col":"col3", "type":"float"},</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;">         |"col4":{"cf":"cf4", "col":"col4", "type":"int"},</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;">         |"col5":{"cf":"cf5", "col":"col5", "type":"bigint"},</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;">         |"col6":{"cf":"cf6", "col":"col6", "type":"smallint"},</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;">         |"col7":{"cf":"cf7", "col":"col7", "type":"string"},</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;">         |"col8":{"cf":"cf8", "col":"col8", "type":"tinyint"}</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;">       |}</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;">     |}""".stripMargin</span> **2) Prepare the data and populate the HBase table:** case class HBaseRecord(col0: String, col1: Boolean,col2: Double, col3: Float,col4: Int,       col5: Long, col6: Short, col7: String, col8: Byte) <span style="font-weight: 400;">object HBaseRecord {</span><span style="font-weight: 400;">def apply(i: Int, t: String): HBaseRecord = { val s = s"""row${"%03d".format(i)}"""       HBaseRecord(s, i % 2 == 0, i.toDouble, i.toFloat,  i, i.toLong, i.toShort,  s"String$i: $t",      i.toByte) }}</span> <span style="font-weight: 400;">val data = (0 to 255).map { i =>  HBaseRecord(i, "extra")}</span> <span style="font-weight: 400;">sc.parallelize(data).toDF.write.options(</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;"> Map(HBaseTableCatalog.tableCatalog -> catalog, HBaseTableCatalog.newTable -> "5"))</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;"> .format("org.apache.spark.sql.execution.datasources.hbase")</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;"> .save()</span> <span style="font-weight: 400;"> </span> **3) Load the DataFrame:** <span style="font-weight: 400;">def withCatalog(cat: String): DataFrame = {</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;"> sqlContext</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;"> .read</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;"> .options(Map(HBaseTableCatalog.tableCatalog->cat))</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;"> .format("org.apache.spark.sql.execution.datasources.hbase")</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;"> .load()</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;">}</span> <span style="font-weight: 400;">val df = withCatalog(catalog)</span> **4) Language integrated query:** <span style="font-weight: 400;">val s = df.filter((($"col0" <= "row050" && $"col0" > "row040") ||</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;"> $"col0" === "row005" ||</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;"> $"col0" === "row020" ||</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;"> $"col0" ===  "r20" ||</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;"> $"col0" <= "row005") &&</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;"> ($"col4" === 1 ||</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;"> $"col4" === 42))</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;"> .select("col0", "col1", "col4")</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;">s.show</span> **5) SQL query:** <span style="font-weight: 400;">df.registerTempTable("table")</span> <span style="font-weight: 400;"></span> <span style="font-weight: 400;">sqlContext.sql("select count(col1) from table").show</span>

## **Configuring Spark-Package**

<span style="font-weight: 400;">Users can use the Spark-on-HBase connector as a standard Spark package. To include the package in your Spark application use:</span> <span style="font-weight: 400;">spark-shell, pyspark, or spark-submit</span> <span style="font-weight: 400;">> $SPARK_HOME/bin/spark-shell --packages zhzhan:shc:0.0.11-1.6.1-s_2.10</span> <span style="font-weight: 400;">Users can include the package as the dependency in your SBT file as well. The format is the spark-package-name:version</span> <span style="font-weight: 400;">spDependencies += "</span><span style="font-weight: 400;">zhzhan</span><span style="font-weight: 400;">/</span><span style="font-weight: 400;">shc</span><span style="font-weight: 400;">:</span><span style="font-weight: 400;">0.0.11-1.6.1-s_2.10</span><span style="font-weight: 400;">"</span>

## **Running in Secure Cluster**

<span style="font-weight: 400;">For running in a Kerberos enabled cluster, the user has to include HBase related jars into the classpath as the HBase token retrieval and renewal is done by Spark, and is independent of the connector. In other words, the user needs to initiate the environment in the normal way, either through kinit or by providing principal/keytab.  The following examples show how to run in a secure cluster with both yarn-client and yarn-cluster mode. Note that SPARK_CLASSPATH has to be set for both modes, and the example jar is just a placeholder for Spark.</span> <span style="font-weight: 400;">export SPARK_CLASSPATH=/usr/hdp/current/hbase-client/lib/hbase-common.jar:/usr/hdp/current/hbase-client/lib/hbase-client.jar:/usr/hdp/current/hbase-client/lib/hbase-server.jar:/usr/hdp/current/hbase-client/lib/hbase-protocol.jar:/usr/hdp/current/hbase-client/lib/guava-12.0.1.jar</span> <span style="font-weight: 400;">Suppose hrt_qa is a headless acount, user can use following command for kinit</span><span style="font-weight: 400;">:</span> <span style="font-weight: 400;">kinit -k -t /tmp/hrt_qa.headless.keytab hrt_qa</span> <span style="font-weight: 400;">/usr/hdp/current/spark-client/bin/spark-submit --class org.apache.spark.sql.execution.datasources.hbase.examples.HBaseSource --master yarn-client --packages zhzhan:shc:0.0.11-1.6.1-s_2.10 --num-executors 4 --driver-memory 512m --executor-memory 512m --executor-cores 1 /usr/hdp/current/spark-client/lib/spark-examples-1.6.1.2.4.2.0-106-hadoop2.7.1.2.4.2.0-106.jar</span> <span style="font-weight: 400;">/usr/hdp/current/spark-client/bin/spark-submit --class org.apache.spark.sql.execution.datasources.hbase.examples.HBaseSource --master yarn-cluster --files /etc/hbase/conf/hbase-site.xml --packages zhzhan:shc:0.0.11-1.6.1-s_2.10 --num-executors 4 --driver-memory 512m --executor-memory 512m --executor-cores 1 /usr/hdp/current/spark-client/lib/spark-examples-1.6.1.2.4.2.0-106-hadoop2.7.1.2.4.2.0-106.jar</span>

## **Putting It All Together**

<span style="font-weight: 400;">We've just given a quick overview of how HBase supports Spark at the DataFrame level.  With the DataFrame API Spark applications can work with data stored in HBase table as easily as any data stored in other data sources. With this new feature, data in HBase tables can be easily consumed by Spark applications and other interactive tools, e.g. users can run a complex SQL query on top of an HBase table inside Spark, perform a table join against Dataframe, or integrate with Spark Streaming to implement a more complicated system.</span>

## **What’s Next?**

<span style="font-weight: 400;">Currently, the connector is hosted in</span> [<span style="font-weight: 400;">Hortonworks repo</span>](https://github.com/hortonworks/shc)<span style="font-weight: 400;">, and published as a</span> [<span style="font-weight: 400;">Spark package.</span>](http://spark-packages.org/package/zhzhan/shc) <span style="font-weight: 400;">It is in the process of being migrated to Apache HBase trunk. During the migration, we identified some critical bugs in the HBase trunk, and they will be fixed along with the merge.  </span><span style="font-weight: 400;">The community work is tracked by the umbrella HBase JIRA</span> [<span style="font-weight: 400;">HBASE-14789,</span> ](https://issues.apache.org/jira/browse/HBASE-14789)<span style="font-weight: 400;">including</span> [<span style="font-weight: 400;">HBASE-14795</span>](https://issues.apache.org/jira/browse/HBASE-14795) <span style="font-weight: 400;">and</span> [<span style="font-weight: 400;">HBASE-14796</span> ](https://issues.apache.org/jira/browse/HBASE-14796)<span style="font-weight: 400;"> to optimize the underlying computing architecture for Scan and BulkGet,  </span>[<span style="font-weight: 400;">HBASE-14801</span> ](https://issues.apache.org/jira/browse/HBASE-14801)<span style="font-weight: 400;">to provide JSON user interface for ease of use,</span> [<span style="font-weight: 400;">HBASE-15336</span>](https://issues.apache.org/jira/browse/HBASE-15336) <span style="font-weight: 400;">for the DataFrame writing path,</span> [<span style="font-weight: 400;">HBASE-15334</span>](https://issues.apache.org/jira/browse/HBASE-15334) <span style="font-weight: 400;">for Avro support,</span> [<span style="font-weight: 400;">HBASE-15333</span> ](https://issues.apache.org/jira/browse/HBASE-15333)<span style="font-weight: 400;"> to support Java primitive types, such as short, int, long, float, and double, etc.,</span> [<span style="font-weight: 400;">HBASE-15335</span>](https://issues.apache.org/jira/browse/HBASE-15335) <span style="font-weight: 400;">to support composite row key, and</span> [<span style="font-weight: 400;">HBASE-15572</span> ](https://issues.apache.org/jira/browse/HBASE-15572)<span style="font-weight: 400;">to add optional timestamp semantics</span><span style="font-weight: 400;">. We look forward to producing a future version of the connector which makes the connector even easier to work with.</span>

## Acknowledgement

<span style="font-weight: 400;">We want to thank Hamel Kothari, Sudarshan Kadambi and Bloomberg team in guiding us in this work and also helping us validate this work. We also want to thank HBase community for providing their feedback and making this better. Finally, this work has leveraged the lessons from earlier Spark HBase integrations and we want to thanks their developers for paving the road.</span>
