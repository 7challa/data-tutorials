{"paragraphs":[{"text":"%md\n\n## Exploring Spark SQL Module\n### with an Airline Dataset\n","user":"anonymous","dateUpdated":"2018-08-06T23:35:59+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604050_332023396","id":"20160410-003138_1880368561","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T23:35:59+0000","dateFinished":"2018-08-06T23:35:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:13504","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Exploring Spark SQL Module</h2>\n<h3>with an Airline Dataset</h3>\n"}]}},{"title":"Introduction","text":"%md\n\nIn this lab you will use Spark SQL via DataFrames API in Part 1 of the lab and SQL API in Part 2 of the lab to explore an Airline Dataset. This is a very interesting dataset that is further explored in other demo notebooks.","user":"anonymous","dateUpdated":"2018-08-06T23:36:01+0000","config":{"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":217,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604051_-343676383","id":"20160410-003138_985055475","dateCreated":"2018-08-06T22:13:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13505","dateFinished":"2018-08-06T23:36:01+0000","dateStarted":"2018-08-06T23:36:01+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>In this lab you will use Spark SQL via DataFrames API in Part 1 of the lab and SQL API in Part 2 of the lab to explore an Airline Dataset. This is a very interesting dataset that is further explored in other demo notebooks.</p>\n"}]}},{"text":"%md\n### Datasets and DataFrames\n\nThis tutorial relies on your understanding of Datasets and DataFrames, for a breif explanation on what they are navigate to the accompanying [**Hortonworks Tutorial**](https://hortonworks.com/tutorial/learning-spark-sql-with-zeppelin/)","user":"anonymous","dateUpdated":"2018-08-06T23:36:04+0000","config":{"colWidth":12,"fontSize":17,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533594329358_-1371233709","id":"20180806-222529_1721680673","dateCreated":"2018-08-06T22:25:29+0000","dateStarted":"2018-08-06T23:36:04+0000","dateFinished":"2018-08-06T23:36:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13506","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Datasets and DataFrames</h3>\n<p>This tutorial relies on your understanding of Datasets and DataFrames, for a breif explanation on what they are navigate to the accompanying <a href=\"https://hortonworks.com/tutorial/learning-spark-sql-with-zeppelin/\"><strong>Hortonworks Tutorial</strong></a></p>\n"}]}},{"title":"Verify Spark Version (should be 2.x)","text":"%spark2\n\nspark.version","user":"anonymous","dateUpdated":"2018-08-06T22:29:51+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"scala","completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604052_-1853641527","id":"20160410-003138_631425785","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T22:29:51+0000","dateFinished":"2018-08-06T22:29:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13507"},{"title":"Download CSV flight data file ","text":"%sh\n\n# You will now download a subset of 2008 flights (only 100k lines)\n# The full dataset may be found here: http://stat-computing.org/dataexpo/2009/the-data.html\n\nwget https://raw.githubusercontent.com/hortonworks/data-tutorials/master/tutorials/hdp/learning-spark-sql-with-zeppelin/assets/flights.csv -O /tmp/flights.csv\necho \"Downloaded!\"","user":"anonymous","dateUpdated":"2018-08-06T22:36:28+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"sh","completionSupport":false},"editorMode":"ace/mode/sh","colWidth":12,"editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604052_741546390","id":"20160410-003138_1540125404","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T22:36:28+0000","dateFinished":"2018-08-06T22:36:41+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13508"},{"title":"Preview Downloaded File","text":"%sh\n\ncat /tmp/flights.csv | head","user":"anonymous","dateUpdated":"2018-08-06T22:29:34+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"sh","completionSupport":false},"editorMode":"ace/mode/sh","colWidth":12,"editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604053_-1551307865","id":"20160410-003138_226044813","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T22:29:34+0000","dateFinished":"2018-08-06T22:29:34+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13509"},{"title":"Move dataset to HDFS (if supported/available)","text":"%sh\n\n# remove existing copies of dataset from HDFS\nhdfs dfs -rm -r -f /tmp/flights.csv\n\n# put data into HDFS\nhdfs dfs -put /tmp/flights.csv /tmp/","user":"anonymous","dateUpdated":"2018-08-06T22:37:24+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"sh","completionSupport":false},"editorMode":"ace/mode/sh","colWidth":12,"editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604053_1188230685","id":"20160410-003138_1267267737","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T22:37:24+0000","dateFinished":"2018-08-06T22:37:30+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13510"},{"title":"Create a DataFrame from CSV file","text":"%spark2\n\n// Create a flights DataFrame from CSV file\nval flights = (spark.read\n              .option(\"header\", \"true\")                              // Use first line as header\n              .option(\"inferSchema\", \"true\")                         // Infer schema\n              .csv(\"/tmp/flights.csv\"))                               // Read data","user":"anonymous","dateUpdated":"2018-08-06T22:37:42+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"scala","completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=186","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=187"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1533593604053_-1375093045","id":"20160410-003138_236600548","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T22:37:42+0000","dateFinished":"2018-08-06T22:37:45+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13511"},{"title":"Print Schema","text":"%spark2\n\n// Print the schema in a tree format\nflights.printSchema()","user":"anonymous","dateUpdated":"2018-08-06T22:37:51+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"scala","completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604053_464080151","id":"20160410-003138_1553179639","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T22:37:52+0000","dateFinished":"2018-08-06T22:37:52+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13512"},{"text":"%md\n### Part 1: Using DataFrame/Dataset API to Analyze the Airline Data\n\nNote: in this lab DataFrame and Dataset API calls will be indistinguishable. Internally, however, *flights* are represented as DataFrames and *delayedFlights* as Datasets in the examples below.","user":"anonymous","dateUpdated":"2018-08-06T23:36:13+0000","config":{"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604054_740851481","id":"20160410-003138_650819453","dateCreated":"2018-08-06T22:13:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13513","dateFinished":"2018-08-06T23:36:13+0000","dateStarted":"2018-08-06T23:36:13+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Part 1: Using DataFrame/Dataset API to Analyze the Airline Data</h3>\n<p>Note: in this lab DataFrame and Dataset API calls will be indistinguishable. Internally, however, <em>flights</em> are represented as DataFrames and <em>delayedFlights</em> as Datasets in the examples below.</p>\n"}]}},{"title":"Show a subset of columns","text":"%spark2\n\n// Show a subset of columns with \"select\"\nflights.select(\"UniqueCarrier\", \"FlightNum\", \"DepDelay\", \"ArrDelay\", \"Distance\").show()","user":"anonymous","dateUpdated":"2018-08-06T22:38:12+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"scala","completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=188"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1533593604054_-182296930","id":"20160410-003138_1188332400","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T22:38:12+0000","dateFinished":"2018-08-06T22:38:13+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13514"},{"title":"Apply a filter to find flights delayed more than 15 min","text":"%spark2\n\n// Create a Dataset containing flights with delayed departure by more than 15 min using \"filter\"\nval delayedFlights = (flights\n                        .select(\"UniqueCarrier\", \"DepDelay\")\n                        .filter($\"DepDelay\" > 15))\n                        \ndelayedFlights.show()","user":"anonymous","dateUpdated":"2018-08-06T22:38:32+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"scala","completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=189"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1533593604054_571330675","id":"20160410-003138_704729700","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T22:38:32+0000","dateFinished":"2018-08-06T22:38:33+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13515"},{"title":"Display percentage of delayed flights","text":"%spark2\n\nval numTotalFlights = flights.count()\nval numDelayedFlights = delayedFlights.count()\n\n// Print total number of delayed flights\nprintln(\"Percentage of Delayed Flights: \" + (numDelayedFlights.toFloat/numTotalFlights*100) + \"%\")","user":"anonymous","dateUpdated":"2018-08-06T22:38:40+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"scala","completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=190","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=191"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1533593604054_1734837271","id":"20160410-003138_1019754695","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T22:38:40+0000","dateFinished":"2018-08-06T22:38:42+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13516"},{"text":"%md\n\nWe can also create a user defined function (UDF) to determine delays.","user":"anonymous","dateUpdated":"2018-08-06T23:36:19+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604054_-1354555491","id":"20161017-203635_1855560775","dateCreated":"2018-08-06T22:13:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13517","dateFinished":"2018-08-06T23:36:20+0000","dateStarted":"2018-08-06T23:36:20+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We can also create a user defined function (UDF) to determine delays.</p>\n"}]}},{"title":" Create a UDF to determine delays","text":"%spark2\n\nimport org.apache.spark.sql.functions.udf\n\n// Define a UDF to find delayed flights\n\n// Assume:\n//  if ArrDelay is not available then Delayed = False\n//  if ArrDelay > 15 min then Delayed = True else False\n\nval isDelayedUDF = udf((time: String) => if (time == \"NA\") 0 else if (time.toInt > 15) 1 else 0)","user":"anonymous","dateUpdated":"2018-08-06T22:38:49+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"scala","completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604055_245591154","id":"20161017-203017_1781904338","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T22:38:50+0000","dateFinished":"2018-08-06T22:38:51+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13518"},{"title":"Create a new DataFrame with IsDelayed column","text":"%spark2\n\nval flightsWithDelays = flights.select($\"Year\", $\"Month\", $\"DayofMonth\", $\"UniqueCarrier\", $\"FlightNum\", $\"DepDelay\", \n                    isDelayedUDF($\"DepDelay\").alias(\"IsDelayed\"))\n                    \nflightsWithDelays.show(5)","user":"anonymous","dateUpdated":"2018-08-06T22:39:18+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"scala","completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=192"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1533593604055_-24739368","id":"20161017-203358_1309594443","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T22:39:19+0000","dateFinished":"2018-08-06T22:39:20+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13519"},{"text":"%md\n\n\nNote that now we have a new table with a column that indicates whether a flight is delayed or not. This will allow us to calculate percentage of delayed flights in one pass.","user":"anonymous","dateUpdated":"2018-08-06T23:36:25+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604055_1485041683","id":"20161017-205652_1397194952","dateCreated":"2018-08-06T22:13:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13520","dateFinished":"2018-08-06T23:36:25+0000","dateStarted":"2018-08-06T23:36:25+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Note that now we have a new table with a column that indicates whether a flight is delayed or not. This will allow us to calculate percentage of delayed flights in one pass.</p>\n"}]}},{"title":"Calculate percentage of delayed flights using flightsWithDelays DataFrame","text":"%spark2\n\nflightsWithDelays.agg((sum(\"IsDelayed\") * 100 / count(\"DepDelay\")).alias(\"Percentage of Delayed Flights\")).show()","user":"anonymous","dateUpdated":"2018-08-06T22:39:32+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"scala","completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=193"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1533593604055_-69369256","id":"20161017-205750_819957102","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T22:39:33+0000","dateFinished":"2018-08-06T22:39:34+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13521"},{"text":"%md\n\nAs you can see above, this is a very clean way of displaying a percentage of delayed flights. UDFs are useful in creating additional functions that are commonly used.\n\nNow let's explore our flights a bit more and find some averages.","user":"anonymous","dateUpdated":"2018-08-06T23:36:28+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604055_-1633040188","id":"20161017-205919_1405069576","dateCreated":"2018-08-06T22:13:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13522","dateFinished":"2018-08-06T23:36:29+0000","dateStarted":"2018-08-06T23:36:29+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>As you can see above, this is a very clean way of displaying a percentage of delayed flights. UDFs are useful in creating additional functions that are commonly used.</p>\n<p>Now let's explore our flights a bit more and find some averages.</p>\n"}]}},{"title":"Find Avg Taxi-in","text":"%spark2\n\n(flights.select(\"Origin\", \"Dest\", \"TaxiIn\")\n        .groupBy(\"Origin\", \"Dest\")\n        .agg(avg(\"TaxiIn\")\n        .alias(\"AvgTaxiIn\"))\n        .orderBy(desc(\"AvgTaxiIn\"))\n        .show(10))","user":"anonymous","dateUpdated":"2018-08-06T22:39:57+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"scala","completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","colWidth":6,"editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=194"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1533593604056_1020078054","id":"20160410-003138_1488719873","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T22:39:57+0000","dateFinished":"2018-08-06T22:40:06+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13523"},{"title":"Find Avg Taxi-out","text":"%spark2\n\n(flights.select(\"Origin\", \"Dest\", \"TaxiOut\")\n        .groupBy(\"Origin\", \"Dest\")\n        .agg(avg(\"TaxiOut\")\n        .alias(\"AvgTaxiOut\"))\n        .orderBy(desc(\"AvgTaxiOut\"))\n        .show(10))","user":"anonymous","dateUpdated":"2018-08-06T22:40:21+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"scala","completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","colWidth":6,"editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=195"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1533593604056_163842904","id":"20160410-003138_840324935","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T22:40:21+0000","dateFinished":"2018-08-06T22:40:27+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13524"},{"text":"%md\n### Part 2: Using SQL API to Analyze the Airline Data","user":"anonymous","dateUpdated":"2018-08-06T23:36:33+0000","config":{"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604056_-1495632378","id":"20160410-003138_582934314","dateCreated":"2018-08-06T22:13:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13525","dateFinished":"2018-08-06T23:36:34+0000","dateStarted":"2018-08-06T23:36:34+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Part 2: Using SQL API to Analyze the Airline Data</h3>\n"}]}},{"title":"Is there a more interactive way to display query results?","text":"%md\n\nAs you can see, the data displayed in Part 1 of this notebook isn't too interactive. To have a more dynamic experience, let's create a temporary (in-memory) view that we can query against and interact with the resulting data in a table or graph format. The temporary view will allow us to execute SQL queries against it.\n\nNote that the temporary view will reside in memory as long as the Spark session is alive.","user":"anonymous","dateUpdated":"2018-08-06T23:36:36+0000","config":{"editorSetting":{},"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604056_-1147487313","id":"20160410-003138_556617784","dateCreated":"2018-08-06T22:13:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13526","dateFinished":"2018-08-06T23:36:36+0000","dateStarted":"2018-08-06T23:36:36+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>As you can see, the data displayed in Part 1 of this notebook isn't too interactive. To have a more dynamic experience, let's create a temporary (in-memory) view that we can query against and interact with the resulting data in a table or graph format. The temporary view will allow us to execute SQL queries against it.</p>\n<p>Note that the temporary view will reside in memory as long as the Spark session is alive.</p>\n"}]}},{"title":"Register a Temporary View","text":"%spark2\n\n// Convert flights DataFrame to a temporary view\nflights.createOrReplaceTempView(\"flightsView\")","user":"anonymous","dateUpdated":"2018-08-06T22:55:12+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"scala","completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"title":true,"results":[],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604057_298067789","id":"20160410-003138_636329356","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T22:55:12+0000","dateFinished":"2018-08-06T22:55:13+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13527"},{"title":"Preview Data in an interactive table format","text":"%spark2.sql\n\nSELECT * FROM flightsView LIMIT 20","user":"anonymous","dateUpdated":"2018-08-06T22:55:26+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"sql","completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/sql","colWidth":12,"editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"Year","index":0,"aggr":"sum"}],"values":[{"name":"Month","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"Year","index":0,"aggr":"sum"},"yAxis":{"name":"Month","index":1,"aggr":"sum"}},"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"Year":"string","Month":"string","DayofMonth":"string","DayOfWeek":"string","DepTime":"string","CRSDepTime":"string","ArrTime":"string","CRSArrTime":"string","UniqueCarrier":"string","FlightNum":"string","TailNum":"string","ActualElapsedTime":"string","CRSElapsedTime":"string","AirTime":"string","ArrDelay":"string","DepDelay":"string","Origin":"string","Dest":"string","Distance":"string","TaxiIn":"string","TaxiOut":"string","Cancelled":"string","CancellationCode":"string","Diverted":"string","CarrierDelay":"string","WeatherDelay":"string","NASDelay":"string","SecurityDelay":"string","LateAircraftDelay":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=196"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1533593604057_-553216821","id":"20160410-003138_318924232","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T22:55:18+0000","dateFinished":"2018-08-06T22:55:22+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13528"},{"title":"Register a User Defined Function (UDF)","text":"%spark2\n\n// Register a helper UDF to find delayed flights\n// Note that this is a UDF specific for use with the sparkSession\n\n// Assume:\n//  if ArrDelay is not available then Delayed = False\n//  if ArrDelay > 15 min then Delayed = True else False\n\nspark.udf.register(\"isDelayedUDF\", (time: String) => if (time == \"NA\") 0 else if (time.toInt > 15) 1 else 0)","user":"anonymous","dateUpdated":"2018-08-06T22:55:27+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"scala","completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604057_2033983879","id":"20160410-003138_40384312","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T22:55:27+0000","dateFinished":"2018-08-06T22:55:28+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13529"},{"title":"Compare Total Number of Delayed Flights by Carrier","text":"%spark2.sql\n--- Compare Total Number of Delayed Flights by Carrier\nSELECT UniqueCarrier, SUM(isDelayedUDF(DepDelay)) AS NumDelays FROM flightsView GROUP BY UniqueCarrier","user":"anonymous","dateUpdated":"2018-08-06T22:55:41+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"sql","completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/sql","colWidth":6,"editorHide":false,"title":true,"results":[{"graph":{"mode":"pieChart","height":296,"optionOpen":false,"keys":[{"name":"UniqueCarrier","index":0,"aggr":"sum"}],"values":[{"name":"NumDelays","index":1,"aggr":"sum"}],"groups":[],"scatter":{"yAxis":{"name":"NumDelays","index":1,"aggr":"sum"}}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=197","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=198","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=199","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=200","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=201"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1533593604057_-1864644669","id":"20160410-003138_134299332","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T22:55:32+0000","dateFinished":"2018-08-06T22:55:35+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13530"},{"title":"Compare Total Delayed Time (min) by Carrier","text":"%spark2.sql\n--- Compare Total Delayed Time (min) by Carrier\nSELECT UniqueCarrier, SUM(DepDelay) AS TotalTimeDelay FROM flightsView GROUP BY UniqueCarrier","user":"anonymous","dateUpdated":"2018-08-06T22:55:38+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"sql","completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/sql","colWidth":6,"editorHide":false,"title":true,"results":[{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"keys":[{"name":"UniqueCarrier","index":0,"aggr":"sum"}],"values":[{"name":"TotalTimeDelay","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"UniqueCarrier","index":0,"aggr":"sum"},"yAxis":{"name":"TotalTimeDelay","index":1,"aggr":"sum"}},"setting":{"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=202","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=203","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=204","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=205","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=206"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1533593604057_1355383408","id":"20160410-003138_163559927","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T22:55:36+0000","dateFinished":"2018-08-06T22:55:37+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13531"},{"title":"Find Average Distance Travelled by Carrier","text":"%spark2.sql\n--- Find Average Distance Travelled by Carrier\nSELECT UniqueCarrier, avg(Distance) AS AvgDistanceTraveled FROM flightsView GROUP BY UniqueCarrier ORDER BY AvgDistanceTraveled DESC","user":"anonymous","dateUpdated":"2018-08-06T22:57:39+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"sql","completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/sql","colWidth":12,"editorHide":false,"title":true,"results":[{"graph":{"mode":"pieChart","height":300,"optionOpen":false,"keys":[{"name":"UniqueCarrier","index":0,"aggr":"sum"}],"values":[{"name":"AvgDistanceTraveled","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"UniqueCarrier","index":0,"aggr":"sum"},"yAxis":{"name":"AvgDistanceTraveled","index":1,"aggr":"sum"}}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=207"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1533593604058_1452053485","id":"20160410-003138_172624929","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T22:57:39+0000","dateFinished":"2018-08-06T22:57:42+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13532"},{"title":"Find Out When Most Flights Get Delayed by Day of Week","text":"%spark2.sql\n\nSELECT DayOfWeek, CASE WHEN isDelayedUDF(DepDelay) = 1 THEN 'delayed' ELSE 'ok' END AS Delay, COUNT(1) AS Count\nFROM flightsView\nGROUP BY DayOfWeek, CASE WHEN isDelayedUDF(DepDelay) = 1 THEN 'delayed' ELSE 'ok' END\nORDER BY DayOfWeek","user":"anonymous","dateUpdated":"2018-08-06T22:57:57+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"sql","completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/sql","colWidth":12,"editorHide":false,"title":true,"results":[{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"keys":[{"name":"DayOfWeek","index":0,"aggr":"sum"}],"values":[{"name":"Count","index":2,"aggr":"sum"}],"groups":[{"name":"Delay","index":1,"aggr":"sum"}],"scatter":{"xAxis":{"name":"DayOfWeek","index":0,"aggr":"sum"},"yAxis":{"name":"Delay","index":1,"aggr":"sum"}},"setting":{"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=208"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1533593604058_-1009106709","id":"20160410-003138_56774606","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T22:57:50+0000","dateFinished":"2018-08-06T22:57:52+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13533"},{"title":"Find Out When Most Flights Get Delayed by Hour","text":"%spark2.sql\n\nSELECT CAST(CRSDepTime / 100 AS INT) AS Hour, CASE WHEN isDelayedUDF(DepDelay) = 1 THEN 'delayed' ELSE 'ok' END AS Delay, COUNT(1) AS Count\nFROM flightsView\nGROUP BY CAST(CRSDepTime / 100 AS INT), CASE WHEN isDelayedUDF(DepDelay) = 1 THEN 'delayed' ELSE 'ok' END\nORDER BY Hour","user":"anonymous","dateUpdated":"2018-08-06T22:58:03+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"sql","completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/sql","colWidth":12,"editorHide":false,"title":true,"results":[{"graph":{"mode":"stackedAreaChart","height":300,"optionOpen":false,"keys":[{"name":"Hour","index":0,"aggr":"sum"}],"values":[{"name":"Count","index":2,"aggr":"sum"}],"groups":[{"name":"Delay","index":1,"aggr":"sum"}],"scatter":{"xAxis":{"name":"Hour","index":0,"aggr":"sum"},"yAxis":{"name":"Delay","index":1,"aggr":"sum"}},"setting":{"stackedAreaChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=209"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1533593604058_-1514887139","id":"20160410-003138_728063774","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T22:58:00+0000","dateFinished":"2018-08-06T22:58:01+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13534"},{"text":"%md\n\n## Persisting Results / Data\n\nFinally, let's persist some of our results by saving our DataFrames in an optimized file format called ORC.\n","user":"anonymous","dateUpdated":"2018-08-06T23:36:46+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604058_1476446616","id":"20161017-212723_1255606607","dateCreated":"2018-08-06T22:13:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13535","dateFinished":"2018-08-06T23:36:47+0000","dateStarted":"2018-08-06T23:36:47+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Persisting Results / Data</h2>\n<p>Finally, let's persist some of our results by saving our DataFrames in an optimized file format called ORC.</p>\n"}]}},{"title":"Save to ORC file","text":"%spark2\n\nimport org.apache.spark.sql.SaveMode\n\n// Save and Overwrite our new DataFrame to an ORC file\nflightsWithDelays.write.format(\"orc\").mode(SaveMode.Overwrite).save(\"flightsWithDelays.orc\")","user":"anonymous","dateUpdated":"2018-08-06T23:28:30+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"scala","completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=210"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1533593604059_649758067","id":"20160410-003138_985965720","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T23:28:31+0000","dateFinished":"2018-08-06T23:28:35+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13536"},{"title":"Load back from an ORC file","text":"%spark2\n\n// Load results back from ORC file\nval test = spark.read.format(\"orc\").load(\"flightsWithDelays.orc\")\n\n// Assert both DataFrames of the same size.\n//   Note that if assertion succeeds no warning messages will be printed\nassert (test.count == flightsWithDelays.count, println(\"Assertion Fail: Files are of different sizes.\"))\n\ntest.show(10)","user":"anonymous","dateUpdated":"2018-08-06T23:29:09+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"scala","completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=211","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=212","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=213"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1533593604060_-1072331002","id":"20160410-003138_1142035788","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T23:29:09+0000","dateFinished":"2018-08-06T23:29:12+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13537"},{"text":"%md\n\nWe can also create permanent tables, instead of temporary views, using `saveAsTable`. The resulting table will still exist even after your Spark program has restarted.","user":"anonymous","dateUpdated":"2018-08-06T23:36:51+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604060_-1557452661","id":"20161017-212315_1033823107","dateCreated":"2018-08-06T22:13:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13538","dateFinished":"2018-08-06T23:36:51+0000","dateStarted":"2018-08-06T23:36:51+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We can also create permanent tables, instead of temporary views, using <code>saveAsTable</code>. The resulting table will still exist even after your Spark program has restarted.</p>\n"}]}},{"title":"Save DataFrame as Permanent Table","text":"%spark2\n\nflightsWithDelays.write.format(\"orc\").mode(SaveMode.Overwrite).saveAsTable(\"flightswithdelaystbl\")","user":"anonymous","dateUpdated":"2018-08-06T22:13:24+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"scala","completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":[],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604061_1491920611","id":"20161017-212148_1432557096","dateCreated":"2018-08-06T22:13:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13539"},{"title":"Show Tables/Views","text":"%spark2.sql\n\nSHOW TABLES\n\n-- Note that flightsWithDelaysTbl is a permanent table instead of a temporary view!","user":"anonymous","dateUpdated":"2018-08-06T22:13:24+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"sql","completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"tableName","index":0,"aggr":"sum"}],"values":[{"name":"isTemporary","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"tableName","index":0,"aggr":"sum"},"yAxis":{"name":"isTemporary","index":1,"aggr":"sum"}}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604062_-1103646741","id":"20161017-212228_2044087527","dateCreated":"2018-08-06T22:13:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13540"},{"title":"Querying a Permanent Table","text":"%spark2.sql\n\nSELECT COUNT(1) AS Total from flightswithdelaystbl  -- As you can see, there's no difference in querying a temporary view vs a permanent table","user":"anonymous","dateUpdated":"2018-08-06T22:13:24+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"sql","completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/sql","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"Total","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"Total","index":0,"aggr":"sum"}}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604062_676979579","id":"20161017-212847_790820933","dateCreated":"2018-08-06T22:13:24+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:13541"},{"title":"Final Words","text":"%md\n\nThis should get you started working in Scala with DataFrame, Dataset and SQL Spark APIs that are part of the Spark SQL Module. You should now have the basic tools and code samples to start working on your own data sets: from brining in/downloading datasets, to moving them from local storage to HDFS, to transforming datasets into Spark DataFrames/Datasets/temporary views, querying the data, performing basic calcuations, visualizing, and finally persisiting your results. That's a great start!","user":"anonymous","dateUpdated":"2018-08-06T23:36:56+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604062_1934818499","id":"20161017-214817_1787337666","dateCreated":"2018-08-06T22:13:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13542","dateFinished":"2018-08-06T23:36:57+0000","dateStarted":"2018-08-06T23:36:57+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>This should get you started working in Scala with DataFrame, Dataset and SQL Spark APIs that are part of the Spark SQL Module. You should now have the basic tools and code samples to start working on your own data sets: from brining in/downloading datasets, to moving them from local storage to HDFS, to transforming datasets into Spark DataFrames/Datasets/temporary views, querying the data, performing basic calcuations, visualizing, and finally persisiting your results. That's a great start!</p>\n"}]}},{"title":"Additional Resources","text":"%md\n\nWe hope you've enjoyed this introductory lab. Below are additional resources that you should find useful:\n\n1. [Hortonworks Apache Spark Tutorials](http://hortonworks.com/tutorials/#tuts-developers) are your natural next step where you can explore Spark in more depth.\n2. [Hortonworks Community Connection (HCC)](https://community.hortonworks.com/spaces/85/data-science.html?type=question) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Hortonworks Apache Spark Docs](https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.5/bk_spark-component-guide/content/ch_developing-spark-apps.html) - official Spark documentation.\n4. [Hortonworks Apache Zeppelin Docs](https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.5/bk_zeppelin-component-guide/content/ch_using_zeppelin.html) - official Zeppelin documentation.","user":"anonymous","dateUpdated":"2018-08-06T23:37:45+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown","completionSupport":false},"editorMode":"ace/mode/markdown","colWidth":10,"editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604062_-20111137","id":"20160410-003138_2048237853","dateCreated":"2018-08-06T22:13:24+0000","dateStarted":"2018-08-06T23:37:45+0000","dateFinished":"2018-08-06T23:37:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13543","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>We hope you've enjoyed this introductory lab. Below are additional resources that you should find useful:</p>\n<ol>\n<li><a href=\"http://hortonworks.com/tutorials/#tuts-developers\">Hortonworks Apache Spark Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n<li><a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\">Hortonworks Community Connection (HCC)</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.5/bk_spark-component-guide/content/ch_developing-spark-apps.html\">Hortonworks Apache Spark Docs</a> - official Spark documentation.</li>\n<li><a href=\"https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.5/bk_zeppelin-component-guide/content/ch_using_zeppelin.html\">Hortonworks Apache Zeppelin Docs</a> - official Zeppelin documentation.</li>\n</ol>\n"}]}},{"text":"%angular\n</br>\n<center>\n<a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\" target='_blank'>\n  <img src=\"http://hortonworks.com/wp-content/uploads/2016/03/logo-hcc.png\" alt=\"HCC\" style=\"width:125px;height:125px;border:0;\" align=\"middle\">\n</a>\n</center>","user":"anonymous","dateUpdated":"2018-08-06T23:37:02+0000","config":{"editorSetting":{},"editorMode":"ace/mode/scala","colWidth":2,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1533593604063_1765525829","id":"20160410-003138_1663715025","dateCreated":"2018-08-06T22:13:24+0000","status":"ABORT","errorMessage":"java.lang.RuntimeException: OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0\nOpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000c0000000, 716177408, 0) failed; error='Cannot allocate memory' (errno=12)\n#\n# There is insufficient memory for the Java Runtime Environment to continue.\n# Native memory allocation (mmap) failed to map 716177408 bytes for committing reserved memory.\n# An error report file with more information is saved as:\n# /home/zeppelin/hs_err_pid98370.log\n\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterManagedProcess.start(RemoteInterpreterManagedProcess.java:205)\n\tat org.apache.zeppelin.interpreter.ManagedInterpreterGroup.getOrCreateInterpreterProcess(ManagedInterpreterGroup.java:64)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getOrCreateInterpreterProcess(RemoteInterpreter.java:111)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.internal_create(RemoteInterpreter.java:164)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:132)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:299)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:407)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:188)\n\tat org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:307)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n","progressUpdateIntervalMs":500,"$$hashKey":"object:13544","dateFinished":"2018-08-06T23:38:02+0000","dateStarted":"2018-08-06T23:37:02+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.lang.RuntimeException: OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0\nOpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000c0000000, 716177408, 0) failed; error='Cannot allocate memory' (errno=12)\n#\n# There is insufficient memory for the Java Runtime Environment to continue.\n# Native memory allocation (mmap) failed to map 716177408 bytes for committing reserved memory.\n# An error report file with more information is saved as:\n# /home/zeppelin/hs_err_pid98370.log\n\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterManagedProcess.start(RemoteInterpreterManagedProcess.java:205)\n\tat org.apache.zeppelin.interpreter.ManagedInterpreterGroup.getOrCreateInterpreterProcess(ManagedInterpreterGroup.java:64)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getOrCreateInterpreterProcess(RemoteInterpreter.java:111)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.internal_create(RemoteInterpreter.java:164)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:132)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:299)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:407)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:188)\n\tat org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:307)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"}]}}],"name":"Learning Spark SQL","id":"2DMJQDYFC","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"angular:shared_process":[],"sh:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}