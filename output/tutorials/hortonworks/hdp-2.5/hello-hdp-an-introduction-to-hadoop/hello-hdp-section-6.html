<div class="tutorial-content">
  <h1 id="lab-4-spark---risk-factor">Lab 4: Spark - Risk Factor</h1>

<h2 id="using-apache-spark-to-compute-driver-risk-factor">Using Apache Spark to compute Driver Risk Factor</h2>

<p><strong>Note:</strong>  This lab is optional and produces the same result as in Lab 3. You may continue on to the next lab if you wish.</p>

<h3 id="introduction">Introduction</h3>

<p>In this tutorial we will introduce Apache Spark. In the earlier section of the lab you have learned how to load data into HDFS and then manipulate it using Hive. We are using the Truck sensor data to better understand risk associated with every driver. This section will teach you how to compute risk using Apache spark.</p>

<h2 id="pre-requisites">Pre-Requisites</h2>

<p>This tutorial is a part of a series of hands on tutorials to get you started on HDP using the Hortonworks sandbox. Please ensure you complete the prerequisites before proceeding with this tutorial.</p>

<ul>
  <li>Hortonworks Sandbox</li>
  <li><a href="http://hortonworks.com/hadoop-tutorial/learning-the-ropes-of-the-hortonworks-sandbox/">Learning the Ropes of the Hortonworks Sandbox</a></li>
  <li>Lab 1: Loading sensor data into HDFS</li>
  <li>Lab 2: Data Manipulation with Apache Hive</li>
  <li>Allow yourself around one hour to complete this tutorial.</li>
</ul>

<h2 id="outline">Outline</h2>

<ul>
  <li><a href="#apache-spark-backdrop">Apache Spark Backdrop</a></li>
  <li><a href="#apache-spark-basics">Apache Spark Basics</a></li>
  <li><a href="#step4.1">Step 4.1: Configure Apache Spark services using Ambari</a></li>
  <li><a href="#step4.2">Step 4.2: Create a Hive Context</a></li>
  <li><a href="#step4.3">Step 4.3: Create RDD from Hive Context</a></li>
  <li><a href="#step4.4">Step 4.4: RDD transformations and actions</a></li>
  <li><a href="#step4.5">Step 4.5: Load and save data into Hive as ORC</a></li>
  <li><a href="#run-spark-in-shell">Appendix A: Run Spark in the Spark Interactive Shell</a></li>
  <li><a href="#login-to-zeppelin">Appendix B: Login to Zeppelin (Optional)</a></li>
  <li><a href="#summary-lab4">Summary</a></li>
  <li><a href="#suggested-readings">Suggested Readings</a></li>
</ul>

<h2 id="background-">Background <a id="apache-spark-backdrop"></a></h2>

<p>MapReduce has been useful, but the amount of time it takes for the jobs to run can at times be exhaustive. Also, MapReduce jobs only work for a specific set of use cases. There is a need for computing framework that works for a wider set of use cases.</p>

<p>Apache Spark was designed to be a fast, general-purpose, easy-to-use computing platform. It extends the MapReduce model and takes it to a whole other level. The speed comes from the in-memory computations. Applications running in memory allow for much faster processing and response.</p>

<h2 id="apache-spark-">Apache Spark <a id="apache-spark-basics"></a></h2>

<p><a href="http://hortonworks.com/hadoop/spark/">Apache Spark</a> is a fast, in-memory data processing engine with elegant and expressive development <a href="https://spark.apache.org/docs/1.6.1/api/R/index.html">APIs</a> in <a href="https://spark.apache.org/docs/1.6.1/api/scala/index.html#package">Scala</a>,<a href="https://spark.apache.org/docs/1.6.1/api/java/index.html">Java</a>, and <a href="https://spark.apache.org/docs/1.6.1/api/python/index.html">Python</a> and <a href="https://spark.apache.org/docs/1.6.1/api/R/index.html">R</a> that allow data workers to efficiently execute machine learning algorithms that require fast iterative access to datasets. Spark on <a href="http://hortonworks.com/hadoop/YARN">Apache Hadoop YARN</a> enables deep integration with Hadoop and other YARN enabled workloads in the enterprise.</p>

<p>You can run batch application such as MapReduce types jobs or iterative algorithms that build upon each other. You can also run interactive queries and process streaming data with your application. Spark also provides a number of libraries which you can easily use to expand beyond the basic Spark capabilities such as Machine Learning algorithms, SQL, streaming, and graph processing. Spark runs on Hadoop clusters such as Hadoop YARN or Apache Mesos, or even in a Standalone Mode with its own scheduler. The Sandbox includes both Spark 1.6 and Spark 2.0.</p>

<p><img src="assets/Lab4_1.png" alt="Lab4_1" /></p>

<p>Let’s get started!</p>

<h3 id="step-41-configure-spark-services-using-ambari-">Step 4.1: Configure Spark services using Ambari <a id="step4.1"></a></h3>

<p>1.  Log on to Ambari Dashboard as <code class="highlighter-rouge">maria_dev</code>. At the bottom left corner of the services column, check that Spark and Zeppelin are running.</p>

<p><strong>Note:</strong> If these services are disabled, start these services.</p>

<p><img src="assets/ambari_dashboard_lab4.png" alt="ambari_dashboard_lab4" /></p>

<h3 id="for-hdp-25-sandbox-users-activate-livy-server">For HDP 2.5 Sandbox Users Activate Livy Server</h3>

<p>Livy Server is a new feature added to the latest Sandbox HDP Platform and it adds extra security while running our spark jobs from Zeppelin Notebook. For this lab, users that have HDP 2.5 Sandbox can use Livy.</p>

<p>2. Now verify the Spark livy server is running:</p>

<p><img src="assets/verify_spark_livy_server_lab4.png" alt="verify_spark_livy_server_lab4" /></p>

<p>3. As you can see our server is down. We need to start it before running spark jobs in Zeppelin. Click on <code class="highlighter-rouge">Livy Server</code>, then click on <strong>sandbox.hortonworks.com</strong>. Now we let’s scroll down to <code class="highlighter-rouge">livy server</code>, press on the <code class="highlighter-rouge">Stopped</code> button and start the server. Press the <code class="highlighter-rouge">OK</code> button in the Confirmation window.</p>

<p><img src="assets/start_livy_server_lab4.png" alt="start_livy_server_lab4" /></p>

<p>Livy Server Started:</p>

<p><img src="assets/livy_server_running_lab4.png" alt="livy_server_running_lab4" /></p>

<p>4. Go back into the Spark Service. Click on <strong>Service Actions</strong> -&gt; <strong>Turn Off Maintenance Mode</strong>.</p>

<p>Log out of Ambari.</p>

<!---### 4.1.1 There are two ways to access Zeppelin.

1\. The first way, open **Zeppelin View** from Ambari views selector:

![zeppelin_view_lab4](assets/zeppelin_view_lab4.png)

2\. Below is an image of the welcome screen from Zeppelin View, as you can see users must login to create notebooks:

![zeppelin_view_welcome_lab4](assets/zeppelin_view_welcome_lab4.png)


3\. The second way is to access Zeppelin at `sandbox.hortonworks.com:9995` through its port number:

~~~
<hostname>:9995
~~~

> Refer to [Learning the Ropes of Hortonworks Sandbox](http://hortonworks.com/hadoop-tutorial/learning-the-ropes-of-the-hortonworks-sandbox/) if you need assistance figuring out your hostname.

You should see a Zeppelin Welcome Page:

![zeppelin_welcome_page](assets/zeppelin_welcome_page_lab4.png)
--->

<p>5. Access Zeppelin at <code class="highlighter-rouge">sandbox.hortonworks.com:9995</code> through its port number:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>&lt;hostname&gt;:9995
</code></pre>
</div>

<blockquote>
  <p>Refer to <a href="http://hortonworks.com/hadoop-tutorial/learning-the-ropes-of-the-hortonworks-sandbox/">Learning the Ropes of Hortonworks Sandbox</a> if you need assistance figuring out your hostname.</p>
</blockquote>

<p>You should see a Zeppelin Welcome Page:</p>

<p><img src="assets/zeppelin_welcome_page_lab4.png" alt="zeppelin_welcome_page" /></p>

<p>Optionally, if you want to find out how to access the Spark shell to run code on Spark refer to <a href="#run-spark-in-shell">Appendix A</a>.</p>

<p>6.  Create a Zeppelin Notebook</p>

<p>Click on a Notebook tab at the top left and hit <strong>Create new note</strong>. Name your notebook <code class="highlighter-rouge">Compute Riskfactor with Spark</code>. By the default, the notebook will load Spark Scala API.</p>

<p><img src="assets/create_new_notebook_hello_hdp_lab4.png" alt="create_new_notebook" /></p>

<p><img src="assets/notebook_name_hello_hdp_lab4.png" alt="notebook_name" /></p>

<h3 id="step-42-create-a-hivecontext-">Step 4.2: Create a HiveContext <a id="step4.2"></a></h3>

<p>For improved Hive integration, HDP 2.5 offers <a href="http://hortonworks.com/blog/orcfile-in-hdp-2-better-compression-better-performance/">ORC file</a> support for Spark. This allows Spark to read data stored in ORC files. Spark can leverage ORC file’s more efficient columnar storage and predicate pushdown capability for even faster in-memory processing. HiveContext is an instance of the Spark SQL execution engine that integrates with data stored in Hive. The more basic SQLContext provides a subset of the Spark SQL support that does not depend on Hive. It reads the configuration for Hive from hive-site.xml on the classpath.</p>

<h4 id="import-sql-libraries">Import sql libraries:</h4>

<p>If you have gone through Pig section, you have to drop the table riskfactor so that you can populate it again using Spark. Copy and paste the following code into your Zeppelin notebook, then click the play button. Alternatively, press <code class="highlighter-rouge">shift+enter</code> to run the code.</p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">hive</span>
<span class="n">show</span> <span class="n">tables</span>
</code></pre>
</div>

<p>We will see that there is a table called <code class="highlighter-rouge">riskfactor</code>, let us drop that:</p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">hive</span>
<span class="n">drop</span> <span class="n">table</span> <span class="n">riskfactor</span>
</code></pre>
</div>

<p>To verify, let us do show tables again:</p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">hive</span>
<span class="n">show</span> <span class="n">tables</span>
</code></pre>
</div>

<p><img src="assets/drop_table_lab4.png" alt="drop_table_lab4" /></p>

<p>Now create it back with the same DDL that we executed in the Pig section, Write the following query:</p>

<p>We can either run the original <code class="highlighter-rouge">%spark</code> interpreter or the <code class="highlighter-rouge">%livy</code> spark interpreter to run spark code. The difference is that livy comes with more security. The default interpreter for spark jobs is <code class="highlighter-rouge">%spark</code>.</p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">spark</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql.hive.orc._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql._</span>
</code></pre>
</div>

<p><img src="assets/import_sql_libraries_hello_hdp_lab4.png" alt="import_sql_libraries" /></p>

<h4 id="instantiate-hivecontext">Instantiate HiveContext</h4>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">spark</span>
<span class="k">val</span> <span class="n">hiveContext</span> <span class="k">=</span> <span class="k">new</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">hive</span><span class="o">.</span><span class="nc">HiveContext</span><span class="o">(</span><span class="n">sc</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="assets/instantiate_hivecontext_hello_hdp_lab4.png" alt="Lab4_6" /></p>

<ul>
  <li><code class="highlighter-rouge">sc</code> stands for <strong>Spark Context</strong>. SparkContext is the main entry point to everything Spark. It can be used to create RDDs and shared variables on the cluster. When you start up the Spark Shell, the SparkContext is automatically initialized for you with the variable <code class="highlighter-rouge">sc</code>.</li>
</ul>

<h3 id="step-43-create-a-rdd-from-hivecontext-">Step 4.3: Create a RDD from HiveContext <a id="step4.3"></a></h3>

<p><strong>What is a RDD?</strong></p>

<p>Spark’s primary core abstraction is called a Resilient Distributed Dataset or RDD. It is a distributed collection of elements that is parallelized across the cluster. In other words, a RDD is an immutable collection of objects that is partitioned and distributed across multiple physical nodes of a YARN cluster and that can be operated in parallel.</p>

<p>There are three methods for creating a RDD:</p>

<ol>
  <li>Parallelize an existing collection. This means that the data already resides within Spark and can now be operated on in parallel.
    <ul>
      <li>Create a RDD by referencing a dataset. This dataset can come from any storage source supported by Hadoop such as HDFS, Cassandra, HBase etc.</li>
      <li>Create a RDD by transforming an existing RDD to create a new RDD.</li>
    </ul>
  </li>
</ol>

<p>We will be using the later two methods in our tutorial.</p>

<p><strong>RDD Transformations and Actions</strong></p>

<p>Typically, RDDs are instantiated by loading data from a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat on a YARN cluster.</p>

<p>Once a RDD is instantiated, you can apply a <a href="https://spark.apache.org/docs/1.2.0/programming-guide.html#rdd-operations">series of operations</a>. All operations fall into one of two types: <a href="https://spark.apache.org/docs/1.2.0/programming-guide.html#transformations">transformations</a> or <a href="https://spark.apache.org/docs/1.2.0/programming-guide.html#actions">actions</a>.</p>

<ul>
  <li><strong>Transformation</strong> operations, as the name suggests, create new datasets from an existing RDD and build out the processing DAG that can then be applied on the partitioned dataset across the YARN cluster. Transformations do not return a value. In fact, nothing is evaluated during the definition of these transformation statements. Spark just creates these Direct Acyclic Graphs or DAG, which will only be evaluated at runtime. We call this <em>lazy</em> evaluation.</li>
  <li>An <strong>Action</strong> operation, on the other hand, executes a DAG and returns a value.</li>
</ul>

<h4 id="431-view-list-of-tables-in-hive-warehouse">4.3.1 View List of Tables in Hive Warehouse</h4>

<p>Use a simple show command to see the list of tables in Hive warehouse.</p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">spark</span>
<span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"show tables"</span><span class="o">).</span><span class="n">collect</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="assets/view_list_tables_hive_hello_hdp_lab4.png" alt="Lab4_7" /></p>

<blockquote>
  <p>Note: false indicates whether the column requires data.</p>
</blockquote>

<p>You will notice that the geolocation table and the driver mileage table that we created earlier in an tutorial are already listed in <strong>Hive metastore</strong> and can be directly queried upon.</p>

<h4 id="432-query-tables-to-build-spark-rdd">4.3.2 Query Tables To Build Spark RDD</h4>

<p>We will do a simple select query to fetch data from geolocation and drivermileage tables to a spark variable. Getting data into Spark this way also allows to copy table schema to RDD.</p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">spark</span>
<span class="k">val</span> <span class="n">geolocation_temp1</span> <span class="k">=</span> <span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"select * from geolocation"</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="assets/query_tables_build_spark_rdd_hello_hdp_lab4.png" alt="Lab4_8" /></p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">spark</span>
<span class="k">val</span> <span class="n">drivermileage_temp1</span> <span class="k">=</span> <span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"select * from drivermileage"</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="assets/drivermileage_spark_rdd_hello_hdp_lab4.png" alt="Lab4_9" /></p>

<h4 id="44-querying-against-a-table-">4.4 Querying Against a Table <a id="step4.4"></a></h4>

<h4 id="441-registering-a-temporary-table">4.4.1 Registering a Temporary Table</h4>

<p>Now let’s register a temporary table and use SQL syntax to query against that table.</p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">spark</span>
<span class="n">geolocation_temp1</span><span class="o">.</span><span class="n">registerTempTable</span><span class="o">(</span><span class="s">"geolocation_temp1"</span><span class="o">)</span>
<span class="n">drivermileage_temp1</span><span class="o">.</span><span class="n">registerTempTable</span><span class="o">(</span><span class="s">"drivermileage_temp1"</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="assets/name_rdd_hello_hdp_lab4.png" alt="name_rdd" /></p>

<p>Next, we will perform an iteration and a filter operation. First, we need to filter drivers that have non-normal events associated with them and then count the number for non-normal events for each driver.</p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">spark</span>
<span class="k">val</span> <span class="n">geolocation_temp2</span> <span class="k">=</span> <span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"SELECT driverid, count(driverid) occurance from geolocation_temp1 where event!='normal' group by driverid"</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="assets/filter_drivers_nonnormal_events_hello_hdp_lab4.png" alt="filter_drivers_nonnormal_events" /></p>

<ul>
  <li>
    <p>As stated earlier about RDD transformations, select operation is a RDD transformation and therefore does not return anything.</p>
  </li>
  <li>
    <p>The resulting table will have a count of total non-normal events associated with each driver. Register this filtered table as a temporary table so that subsequent SQL queries can be applied to it.</p>
  </li>
</ul>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">spark</span>
<span class="n">geolocation_temp2</span><span class="o">.</span><span class="n">registerTempTable</span><span class="o">(</span><span class="s">"geolocation_temp2"</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="assets/register_filtered_table_hello_hdp_lab4.png" alt="register_table" /></p>

<ul>
  <li>You can view the result by executing an action operation on the RDD.</li>
</ul>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">spark</span>
<span class="n">geolocation_temp2</span><span class="o">.</span><span class="n">take</span><span class="o">(</span><span class="mi">10</span><span class="o">).</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="assets/view_results_op_on_rdd_hello_hdp_lab4.png" alt="Lab4_11" /></p>

<h4 id="442--perform-join-operation">4.4.2  Perform join Operation</h4>

<p>In this section we will perform a join operation geolocation_temp2 table has details of drivers and count of their respective non-normal events. drivermileage_temp1 table has details of total miles travelled by each driver.</p>

<ul>
  <li>We will join two tables on common column, which in our case is <code class="highlighter-rouge">driverid</code>.</li>
</ul>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">spark</span>
<span class="k">val</span> <span class="n">joined</span> <span class="k">=</span> <span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"select a.driverid,a.occurance,b.totmiles from geolocation_temp2 a,drivermileage_temp1 b where a.driverid=b.driverid"</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="assets/join_op_column_hello_hdp_lab4.png" alt="Lab4_12" /></p>

<ul>
  <li>The resulting data set will give us total miles and total non-normal events for a particular driver. Register this filtered table as a temporary table so that subsequent SQL queries can be applied to it.</li>
</ul>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">spark</span>
<span class="n">joined</span><span class="o">.</span><span class="n">registerTempTable</span><span class="o">(</span><span class="s">"joined"</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="assets/register_joined_table_hello_hdp_lab4.png" alt="register_join_table" /></p>

<ul>
  <li>You can view the result by executing action operation on RDD.</li>
</ul>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">spark</span>
<span class="n">joined</span><span class="o">.</span><span class="n">take</span><span class="o">(</span><span class="mi">10</span><span class="o">).</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="assets/show_results_joined_table_hello_hdp_lab4.png" alt="Lab4_13" /></p>

<h4 id="443--compute-driver-risk-factor">4.4.3  Compute Driver Risk Factor</h4>

<p>In this section we will associate a driver risk factor with every driver. Driver risk factor will be calculated by dividing total miles travelled by non-normal event occurrences.</p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">spark</span>
<span class="k">val</span> <span class="n">risk_factor_spark</span><span class="k">=</span><span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"select driverid, occurance, totmiles, totmiles/occurance riskfactor from joined"</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="assets/calculate_riskfactor_hello_hdp_lab4.png" alt="Lab4_14" /></p>

<ul>
  <li>The resulting data set will give us total miles and total non normal events and what is a risk for a particular driver. Register this filtered table as a temporary table so that subsequent SQL queries can be applied to it.</li>
</ul>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">spark</span>
<span class="n">risk_factor_spark</span><span class="o">.</span><span class="n">registerTempTable</span><span class="o">(</span><span class="s">"risk_factor_spark"</span><span class="o">)</span>
</code></pre>
</div>

<ul>
  <li>View the results</li>
</ul>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">spark</span>
<span class="n">risk_factor_spark</span><span class="o">.</span><span class="n">take</span><span class="o">(</span><span class="mi">10</span><span class="o">).</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="assets/view_results_filtertable_hello_hdp_lab4.png" alt="Lab4_15" /></p>

<h3 id="step-45-load-and-save-data-into-hive-as-orc-">Step 4.5: Load and Save Data into Hive as ORC <a id="step4.5"></a></h3>

<p>In this section we store data in a smart ORC (Optimized Row Columnar) format using Spark. ORC is a self-describing type-aware columnar file format designed for Hadoop workloads. It is optimized for large streaming reads and with integrated support for finding required rows fast. Storing data in a columnar format lets the reader read, decompress, and process only the values required for the current query. Because ORC files are type aware, the writer chooses the most appropriate encoding for the type and builds an internal index as the file is persisted.</p>

<p>Predicate pushdown uses those indexes to determine which stripes in a file need to be read for a particular query and the row indexes can narrow the search to a particular set of 10,000 rows. ORC supports the complete set of types in Hive, including the complex types: structs, lists, maps, and unions.</p>

<h4 id="451-create-an-orc-table">4.5.1 Create an ORC table</h4>

<p>Create a table and store it as ORC. Specifying as <em>orc</em> at the end of the SQL statement below ensures that the Hive table is stored in the ORC format.</p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">spark</span>
<span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"create table finalresults( driverid String, occurance bigint,totmiles bigint,riskfactor double) stored as orc"</span><span class="o">).</span><span class="n">toDF</span><span class="o">()</span>
</code></pre>
</div>

<blockquote>
  <p>Note: toDF() creates a DataFrame with columns driverid String, occurance bigin, etc.</p>
</blockquote>

<p><img src="assets/create_orc_table_hello_hdp_lab4.png" alt="create_orc_table" /></p>

<h4 id="452-convert-data-into-orc-table">4.5.2 Convert data into ORC table</h4>

<p>Before we load the data into hive table that we created above, we will have to convert our data file into ORC format too.</p>

<blockquote>
  <p><strong>Note:</strong> For Spark 1.4.1 and higher, use</p>
</blockquote>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">spark</span>
<span class="n">risk_factor_spark</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">"orc"</span><span class="o">).</span><span class="n">save</span><span class="o">(</span><span class="s">"risk_factor_spark"</span><span class="o">)</span>
</code></pre>
</div>

<p>If you used the above script, skip the following instruction and move to 4.5.3.</p>

<blockquote>
  <p><strong>Note:</strong> For Spark 1.3.1, use</p>
</blockquote>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">spark</span>
<span class="n">risk_factor_spark</span><span class="o">.</span><span class="n">saveAsOrcFile</span><span class="o">(</span><span class="s">"risk_factor_spark"</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="assets/convert_orc_table_hello_hdp_lab4.png" alt="risk_factor_orc" /></p>

<h4 id="453-load-the-data-into-hive-table-using-load-data-command">4.5.3 Load the data into Hive table using load data command</h4>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">spark</span>
<span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"load data inpath 'risk_factor_spark' into table finalresults"</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="assets/load_data_to_finalresults_hello_hdp_lab4.png" alt="load_data_to_finalresults" /></p>

<h4 id="454-create-the-final-table-riskfactor-using-ctas">4.5.4 Create the final table Riskfactor using CTAS</h4>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">spark</span>
<span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"create table riskfactor as select * from finalresults"</span><span class="o">)</span>
</code></pre>
</div>

<p><img src="assets/create_table_riskfactor_spark.png" alt="create_table_riskfactor_spark" /></p>

<h4 id="455-verify-data-successfully-populated-hive-table-in-hive-check-2">4.5.5 Verify Data Successfully Populated Hive Table in Hive (Check 2)</h4>

<p>Execute a select query to verify your table has been successfully stored. You can go to Ambari Hive user view to check whether the Hive table you created has the data populated in it.</p>

<p><img src="assets/riskfactor_table_populated.png" alt="riskfactor_table_populated" /></p>

<blockquote>
  <p>Hive riskfactor table populated</p>
</blockquote>

<p>Did both tables have the same data up to 10 rows?</p>

<h2 id="full-spark-code-review-for-lab">Full Spark Code Review for Lab</h2>

<p><strong>Import hive and sql libraries</strong></p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="o">%</span><span class="n">spark</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql.hive.orc._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql._</span>

<span class="k">val</span> <span class="n">hiveContext</span> <span class="k">=</span> <span class="k">new</span> <span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="o">.</span><span class="n">hive</span><span class="o">.</span><span class="nc">HiveContext</span><span class="o">(</span><span class="n">sc</span><span class="o">)</span>
</code></pre>
</div>

<p><strong>Shows tables in the default hive database</strong></p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"show tables"</span><span class="o">).</span><span class="n">collect</span><span class="o">.</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</code></pre>
</div>

<p><strong>Select all rows and columns from tables, stores hive script into variable
and registers variables as RDD</strong></p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="k">val</span> <span class="n">geolocation_temp1</span> <span class="k">=</span> <span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"select * from geolocation"</span><span class="o">)</span>

<span class="k">val</span> <span class="n">drivermileage_temp1</span> <span class="k">=</span> <span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"select * from drivermileage"</span><span class="o">)</span>

<span class="n">geolocation_temp1</span><span class="o">.</span><span class="n">registerTempTable</span><span class="o">(</span><span class="s">"geolocation_temp1"</span><span class="o">)</span>
<span class="n">drivermileage_temp1</span><span class="o">.</span><span class="n">registerTempTable</span><span class="o">(</span><span class="s">"drivermileage_temp1"</span><span class="o">)</span>

<span class="k">val</span> <span class="n">geolocation_temp2</span> <span class="k">=</span> <span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"SELECT driverid, count(driverid) occurance from geolocation_temp1  where event!='normal' group by driverid"</span><span class="o">)</span>

<span class="n">geolocation_temp2</span><span class="o">.</span><span class="n">registerTempTable</span><span class="o">(</span><span class="s">"geolocation_temp2"</span><span class="o">)</span>
</code></pre>
</div>

<p><strong>Load first 10 rows from geolocation_temp2, which is the data from
drivermileage table</strong></p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="n">geolocation_temp2</span><span class="o">.</span><span class="n">take</span><span class="o">(</span><span class="mi">10</span><span class="o">).</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</code></pre>
</div>

<p><strong>Create joined to join 2 tables by the same driverid and register joined
as a RDD</strong></p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="k">val</span> <span class="n">joined</span> <span class="k">=</span> <span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"select a.driverid,a.occurance,b.totmiles from geolocation_temp2 a,drivermileage_temp1 b where a.driverid=b.driverid"</span><span class="o">)</span>

<span class="n">joined</span><span class="o">.</span><span class="n">registerTempTable</span><span class="o">(</span><span class="s">"joined"</span><span class="o">)</span>
</code></pre>
</div>

<p><strong>Load first 10 rows and columns in joined</strong></p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="n">joined</span><span class="o">.</span><span class="n">take</span><span class="o">(</span><span class="mi">10</span><span class="o">).</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</code></pre>
</div>

<p><strong>Initialize risk_factor_spark and register as an RDD</strong></p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="k">val</span> <span class="n">risk_factor_spark</span><span class="k">=</span><span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"select driverid, occurance, totmiles, totmiles/occurance riskfactor from joined"</span><span class="o">)</span>

<span class="n">risk_factor_spark</span><span class="o">.</span><span class="n">registerTempTable</span><span class="o">(</span><span class="s">"risk_factor_spark"</span><span class="o">)</span>
</code></pre>
</div>

<p><strong>Print the first 10 lines from the risk_factor_spark table</strong></p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="n">risk_factor_spark</span><span class="o">.</span><span class="n">take</span><span class="o">(</span><span class="mi">10</span><span class="o">).</span><span class="n">foreach</span><span class="o">(</span><span class="n">println</span><span class="o">)</span>
</code></pre>
</div>

<p><strong>Create table finalresults in Hive, save it as ORC, load data into it,
and then create the final table called riskfactor using CTAS</strong></p>

<div class="language-scala highlighter-rouge"><pre class="highlight"><code><span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"create table finalresults( driverid String, occurance bigint,totmiles bigint,riskfactor double) stored as orc"</span><span class="o">).</span><span class="n">toDF</span><span class="o">()</span>

<span class="n">risk_factor_spark</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">format</span><span class="o">(</span><span class="s">"orc"</span><span class="o">).</span><span class="n">save</span><span class="o">(</span><span class="s">"risk_factor_spark"</span><span class="o">)</span>

<span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"load data inpath 'risk_factor_spark' into table finalresults"</span><span class="o">)</span>

<span class="n">hiveContext</span><span class="o">.</span><span class="n">sql</span><span class="o">(</span><span class="s">"create table riskfactor as select * from finalresults"</span><span class="o">)</span>
</code></pre>
</div>

<h2 id="appendix-a-run-spark-code-in-the-spark-interactive-shell-">Appendix A: Run Spark Code in the Spark Interactive Shell <a id="run-spark-in-shell"></a></h2>

<p>1) Open your terminal or putty.  SSH into the Sandbox using <code class="highlighter-rouge">root</code> as login and <code class="highlighter-rouge">hadoop</code> as password.</p>

<div class="highlighter-rouge"><pre class="highlight"><code>login: root
password: hadoop
</code></pre>
</div>

<p>Optionally, if you don’t have an SSH client installed and configured you can use the built-in web client which can be accessed from here: http://host:4200 (use the same username and password provided above)</p>

<p>2) Let’s enter the Spark interactive shell (spark repl). Type the command</p>

<div class="highlighter-rouge"><pre class="highlight"><code>spark-shell
</code></pre>
</div>

<p>This will load the default Spark Scala API. Issue the command <code class="highlighter-rouge">exit</code> to drop out of the Spark Shell.</p>

<p><img src="assets/spark_shell_hello_hdp_lab4.png" alt="spark_shell_welcome_page" /></p>

<blockquote>
  <p><strong>Note:</strong> Hive comes preconfigured with HDP Sandbox.</p>
</blockquote>

<p>The coding exercise we just went through can be also completed using a Spark shell. Just as we did in Zeppelin, you can copy and paste the code.</p>

<h2 id="summary-">Summary <a id="summary-lab4"></a></h2>
<p>Congratulations! Let’s summarize the spark coding skills and knowledge we acquired to compute risk factor associated with every driver. Apache Spark is efficient for computation because of its <strong>in-memory data processing engine</strong>. We learned how to integrate hive with spark by creating a <strong>Hive Context</strong>. We used our existing data from Hive to create an <strong>RDD</strong>. We learned to perform <strong>RDD transformations and actions</strong> to create new datasets from existing RDDs. These new datasets include filtered, manipulated and processed data. After we computed <strong>risk factor</strong>, we learned to load and save data into Hive as <strong>ORC</strong>.</p>

<h2 id="suggested-readings-">Suggested Readings <a id="suggested-readings"></a></h2>

<p>To learn more about Spark, checkout these resources:</p>
<ul>
  <li><a href="http://hortonworks.com/hadoop/spark/">Apache Spark</a></li>
  <li><a href="http://spark.apache.org/">Apache Spark Welcome</a></li>
  <li><a href="http://spark.apache.org/docs/latest/programming-guide.html#passing-functions-to-spark">Spark Programming Guide</a></li>
  <li><a href="http://www.amazon.com/Learning-Spark-Lightning-Fast-Data-Analysis/dp/1449358624/ref=sr_1_1?ie=UTF8&amp;qid=1456010684&amp;sr=8-1&amp;keywords=apache+spark">Learning Spark</a></li>
  <li><a href="http://www.amazon.com/Advanced-Analytics-Spark-Patterns-Learning/dp/1491912766/ref=pd_bxgy_14_img_2?ie=UTF8&amp;refRID=19EGG68CJ0NTNE9RQ2VX">Advanced Analytics with Spark</a></li>
  <li><a href="http://hortonworks.com/training/class/hdp-developer-apache-spark-using-scala/">HDP DEVELOPER: APACHE SPARK USING SCALA</a></li>
  <li><a href="http://hortonworks.com/training/class/hdp-developer-apache-spark-using-python/">HDP DEVELOPER: APACHE SPARK USING PYTHON</a></li>
</ul>

</div>

<div id="tutorial-footer">
  <hr>
  <h2>Tutorial Q&amp;A and Reporting Issues</h2>
  <p>If you need help or have questions with this tutorial, please first check HCC for existing Answers to questions on this tutorial using the Find Answers button.  If you don't find your answer you can post a new HCC question for this tutorial using the Ask Questions button.</p>
  <p><a class="btn" href="https://community.hortonworks.com/topics/tutorial-/tutorials/hortonworks/hdp-2.5/hello-hdp-an-introduction-to-hadoop/hello-hdp-section-6.html" role="button">Find Answers</a> <a class="btn pull-right" href="https://community.hortonworks.com/questions/ask.html?space=81&topics=tutorial-hdp-2.5.0&topics=hdp-2.5.0" role="button">Ask Questions</a></p>
  <p>Tutorial Name: <strong>Hello HDP An Introduction to Hadoop with Hive and Pig</strong></p>
  <p>HCC Tutorial Tag:<strong> tutorial-/tutorials/hortonworks/hdp-2.5/hello-hdp-an-introduction-to-hadoop/hello-hdp-section-6</strong> and <strong>hdp-2.5.0</strong></p>
  <p>If the tutorial has multiple labs please indicate which lab your question corresponds to. Please provide any feedback related to that lab.</p>
  <p>All Hortonworks, partner and community tutorials are posted in the Hortonworks github and can be contributed via the <a href="https://github.com/hortonworks/tutorials/wiki">Hortonworks Tutorial Contribution Guide</a>.  If you are certain there is an issue or bug with the tutorial, please <a href="https://github.com/hortonworks/tutorials/wiki#issues-with-tutorials">create an issue</a> on the repository and we will do our best to resolve it!</p>
</div>

